{
    "docs": [
        {
            "location": "/about/index.html", 
            "text": "Laplacians.jl\n\n\n\n\n\n\nLaplacians is a package containing graph algorithms, with an emphasis on tasks related to spectral and algebraic graph theory. It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.\n\n\nAll graphs are represented by sparse adjacency matrices. This is both for speed, and because our main concerns are algebraic tasks. It does not handle dynamic graphs. It would be very slow to implement dynamic graphs this way.\n\n\nThe documentation may be found in \nhttp://danspielman.github.io/Laplacians.jl/about/index.html\n.\n\n\nThis includes instructions for installing Julia, and some tips for how to start using it.  It also includes guidelines for Dan Spielman's collaborators.\n\n\nFor some examples of some of the things you can do with Laplacians, look at \n\n\n\n\nthis Julia notebook\n.\n\n\nLow Stretch Spanning Trees\n\n\nInformation about solving Laplacian equations\n\n\nAnd, try the chimera and wtedChimera graph generators.  They are designed to generate a wide variety of graphs so as to exercise code.\n\n\n\n\nIf you want to solve Laplacian equations, we recommend the KMPLapSolver.  For SDD equations, we recommend the KMPSDDSolver.\n\n\nThe algorithms provide by Laplacians.jl include:\n\n\n\n\nakpw\n, a heuristic for computing low stretch spanning trees written by Daniel Spielman, inspired by the algorithm from the paper \"A graph-theoretic\n\n\n\n\ngame and its application to the k-server problem\" by Alon, Karp, Peleg, and West, \nSIAM Journal on Computing\n, 1995.\n\n\n\n\nKMPLapSolver\n and \nKMPSDDSolver\n: linear equation solvers based on the paper \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng, \nSIAM Journal on Computing\n, 2014.\n\n\nsamplingSDDSolver\n and \nsamplingLapSolver\n, based on the paper \"Approximate Gaussian Elimination for Laplacians:\n\n\n\n\nFast, Sparse, and Simple\" by Rasmus Kyng and Sushant Sachdeva, FOCS 2016. \n\n\n\n\nchimera\n and \nwtedChimera\n graph generators for testing graph algorithms, by Daniel Spielman.\n\n\nLocal Graph Clustering Heuristics, implemented by Serban Stan, including \nprn\n a version of PageRank Nibble based on \"Using PageRank to Locally Partition a Graph\", \nInternet Mathematics\n and \nLocalImprove\n based on \"Flow-Based Algorithms for Local Graph Clustering\" by Zeyuan Allen-Zhu and Lorenzo Orecchia, SODA 2014.\n\n\n\n\n\n\nCurrent Development Version\n\n\nTo get the current version of the master branch, run \nPkg.checkout(\"Laplacians\")\n\n\n\n\nVersion 0.0.3, November 20, 2016\n\n\nThis version works with Julia 0.5. This is what you retrieve when you run \nPkg.add(\"Laplacians\")\n\n\nWarning: the behavior of chimera and wtedChimera differs between Julia 0.4 and Julia 0.5 because randperm acts differently in these.\n\n\n\n\nVersion 0.0.2, November 19, 2016\n\n\nThis is the version that works with Julia 0.4. It was captured right before the upgrade to Julia 0.5", 
            "title": "About"
        }, 
        {
            "location": "/about/index.html#laplaciansjl", 
            "text": "Laplacians is a package containing graph algorithms, with an emphasis on tasks related to spectral and algebraic graph theory. It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.  All graphs are represented by sparse adjacency matrices. This is both for speed, and because our main concerns are algebraic tasks. It does not handle dynamic graphs. It would be very slow to implement dynamic graphs this way.  The documentation may be found in  http://danspielman.github.io/Laplacians.jl/about/index.html .  This includes instructions for installing Julia, and some tips for how to start using it.  It also includes guidelines for Dan Spielman's collaborators.  For some examples of some of the things you can do with Laplacians, look at    this Julia notebook .  Low Stretch Spanning Trees  Information about solving Laplacian equations  And, try the chimera and wtedChimera graph generators.  They are designed to generate a wide variety of graphs so as to exercise code.   If you want to solve Laplacian equations, we recommend the KMPLapSolver.  For SDD equations, we recommend the KMPSDDSolver.  The algorithms provide by Laplacians.jl include:   akpw , a heuristic for computing low stretch spanning trees written by Daniel Spielman, inspired by the algorithm from the paper \"A graph-theoretic   game and its application to the k-server problem\" by Alon, Karp, Peleg, and West,  SIAM Journal on Computing , 1995.   KMPLapSolver  and  KMPSDDSolver : linear equation solvers based on the paper \"Approaching optimality for solving SDD systems\" by Koutis, Miller, and Peng,  SIAM Journal on Computing , 2014.  samplingSDDSolver  and  samplingLapSolver , based on the paper \"Approximate Gaussian Elimination for Laplacians:   Fast, Sparse, and Simple\" by Rasmus Kyng and Sushant Sachdeva, FOCS 2016.    chimera  and  wtedChimera  graph generators for testing graph algorithms, by Daniel Spielman.  Local Graph Clustering Heuristics, implemented by Serban Stan, including  prn  a version of PageRank Nibble based on \"Using PageRank to Locally Partition a Graph\",  Internet Mathematics  and  LocalImprove  based on \"Flow-Based Algorithms for Local Graph Clustering\" by Zeyuan Allen-Zhu and Lorenzo Orecchia, SODA 2014.", 
            "title": "Laplacians.jl"
        }, 
        {
            "location": "/about/index.html#current-development-version", 
            "text": "To get the current version of the master branch, run  Pkg.checkout(\"Laplacians\")", 
            "title": "Current Development Version"
        }, 
        {
            "location": "/about/index.html#version-003-november-20-2016", 
            "text": "This version works with Julia 0.5. This is what you retrieve when you run  Pkg.add(\"Laplacians\")  Warning: the behavior of chimera and wtedChimera differs between Julia 0.4 and Julia 0.5 because randperm acts differently in these.", 
            "title": "Version 0.0.3, November 20, 2016"
        }, 
        {
            "location": "/about/index.html#version-002-november-19-2016", 
            "text": "This is the version that works with Julia 0.4. It was captured right before the upgrade to Julia 0.5", 
            "title": "Version 0.0.2, November 19, 2016"
        }, 
        {
            "location": "/Installation/index.html", 
            "text": "Installation\n\n\nBefore you can use Laplacians, you need Julia. So, we'll begin with instructions for installing Julia.  I (Dan S.) found that it worked best if I installed Python first.  So, I'll suggest that you do the same.\n\n\nAll of these instruction assume you are using a Mac. \n\n\n\n\nPython\n\n\nInstall python.  I recommend the anaconda distribution \nhttps://www.continuum.io/downloads\n.\n\n\nOnce you install python, you are going to want two packages: a plotting package that Julia will use, and jupyter notebooks for interacting with Julia.  Install them like\n\n\nconda install matplotlib\nconda install mathjax\nconda install jupyter\n\n\n\n\n\n\nJulia\n\n\nYou can get Julia from  \nhttp://julialang.org/\n.   If you are using a Mac, you may wish to create a symnolic link to the Julia executable so that you can call it from a terminal.  For example, you can do this like:\n\n\ncd /usr/local/bin/\nln -s julia /Applications/Julia-0.5.app/Contents/Resources/julia/bin/julia\n\n\n\n\nOnce you have this, you will want Julia notebooks.  To install this, run \njulia\n and type\n\n\njulia\n Pkg.add(\nIJulia\n)\njulia\n using IJulia\n\n\n\n\nThis will install the package, and put the current julia kernel into \njupyter\n.  In the future, you can launch the Julia notebook by typing (in a terminal)\n\n\njupyter notebook\n\n\n\n\n\n\nLaplacians\n\n\nIn theory, all you need to do now is type either\n\n\njulia\n Pkg.add(\nLaplacians\n)\n\n\n\n\nTo use the package, you then type\n\n\njulia\n using Laplacians\n\n\n\n\nThis should add all the packages upon which Laplacians explicitly depends. \n\n\nLaplacians might add some packages that you do not want, or which might not be available on your system.  If you do not want to load PyPlot, then set \n\n\njulia\n LAPLACIANS_NOPLOT = true\n\n\n\n\nbefore typing \nusing Laplacians\n. Similarly, you can avoid loading PyAmg by setting\n\n\njulia\n LAPLACIANS_NOAMG = true\n\n\n\n\nActually, defining these variables to anything will have the same effect.  So, setting them to false has the same effect as setting them to true.\n\n\nTo see if Laplacians is working, try typing\n\n\na = chimera(100,6)\nspectralDrawing(a)\n\n\n\n\nor\n\n\na = generalizedNecklace(grid2(6),grid2(3),2)\nspectralDrawing(a)", 
            "title": "Installation"
        }, 
        {
            "location": "/Installation/index.html#installation", 
            "text": "Before you can use Laplacians, you need Julia. So, we'll begin with instructions for installing Julia.  I (Dan S.) found that it worked best if I installed Python first.  So, I'll suggest that you do the same.  All of these instruction assume you are using a Mac.", 
            "title": "Installation"
        }, 
        {
            "location": "/Installation/index.html#python", 
            "text": "Install python.  I recommend the anaconda distribution  https://www.continuum.io/downloads .  Once you install python, you are going to want two packages: a plotting package that Julia will use, and jupyter notebooks for interacting with Julia.  Install them like  conda install matplotlib\nconda install mathjax\nconda install jupyter", 
            "title": "Python"
        }, 
        {
            "location": "/Installation/index.html#julia", 
            "text": "You can get Julia from   http://julialang.org/ .   If you are using a Mac, you may wish to create a symnolic link to the Julia executable so that you can call it from a terminal.  For example, you can do this like:  cd /usr/local/bin/\nln -s julia /Applications/Julia-0.5.app/Contents/Resources/julia/bin/julia  Once you have this, you will want Julia notebooks.  To install this, run  julia  and type  julia  Pkg.add( IJulia )\njulia  using IJulia  This will install the package, and put the current julia kernel into  jupyter .  In the future, you can launch the Julia notebook by typing (in a terminal)  jupyter notebook", 
            "title": "Julia"
        }, 
        {
            "location": "/Installation/index.html#laplacians", 
            "text": "In theory, all you need to do now is type either  julia  Pkg.add( Laplacians )  To use the package, you then type  julia  using Laplacians  This should add all the packages upon which Laplacians explicitly depends.   Laplacians might add some packages that you do not want, or which might not be available on your system.  If you do not want to load PyPlot, then set   julia  LAPLACIANS_NOPLOT = true  before typing  using Laplacians . Similarly, you can avoid loading PyAmg by setting  julia  LAPLACIANS_NOAMG = true  Actually, defining these variables to anything will have the same effect.  So, setting them to false has the same effect as setting them to true.  To see if Laplacians is working, try typing  a = chimera(100,6)\nspectralDrawing(a)  or  a = generalizedNecklace(grid2(6),grid2(3),2)\nspectralDrawing(a)", 
            "title": "Laplacians"
        }, 
        {
            "location": "/Julia/index.html", 
            "text": "Using Julia\n\n\nThese are some things you might want to know about using Julia if it is new to you.  There are now many other resources that can explain Julia to you.  But, we keep this section here for reference.\n\n\n\n\nDocstrings\n\n\nJulia 0.5 lets you take advantage of docstrings. For example, \n?ringGraph\n produces\n\n\nThe simple ring on n vertices\n\n\n\n\nWhen having a multiline comment, make sure that lines don't have starting and trailing spaces. This will mess up the indentation when calling '?func_name'.\n\n\n\n\nJulia Notebooks\n\n\nTo get the Julia notebooks working, I presently type \njupyter notebook\n. I then select the kernel to be Julia-0.5.0. It seems important to run this command from a directory that contains all the directories that have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks from other directories.  That has only given me trouble.\n\n\nTo turn a notebook into html, you type something like\n\n\njupyter nbconvert Laplacians.ipynb\n\n\n\n\nor\n\n\njupyter nbconvert --to markdown --stdout Sampler.ipynb \n SamplerNotebook.md\n\n\n\n\n\n\nWorkflows\n\n\nJulia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:\n\n\n\n\nDan's current workflow:\n\n\n\n\nI use emacs (which has a mode for Julia) and the notebooks.\n\n\nI develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.\n\n\nAfter modifying code, I reload it with \ninclude(\"develX.jl\")\n.  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.\n\n\nI am writing this documention in MacDown.\n\n\n\n\n\n\nAdd your current workflow here:\n\n\n\n\nThings to be careful of (common bugs)\n\n\n\n\nJulia passes vectors and matrices to routines by reference, rather than by copying them.  If you type \nx = y\n when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type \nx = copy(y)\n.  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.\n\n\nOn the other hand, if you type \nx = x + y\n, then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type \nx += y\n.  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.\n\n\n\n\n\n\nAdds b in to a\n\nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend\n\n\nFails to add b in to a\n\nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2\n\n\n\n\n\n\n\nIf you are used to programming in Matlab, you might be tempted to type a line like \nfor i in 1:10,\n.  \nDo not put extra commas in Julia!\n  It will cause bad things to happen.\n\n\nTo get a vector with entries 1 through n, type \ncollect(1:n)\n.  The object \n1:n\n is a range, rather than a vector.\n\n\nJulia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the \nsetValue\n function. \nsetValue(G, u, i, 0)\n will set \nweighti(G, u, i)\n to 0 while also leaving \n(u, nbri(G, u, i))\n in the matrix.  \nNote This problem may have been fixed with Julia version 0.5.\n\n\n\n\n\n\nUseful Julia functions\n\n\nI am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.\n\n\n\n\ndocstrings: in the above example, I used a docstring to document each function.  You can get these by typing \n?addA2B\n.  You can also  \nwrite longer docstrings and use markdown\n.  I suggest putting them in front of every function.\n\n\nmethods(foo)\n lists all methods with the name foo.\n\n\nfieldnames(footype)\n tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type \nnames(footype)\n\n\n\n\njulia\n a = sparse(rand(3,3));\njulia\n fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval\n\n\n\n\n\n\nOptimizing code in Julia\n\n\nThe best way that I've found of figuring out what's slowing down my code has been to use \n@code_warntype\n.  \n\n\nNote that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use \n@time\n to time your code.\n\n\nI recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.\n\n\n\n\nHow should notebooks play with Git?\n\n\nThe great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#using-julia", 
            "text": "These are some things you might want to know about using Julia if it is new to you.  There are now many other resources that can explain Julia to you.  But, we keep this section here for reference.", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#docstrings", 
            "text": "Julia 0.5 lets you take advantage of docstrings. For example,  ?ringGraph  produces  The simple ring on n vertices  When having a multiline comment, make sure that lines don't have starting and trailing spaces. This will mess up the indentation when calling '?func_name'.", 
            "title": "Docstrings"
        }, 
        {
            "location": "/Julia/index.html#julia-notebooks", 
            "text": "To get the Julia notebooks working, I presently type  jupyter notebook . I then select the kernel to be Julia-0.5.0. It seems important to run this command from a directory that contains all the directories that have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks from other directories.  That has only given me trouble.  To turn a notebook into html, you type something like  jupyter nbconvert Laplacians.ipynb  or  jupyter nbconvert --to markdown --stdout Sampler.ipynb   SamplerNotebook.md", 
            "title": "Julia Notebooks"
        }, 
        {
            "location": "/Julia/index.html#workflows", 
            "text": "Julia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:", 
            "title": "Workflows"
        }, 
        {
            "location": "/Julia/index.html#dans-current-workflow", 
            "text": "I use emacs (which has a mode for Julia) and the notebooks.  I develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.  After modifying code, I reload it with  include(\"develX.jl\") .  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.  I am writing this documention in MacDown.", 
            "title": "Dan's current workflow:"
        }, 
        {
            "location": "/Julia/index.html#add-your-current-workflow-here", 
            "text": "", 
            "title": "Add your current workflow here:"
        }, 
        {
            "location": "/Julia/index.html#things-to-be-careful-of-common-bugs", 
            "text": "Julia passes vectors and matrices to routines by reference, rather than by copying them.  If you type  x = y  when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type  x = copy(y) .  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.  On the other hand, if you type  x = x + y , then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type  x += y .  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.    Adds b in to a \nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend Fails to add b in to a \nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2   If you are used to programming in Matlab, you might be tempted to type a line like  for i in 1:10, .   Do not put extra commas in Julia!   It will cause bad things to happen.  To get a vector with entries 1 through n, type  collect(1:n) .  The object  1:n  is a range, rather than a vector.  Julia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the  setValue  function.  setValue(G, u, i, 0)  will set  weighti(G, u, i)  to 0 while also leaving  (u, nbri(G, u, i))  in the matrix.   Note This problem may have been fixed with Julia version 0.5.", 
            "title": "Things to be careful of (common bugs)"
        }, 
        {
            "location": "/Julia/index.html#useful-julia-functions", 
            "text": "I am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.   docstrings: in the above example, I used a docstring to document each function.  You can get these by typing  ?addA2B .  You can also   write longer docstrings and use markdown .  I suggest putting them in front of every function.  methods(foo)  lists all methods with the name foo.  fieldnames(footype)  tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type  names(footype)   julia  a = sparse(rand(3,3));\njulia  fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval", 
            "title": "Useful Julia functions"
        }, 
        {
            "location": "/Julia/index.html#optimizing-code-in-julia", 
            "text": "The best way that I've found of figuring out what's slowing down my code has been to use  @code_warntype .    Note that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use  @time  to time your code.  I recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.", 
            "title": "Optimizing code in Julia"
        }, 
        {
            "location": "/Julia/index.html#how-should-notebooks-play-with-git", 
            "text": "The great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "How should notebooks play with Git?"
        }, 
        {
            "location": "/Examples/index.html", 
            "text": "Examples\n\n\nThe following are links to html files of Julia notebooks. These notebooks are also in the notebook directory, and can be open there so that you can run the code live. You should be able to find them under ~/.julia/v0.4/Laplacians/notebooks.\n\n\n\n\nFirstNotebook\n\n\nSolvers\n\n\nSampler\n\n\nLocalClustering\n\n\nLocalClustering Statistics", 
            "title": "Examples"
        }, 
        {
            "location": "/Examples/index.html#examples", 
            "text": "The following are links to html files of Julia notebooks. These notebooks are also in the notebook directory, and can be open there so that you can run the code live. You should be able to find them under ~/.julia/v0.4/Laplacians/notebooks.   FirstNotebook  Solvers  Sampler  LocalClustering  LocalClustering Statistics", 
            "title": "Examples"
        }, 
        {
            "location": "/CSCgraph/index.html", 
            "text": "Using sparse matrices as graphs\n\n\nThe routines \ndeg\n, \nnbri\n and \nweighti\n will let you treat a sparse matrix like a graph.\n\n\ndeg(graph, u)\n is the degree of node u. \nnbri(graph, u, i)\n is the ith neighbor of node u. \nweighti(graph, u, i)\n is the weight of the edge to the ith neighbor of node u.\n\n\nNote that we start indexing from 1.\n\n\nFor example, to iterate over the neighbors of node v,   and play with the attached nodes, you could write code like:\n\n\n  for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end\n\n\n\n\nBut, this turns out to be much slower than working with the structure directly, like\n\n\n  for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end\n\n\n\n\n\n\n[ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.\n\n\n\n\n\n\nThe SparseMatrixCSC data structure\n\n\nYou can explore what is going on with the data structure by looking at some examples.  For example, here is a randomly weighted complete graph on 4 vertices, first displayed as a matrix:\n\n\ngr = round(10*uniformWeight(completeGraph(4)))\n\n4x4 sparse matrix with 12 Float64 entries:\n    [2, 1]  =  3.0\n    [3, 1]  =  3.0\n    [4, 1]  =  6.0\n    [1, 2]  =  3.0\n    [3, 2]  =  1.0\n    [4, 2]  =  2.0\n    [1, 3]  =  3.0\n    [2, 3]  =  1.0\n    [4, 3]  =  7.0\n    [1, 4]  =  6.0\n    [2, 4]  =  2.0\n    [3, 4]  =  7.0\n\nfull(gr)\n\n4x4 Array{Float64,2}:\n 0.0  3.0  3.0  6.0\n 3.0  0.0  1.0  2.0\n 3.0  1.0  0.0  7.0\n 6.0  2.0  7.0  0.0\n\n\n\n\nTo see the underlying data structure, use \nfieldnames\n.\n\n\nfieldnames(gr)\n\n5-element Array{Symbol,1}:\n :m     \n :n     \n :colptr\n :rowval\n :nzval \n\n\n\n\nm\n and \nn\n are the dimensions of the matrix. The entries of the matrix are stored in nzval. colptr[i] is the index in nzval of the first nonzero entry in column i.  rowval tells you which rows in each column are nonzero. The indices of the nonzero entries in column i are stored in  rowval[colptr[i]] through rowval[colptr[i+1]-1].\n\n\ngr.colptr \n\n5-element Array{Int64,1}:\n  1\n  4\n  7\n 10\n 13\n\n [gr.rowval gr.nzval]\n\n 12x2 Array{Float64,2}:\n 2.0  3.0\n 3.0  3.0\n 4.0  6.0\n 1.0  3.0\n 3.0  1.0\n 4.0  2.0\n 1.0  3.0\n 2.0  1.0\n 4.0  7.0\n 1.0  6.0\n 2.0  2.0\n 3.0  7.0", 
            "title": "Sparse matrices as graphs"
        }, 
        {
            "location": "/CSCgraph/index.html#using-sparse-matrices-as-graphs", 
            "text": "The routines  deg ,  nbri  and  weighti  will let you treat a sparse matrix like a graph.  deg(graph, u)  is the degree of node u.  nbri(graph, u, i)  is the ith neighbor of node u.  weighti(graph, u, i)  is the weight of the edge to the ith neighbor of node u.  Note that we start indexing from 1.  For example, to iterate over the neighbors of node v,   and play with the attached nodes, you could write code like:    for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end  But, this turns out to be much slower than working with the structure directly, like    for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end   [ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.", 
            "title": "Using sparse matrices as graphs"
        }, 
        {
            "location": "/CSCgraph/index.html#the-sparsematrixcsc-data-structure", 
            "text": "You can explore what is going on with the data structure by looking at some examples.  For example, here is a randomly weighted complete graph on 4 vertices, first displayed as a matrix:  gr = round(10*uniformWeight(completeGraph(4)))\n\n4x4 sparse matrix with 12 Float64 entries:\n    [2, 1]  =  3.0\n    [3, 1]  =  3.0\n    [4, 1]  =  6.0\n    [1, 2]  =  3.0\n    [3, 2]  =  1.0\n    [4, 2]  =  2.0\n    [1, 3]  =  3.0\n    [2, 3]  =  1.0\n    [4, 3]  =  7.0\n    [1, 4]  =  6.0\n    [2, 4]  =  2.0\n    [3, 4]  =  7.0\n\nfull(gr)\n\n4x4 Array{Float64,2}:\n 0.0  3.0  3.0  6.0\n 3.0  0.0  1.0  2.0\n 3.0  1.0  0.0  7.0\n 6.0  2.0  7.0  0.0  To see the underlying data structure, use  fieldnames .  fieldnames(gr)\n\n5-element Array{Symbol,1}:\n :m     \n :n     \n :colptr\n :rowval\n :nzval   m  and  n  are the dimensions of the matrix. The entries of the matrix are stored in nzval. colptr[i] is the index in nzval of the first nonzero entry in column i.  rowval tells you which rows in each column are nonzero. The indices of the nonzero entries in column i are stored in  rowval[colptr[i]] through rowval[colptr[i+1]-1].  gr.colptr \n\n5-element Array{Int64,1}:\n  1\n  4\n  7\n 10\n 13\n\n [gr.rowval gr.nzval]\n\n 12x2 Array{Float64,2}:\n 2.0  3.0\n 3.0  3.0\n 4.0  6.0\n 1.0  3.0\n 3.0  1.0\n 4.0  2.0\n 1.0  3.0\n 2.0  1.0\n 4.0  7.0\n 1.0  6.0\n 2.0  2.0\n 3.0  7.0", 
            "title": "The SparseMatrixCSC data structure"
        }, 
        {
            "location": "/LSST/index.html", 
            "text": "Low Stretch Spanning Trees\n\n\nWe have implemented a variant of the algorithm of Alon, Karp, Peleg and West for computing low stretch spanning trees.  It is called \nakpw\n.  For unweighted graphs we provide a faster variant called \nakpwU\n.  If you require faster algorithms, with possibly higher average stretch, you can try the heuristics \nrandishPrim\n or \nrandishKruskal\n.\n\n\nYou can compute the stretch of a graph with respect to a spanning tree with the routine \ncompStretches\n.  It returns a sparse matrix with one entry for each edge in the graph giving its stretch.  For example:\n\n\njulia\n graph = grid2(4)\n16x16 sparse matrix with 48 Float64 entries:\n\njulia\n tree = akpwU(graph)\n16x16 sparse matrix with 30 Float64 entries:\n\njulia\n st = compStretches(tree,graph)\n16x16 sparse matrix with 48 Float64 entries:\n    [2 ,  1]  =  1.0\n    [5 ,  1]  =  1.0\n    [1 ,  2]  =  1.0\n    [3 ,  2]  =  1.0\n    [6 ,  2]  =  3.0\n    [2 ,  3]  =  1.0\n    [4 ,  3]  =  1.0\n    [7 ,  3]  =  1.0\n    [3 ,  4]  =  1.0\n    [8 ,  4]  =  3.0\n    [1 ,  5]  =  1.0\n    [6 ,  5]  =  5.0\n    \u22ee\n    [8 , 12]  =  3.0\n    [11, 12]  =  1.0\n    [16, 12]  =  1.0\n    [9 , 13]  =  1.0\n    [14, 13]  =  3.0\n    [10, 14]  =  1.0\n    [13, 14]  =  3.0\n    [15, 14]  =  3.0\n    [11, 15]  =  1.0\n    [14, 15]  =  3.0\n    [16, 15]  =  3.0\n    [12, 16]  =  1.0\n    [15, 16]  =  3.0\n\n\n\n\nHere is an example demonstrating the average stretches and times taken by these algorithms on a large graph.\n\n\n\njulia\n graph = chimera(1000000,1);\n\njulia\n @time tree = akpw(graph);\n  5.700285 seconds (16.10 M allocations: 1.263 GB, 11.16% gc time)\n\njulia\n aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n8.793236275779616\n\njulia\n @time tree = randishPrim(graph);\n  3.736225 seconds (3.21 M allocations: 566.887 MB, 6.40% gc time)\n\njulia\n aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n10.800094649795756\n\njulia\n @time tree = randishKruskal(graph);\n  2.515443 seconds (3.21 M allocations: 423.529 MB, 4.35% gc time)\n\njulia\n aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n37.819948689847564\n\n\n\n\n\nOf course, you can get very different results on very different graphs.  But, the ordering of these algorithms respect to time and stretch will usually follow this pattern.", 
            "title": "Low Stretch Spanning Trees"
        }, 
        {
            "location": "/LSST/index.html#low-stretch-spanning-trees", 
            "text": "We have implemented a variant of the algorithm of Alon, Karp, Peleg and West for computing low stretch spanning trees.  It is called  akpw .  For unweighted graphs we provide a faster variant called  akpwU .  If you require faster algorithms, with possibly higher average stretch, you can try the heuristics  randishPrim  or  randishKruskal .  You can compute the stretch of a graph with respect to a spanning tree with the routine  compStretches .  It returns a sparse matrix with one entry for each edge in the graph giving its stretch.  For example:  julia  graph = grid2(4)\n16x16 sparse matrix with 48 Float64 entries:\n\njulia  tree = akpwU(graph)\n16x16 sparse matrix with 30 Float64 entries:\n\njulia  st = compStretches(tree,graph)\n16x16 sparse matrix with 48 Float64 entries:\n    [2 ,  1]  =  1.0\n    [5 ,  1]  =  1.0\n    [1 ,  2]  =  1.0\n    [3 ,  2]  =  1.0\n    [6 ,  2]  =  3.0\n    [2 ,  3]  =  1.0\n    [4 ,  3]  =  1.0\n    [7 ,  3]  =  1.0\n    [3 ,  4]  =  1.0\n    [8 ,  4]  =  3.0\n    [1 ,  5]  =  1.0\n    [6 ,  5]  =  5.0\n    \u22ee\n    [8 , 12]  =  3.0\n    [11, 12]  =  1.0\n    [16, 12]  =  1.0\n    [9 , 13]  =  1.0\n    [14, 13]  =  3.0\n    [10, 14]  =  1.0\n    [13, 14]  =  3.0\n    [15, 14]  =  3.0\n    [11, 15]  =  1.0\n    [14, 15]  =  3.0\n    [16, 15]  =  3.0\n    [12, 16]  =  1.0\n    [15, 16]  =  3.0  Here is an example demonstrating the average stretches and times taken by these algorithms on a large graph.  \njulia  graph = chimera(1000000,1);\n\njulia  @time tree = akpw(graph);\n  5.700285 seconds (16.10 M allocations: 1.263 GB, 11.16% gc time)\n\njulia  aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n8.793236275779616\n\njulia  @time tree = randishPrim(graph);\n  3.736225 seconds (3.21 M allocations: 566.887 MB, 6.40% gc time)\n\njulia  aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n10.800094649795756\n\njulia  @time tree = randishKruskal(graph);\n  2.515443 seconds (3.21 M allocations: 423.529 MB, 4.35% gc time)\n\njulia  aveStretch = sum(compStretches(tree,graph))/nnz(graph)\n37.819948689847564  Of course, you can get very different results on very different graphs.  But, the ordering of these algorithms respect to time and stretch will usually follow this pattern.", 
            "title": "Low Stretch Spanning Trees"
        }, 
        {
            "location": "/usingSolvers/index.html", 
            "text": "Solving linear equations in Laplacians\n\n\nFor some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.\n\n\n\n\nDirect Solvers\n\n\nYou can compute a cholesky factor directly with \ncholfact\n.  It does  more than just compute the factor, and it saves its result in a data structure that implements \n\\\n.  It uses SuiteSparse by Tim Davis.\n\n\nHere is an example of how you would use it to solve a general non-singular linear system.\n\n\na = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14\n\n\n\n\nLaplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.\n\n\nla = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15\n\n\n\n\nHere are two other ways of using the wrapper:\n\n\nlapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n);\nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15\n\n\n\n\n\n\nIterative Solvers\n\n\nThe first, of course, is the Conjugate Gradient (cg).\n\n\nOur implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance \ntol\n, the maximum number of iterations, \nmaxits\n, and the maximum number of seconds, \nmaxtime\n.  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  For extra information, set \nverbose\n to true.  Here are examples.\n\n\nsrand(1)\nn = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100,verbose=true);\n    CG stopped after: 66 iterations with relative error 4.901070762774203e-7.\nnorm(a*x - b)/norm(b)\n    4.901071329720876e-7\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\n    CG stopped after: 50 iterations with relative error 2.18672511297479336887519117065525148757254642683072581090418060286711737398731e-38.\nnorm(a*xbig - bbig)\n    2.186725112974793368875191170655251487433448469718749360094794057951719231569009e-38\n\n\n\n\nAs a sanity check, we do two speed tests against Matlab.\n\n\nla = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064\n\n\n\n\nAnd, in Matlab:\n\n\n a = grid2(200);\n\n la = lap(a);\n\n b = randn(length(a),1); b = b - mean(b);\n\n tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.\n\n norm(la*x-b)\n\nans =\n\n   1.9730e-04\n\n\n\n\nPCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.\n\n\nsrand(1)\na = mapweight(grid2(200),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000,verbose=true);\n\n    PCG stopped after: 2000 iterations with relative error 1.535193885968155e-5.\n      2.719655 seconds (42.41 k allocations: 1.194 GB, 6.21% gc time)\n\n\n\n\nIf our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.\n\n\n@time x = pcg(la,b,pre,tol=1e-1,maxits=10^5,verbose=true);\n    PCG stopped after: 452 iterations with relative error 0.09944363573171432.\n    0.649586 seconds (9.67 k allocations: 277.034 MB, 5.74% gc time) \nnorm(la*x - b)/norm(b)\n    0.09944363573090279\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5);\n    4.526119 seconds (16.03 k allocations: 1.195 GB, 4.16% gc time)\n\nnorm(la*x - b)/norm(b)\n    0.0981610595725004  \n\n\n\n\n\n\nLow-Stretch Spanning Trees\n\n\nIn order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code that is guaranteed to produce these.  Instead, we supply three heuristics: \nakpw\n which is inspired by the algorith of Alon, Karp, Peleg and West, and  randomized versions of Prim and Kruskall's algorithm. \nrandishKruskall\n samples the remaining edges with probability proportional to their weight.  \nrandishPrim\n samples edges on the boundary while using the same rule.\n\n\nSee \nLow Stretch Spanning Trees\n\n\nLet's try using a low-stretch spanning tree as a preconditioner for that last example.  First, we compute a low stretch tree and, for fun, check its average stretch.\n\n\n@time tree = akpw(a);\n    0.195370 seconds (1.07 M allocations: 86.283 MB, 5.84% gc time)\naveStretch = sum(compStretches(tree,a))/nnz(a)\n    6.527424129533183\n\n\n\n\nTo use it as a preconditioner, we must construct a function that inverts linear systems in its Laplacian:\n\n\nltree = lap(tree)\n@time ltreeSolver = lapChol(ltree);\n    0.045963 seconds (92 allocations: 17.703 MB, 9.51% gc time)\nnorm(ltree*ltreeSolver(b) - b)\n    8.209228285591316e-8\n\n\n\n\nNow, let's see how well it works as a preconditioner.\n\n\n@time x = pcg(la,b,ltreeSolver,tol=1e-1,maxits=10^5,verbose=true);\n    PCG stopped after: 153 iterations with relative error 0.09900131423570005.\n    0.658006 seconds (12.27 k allocations: 654.998 MB, 11.53% gc time)\n\n\n\n\nThat's a lot like using the diagonal preconditioner.  However, we see that the tree performs fewer iterations, and is faster when we seek higher accuracy.\n\n\n@time x = pcg(la,b,pre,tol=1e-6,maxtime=10,verbose=true);\n    PCG stopped after: 2435 iterations with relative error 9.92454258869586e-7.\n    3.431920 seconds (51.35 k allocations: 1.454 GB, 6.16% gc time)\n\n@time x = pcg(la,b,ltreeSolver,tol=1e-6,maxtime=10,verbose=true);\n    PCG stopped after: 605 iterations with relative error 9.78414046170984e-7.\n    2.510224 seconds (48.01 k allocations: 2.527 GB, 11.68% gc time)\n\n\n\n\n\n\nVarious Linear System Solvers\n\n\nWe have a blend of solvers geared towards solving symmetric linear systems. Some of these solvers, that have historically showcased good performance, have been grouped into two sections: SDDSolvers and LapSolvers.\n\n\nSDDSolvers\n5-element Array{Function,1}:\n  Laplacians.augTreeSolver    \n  Laplacians.KMPSDDSolver     \n  Laplacians.hybridSDDSolver  \n  Laplacians.samplingSDDSolver\n  Laplacians.AMGSolver\n\nLapSolvers\n5-element Array{Function,1}:\n  Laplacians.augTreeLapSolver \n  Laplacians.KMPLapSolver     \n  Laplacians.hybridLapSolver  \n  Laplacians.samplingLapSolver\n  Laplacians.AMGLapSolver \n\n\n\n\nThe SDD solvers take in a SDD matrix. They can also take in the usual tol, maxits and maxtime parameters. The Laplacian solvers, in contrast, are fed in an adjacency matrix. They are also subject to tol, maxits and maxtime. Following are examples of how to run these solvers on a given linear system.\n\n\na = mapweight(grid2(100),x-\n1/(rand(1)[1]));\nn = a.n\nla = lap(a); \ndval = zeros(n); dval[1] = dval[n] = 1e-3;\nsdd = la + spdiagm(dval);\nb = randn(n); b = b - mean(b);\n\n\n\n\nNow, to run the SDD solvers.\n\n\nfor solver in SDDSolvers\n    println(\nSolver \n, solver)\n    @time f = solver(sdd, maxits=1000, maxtime=10, tol=1e-4, verbose=false)\n    @time x = f(b);\n    println(\nRelative norm: \n, norm(sdd * x - b) / norm(b), \n\\n\n)\nend\n\nSolver Laplacians.augTreeSolver\n  0.067958 seconds (235.22 k allocations: 35.807 MB, 8.41% gc time)\n  0.060869 seconds (2.72 k allocations: 70.339 MB, 17.35% gc time)\nRelative norm: 9.250884271915595e-5\n\nSolver Laplacians.KMPSDDSolver\n  0.063388 seconds (254.46 k allocations: 44.168 MB, 14.65% gc time)\n  0.102330 seconds (8.82 k allocations: 133.219 MB, 18.42% gc time)\nRelative norm: 9.355785387633888e-5\n\nSolver Laplacians.hybridSDDSolver\n  0.064385 seconds (264.54 k allocations: 41.390 MB, 12.33% gc time)\n  0.379060 seconds (2.07 M allocations: 266.961 MB, 9.90% gc time)\nRelative norm: 9.832304243953024e-5\n\nSolver Laplacians.samplingSDDSolver\n  0.135522 seconds (706.74 k allocations: 101.318 MB, 13.69% gc time)\n  0.179568 seconds (3.14 M allocations: 144.395 MB, 13.88% gc time)\nRelative norm: 9.95716064856691e-5\n\nSolver Laplacians.AMGSolver\n  0.030372 seconds (166 allocations: 860.125 KB)\n  0.116332 seconds (1.55 k allocations: 4.400 MB)\nRelative norm: 9.519986283623698e-5\n\n\n\n\nNext, the Laplacian solvers.\n\n\nfor solver in LapSolvers\n    println(\nSolver \n, solver)\n    @time f = solver(a, maxits=1000, maxtime=10, tol=1e-4, verbose=false)\n    @time x = f(b);\n    println(\nRelative norm: \n, norm(sdd * x - b) / norm(b), \n\\n\n)\nend\n\nSolver Laplacians.augTreeLapSolver\n  0.072029 seconds (235.32 k allocations: 38.405 MB, 12.87% gc time)\n  0.088307 seconds (6.51 k allocations: 98.563 MB, 18.56% gc time)\nRelative norm: 0.00015469840172506333\n\nSolver Laplacians.KMPLapSolver\n  0.054602 seconds (254.18 k allocations: 39.445 MB, 15.07% gc time)\n  0.107957 seconds (8.38 k allocations: 126.549 MB, 21.16% gc time)\nRelative norm: 0.0001558714652838137\n\nSolver Laplacians.hybridLapSolver\n  0.053575 seconds (267.23 k allocations: 36.942 MB, 6.97% gc time)\n  0.407590 seconds (3.02 M allocations: 286.329 MB, 10.58% gc time)\nRelative norm: 0.00015420023270231535\n\nSolver Laplacians.samplingLapSolver\n  0.119625 seconds (706.35 k allocations: 90.641 MB, 21.37% gc time)\n  0.218231 seconds (3.52 M allocations: 161.625 MB, 14.20% gc time)\nRelative norm: 0.00015470313398451233\n\nSolver Laplacians.AMGLapSolver\n  0.043749 seconds (235 allocations: 2.744 MB)\n  0.179676 seconds (1.57 k allocations: 4.402 MB)\nRelative norm: 0.00014173955766018617", 
            "title": "Solvers"
        }, 
        {
            "location": "/usingSolvers/index.html#solving-linear-equations-in-laplacians", 
            "text": "For some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.", 
            "title": "Solving linear equations in Laplacians"
        }, 
        {
            "location": "/usingSolvers/index.html#direct-solvers", 
            "text": "You can compute a cholesky factor directly with  cholfact .  It does  more than just compute the factor, and it saves its result in a data structure that implements  \\ .  It uses SuiteSparse by Tim Davis.  Here is an example of how you would use it to solve a general non-singular linear system.  a = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14  Laplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.  la = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15  Here are two other ways of using the wrapper:  lapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n);\nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15", 
            "title": "Direct Solvers"
        }, 
        {
            "location": "/usingSolvers/index.html#iterative-solvers", 
            "text": "The first, of course, is the Conjugate Gradient (cg).  Our implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance  tol , the maximum number of iterations,  maxits , and the maximum number of seconds,  maxtime .  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  For extra information, set  verbose  to true.  Here are examples.  srand(1)\nn = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100,verbose=true);\n    CG stopped after: 66 iterations with relative error 4.901070762774203e-7.\nnorm(a*x - b)/norm(b)\n    4.901071329720876e-7\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\n    CG stopped after: 50 iterations with relative error 2.18672511297479336887519117065525148757254642683072581090418060286711737398731e-38.\nnorm(a*xbig - bbig)\n    2.186725112974793368875191170655251487433448469718749360094794057951719231569009e-38  As a sanity check, we do two speed tests against Matlab.  la = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064  And, in Matlab:   a = grid2(200);  la = lap(a);  b = randn(length(a),1); b = b - mean(b);  tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.  norm(la*x-b)\n\nans =\n\n   1.9730e-04  PCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.  srand(1)\na = mapweight(grid2(200),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000,verbose=true);\n\n    PCG stopped after: 2000 iterations with relative error 1.535193885968155e-5.\n      2.719655 seconds (42.41 k allocations: 1.194 GB, 6.21% gc time)  If our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.  @time x = pcg(la,b,pre,tol=1e-1,maxits=10^5,verbose=true);\n    PCG stopped after: 452 iterations with relative error 0.09944363573171432.\n    0.649586 seconds (9.67 k allocations: 277.034 MB, 5.74% gc time) \nnorm(la*x - b)/norm(b)\n    0.09944363573090279\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5);\n    4.526119 seconds (16.03 k allocations: 1.195 GB, 4.16% gc time)\n\nnorm(la*x - b)/norm(b)\n    0.0981610595725004", 
            "title": "Iterative Solvers"
        }, 
        {
            "location": "/usingSolvers/index.html#low-stretch-spanning-trees", 
            "text": "In order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code that is guaranteed to produce these.  Instead, we supply three heuristics:  akpw  which is inspired by the algorith of Alon, Karp, Peleg and West, and  randomized versions of Prim and Kruskall's algorithm.  randishKruskall  samples the remaining edges with probability proportional to their weight.   randishPrim  samples edges on the boundary while using the same rule.  See  Low Stretch Spanning Trees  Let's try using a low-stretch spanning tree as a preconditioner for that last example.  First, we compute a low stretch tree and, for fun, check its average stretch.  @time tree = akpw(a);\n    0.195370 seconds (1.07 M allocations: 86.283 MB, 5.84% gc time)\naveStretch = sum(compStretches(tree,a))/nnz(a)\n    6.527424129533183  To use it as a preconditioner, we must construct a function that inverts linear systems in its Laplacian:  ltree = lap(tree)\n@time ltreeSolver = lapChol(ltree);\n    0.045963 seconds (92 allocations: 17.703 MB, 9.51% gc time)\nnorm(ltree*ltreeSolver(b) - b)\n    8.209228285591316e-8  Now, let's see how well it works as a preconditioner.  @time x = pcg(la,b,ltreeSolver,tol=1e-1,maxits=10^5,verbose=true);\n    PCG stopped after: 153 iterations with relative error 0.09900131423570005.\n    0.658006 seconds (12.27 k allocations: 654.998 MB, 11.53% gc time)  That's a lot like using the diagonal preconditioner.  However, we see that the tree performs fewer iterations, and is faster when we seek higher accuracy.  @time x = pcg(la,b,pre,tol=1e-6,maxtime=10,verbose=true);\n    PCG stopped after: 2435 iterations with relative error 9.92454258869586e-7.\n    3.431920 seconds (51.35 k allocations: 1.454 GB, 6.16% gc time)\n\n@time x = pcg(la,b,ltreeSolver,tol=1e-6,maxtime=10,verbose=true);\n    PCG stopped after: 605 iterations with relative error 9.78414046170984e-7.\n    2.510224 seconds (48.01 k allocations: 2.527 GB, 11.68% gc time)", 
            "title": "Low-Stretch Spanning Trees"
        }, 
        {
            "location": "/usingSolvers/index.html#various-linear-system-solvers", 
            "text": "We have a blend of solvers geared towards solving symmetric linear systems. Some of these solvers, that have historically showcased good performance, have been grouped into two sections: SDDSolvers and LapSolvers.  SDDSolvers\n5-element Array{Function,1}:\n  Laplacians.augTreeSolver    \n  Laplacians.KMPSDDSolver     \n  Laplacians.hybridSDDSolver  \n  Laplacians.samplingSDDSolver\n  Laplacians.AMGSolver\n\nLapSolvers\n5-element Array{Function,1}:\n  Laplacians.augTreeLapSolver \n  Laplacians.KMPLapSolver     \n  Laplacians.hybridLapSolver  \n  Laplacians.samplingLapSolver\n  Laplacians.AMGLapSolver   The SDD solvers take in a SDD matrix. They can also take in the usual tol, maxits and maxtime parameters. The Laplacian solvers, in contrast, are fed in an adjacency matrix. They are also subject to tol, maxits and maxtime. Following are examples of how to run these solvers on a given linear system.  a = mapweight(grid2(100),x- 1/(rand(1)[1]));\nn = a.n\nla = lap(a); \ndval = zeros(n); dval[1] = dval[n] = 1e-3;\nsdd = la + spdiagm(dval);\nb = randn(n); b = b - mean(b);  Now, to run the SDD solvers.  for solver in SDDSolvers\n    println( Solver  , solver)\n    @time f = solver(sdd, maxits=1000, maxtime=10, tol=1e-4, verbose=false)\n    @time x = f(b);\n    println( Relative norm:  , norm(sdd * x - b) / norm(b),  \\n )\nend\n\nSolver Laplacians.augTreeSolver\n  0.067958 seconds (235.22 k allocations: 35.807 MB, 8.41% gc time)\n  0.060869 seconds (2.72 k allocations: 70.339 MB, 17.35% gc time)\nRelative norm: 9.250884271915595e-5\n\nSolver Laplacians.KMPSDDSolver\n  0.063388 seconds (254.46 k allocations: 44.168 MB, 14.65% gc time)\n  0.102330 seconds (8.82 k allocations: 133.219 MB, 18.42% gc time)\nRelative norm: 9.355785387633888e-5\n\nSolver Laplacians.hybridSDDSolver\n  0.064385 seconds (264.54 k allocations: 41.390 MB, 12.33% gc time)\n  0.379060 seconds (2.07 M allocations: 266.961 MB, 9.90% gc time)\nRelative norm: 9.832304243953024e-5\n\nSolver Laplacians.samplingSDDSolver\n  0.135522 seconds (706.74 k allocations: 101.318 MB, 13.69% gc time)\n  0.179568 seconds (3.14 M allocations: 144.395 MB, 13.88% gc time)\nRelative norm: 9.95716064856691e-5\n\nSolver Laplacians.AMGSolver\n  0.030372 seconds (166 allocations: 860.125 KB)\n  0.116332 seconds (1.55 k allocations: 4.400 MB)\nRelative norm: 9.519986283623698e-5  Next, the Laplacian solvers.  for solver in LapSolvers\n    println( Solver  , solver)\n    @time f = solver(a, maxits=1000, maxtime=10, tol=1e-4, verbose=false)\n    @time x = f(b);\n    println( Relative norm:  , norm(sdd * x - b) / norm(b),  \\n )\nend\n\nSolver Laplacians.augTreeLapSolver\n  0.072029 seconds (235.32 k allocations: 38.405 MB, 12.87% gc time)\n  0.088307 seconds (6.51 k allocations: 98.563 MB, 18.56% gc time)\nRelative norm: 0.00015469840172506333\n\nSolver Laplacians.KMPLapSolver\n  0.054602 seconds (254.18 k allocations: 39.445 MB, 15.07% gc time)\n  0.107957 seconds (8.38 k allocations: 126.549 MB, 21.16% gc time)\nRelative norm: 0.0001558714652838137\n\nSolver Laplacians.hybridLapSolver\n  0.053575 seconds (267.23 k allocations: 36.942 MB, 6.97% gc time)\n  0.407590 seconds (3.02 M allocations: 286.329 MB, 10.58% gc time)\nRelative norm: 0.00015420023270231535\n\nSolver Laplacians.samplingLapSolver\n  0.119625 seconds (706.35 k allocations: 90.641 MB, 21.37% gc time)\n  0.218231 seconds (3.52 M allocations: 161.625 MB, 14.20% gc time)\nRelative norm: 0.00015470313398451233\n\nSolver Laplacians.AMGLapSolver\n  0.043749 seconds (235 allocations: 2.744 MB)\n  0.179676 seconds (1.57 k allocations: 4.402 MB)\nRelative norm: 0.00014173955766018617", 
            "title": "Various Linear System Solvers"
        }, 
        {
            "location": "/Developing/index.html", 
            "text": "Developing Laplacians.jl\n\n\n\n\nLearn to use git\n\n\n\n\nIf you don't know anything about git, then just know that you should make a branch for you own code.  Type\n\n\n\n\ngit checkout -b MyName\n\n\n\n\n\n\nNow, read about Git.  I recommend the book \nPro Git\n, which is available online for free.\n\n\nStop thinking about Git like subversion or dropbox.\n\n\nThe master branch will be the one for public consumption. It should (mostly) work.\n\n\nYou should also read the\n\n\n\n\nsection of the Julia docs about building packages.\n\n\n\n\nFast code?\n\n\nJust go for it. Don't worry about writing fast code at first. Just get it to work. We can speed it up later.\n\n\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"\n\n\n\n\nDocumentation\n\n\nThis documentation is still very rough. It is generated by a combination of Markdown and semi-automatic generation, using the \nDocumenter.jl\n package.  The one annoying feature of this package is that it will not allow the inclusion of a docstring on more than one page.  I don't know why.\n\n\nThe steps to generate and improve the documentation are:\n\n\n\n\nEdit Markdown files in the \ndocs\n directory.  For example, you could use MacDown to do this.\n\n\nIf you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.\n\n\nAdd docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.\n\n\nRun \njulia make.jl; mkdocs build\n in the \ndocs\n directory to generate the documentation from the Markdown.\n\n\n\n\nWARNING\n: You should not include any pages that are generated in the git repository.  So, make sure that your .gitignore file contains the line \ndocs/build\n.\n\n\n\n\n\n\nOnce you like the documentation, you can upload it with\n\n\n\n\n\n\nmkdocs gh-deploy --clean -b gh-pages\n\n\n\n\n\n\nWarning:\n mkdocs deletes everying in gh-pages that it does not put there itself.\n\n\nIf you create a Julia notebook that you would like to include as documentation.   You should  put it in the notebooks directory (.julia/v0.5/Laplacians/notebooks) and then link to it's page on GitHub.  While it seems that one should convert it to html (and one can), and then include it in MkDocs, MkDocs does something funny to the resulting html that does not look nice.\n\n\n\n\n\n\nParametric Types\n\n\nA sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\n\n\n\n\nTv\n, sometimes written \nTval\n denotes the types of the values, and \nTi\n or \nTind\n denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths, \nstart\n is of type \nTi\n.  Inside the code, whenever we write something like \npArray = zeros(Ti,n)\n, it creates an array of zeros of type Ti.  Using these parameteric types is \nmuch\n faster than leaving the types unfixed.\n\n\n\n\nData structures:\n\n\n\n\nIntHeap\n a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.\n\n\n\n\n\n\nInterface issue:\n\n\nThere are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.\n\n\n\n\nIntegration with other packages.\n\n\nThere are other graph packages that we might want to sometimes use.\n\n\n\n\nGraphs.jl\n : I found this one to be too slow and awkward to be useful.\n\n\nLightGraphs.jl\n : this looks more promising.  We will have to check it out.  It is reasonably fast, and the code looks pretty.", 
            "title": "Devleoping Laplacians"
        }, 
        {
            "location": "/Developing/index.html#developing-laplaciansjl", 
            "text": "", 
            "title": "Developing Laplacians.jl"
        }, 
        {
            "location": "/Developing/index.html#learn-to-use-git", 
            "text": "If you don't know anything about git, then just know that you should make a branch for you own code.  Type   git checkout -b MyName   Now, read about Git.  I recommend the book  Pro Git , which is available online for free.  Stop thinking about Git like subversion or dropbox.  The master branch will be the one for public consumption. It should (mostly) work.  You should also read the   section of the Julia docs about building packages.", 
            "title": "Learn to use git"
        }, 
        {
            "location": "/Developing/index.html#fast-code", 
            "text": "Just go for it. Don't worry about writing fast code at first. Just get it to work. We can speed it up later.  Within some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"", 
            "title": "Fast code?"
        }, 
        {
            "location": "/Developing/index.html#documentation", 
            "text": "This documentation is still very rough. It is generated by a combination of Markdown and semi-automatic generation, using the  Documenter.jl  package.  The one annoying feature of this package is that it will not allow the inclusion of a docstring on more than one page.  I don't know why.  The steps to generate and improve the documentation are:   Edit Markdown files in the  docs  directory.  For example, you could use MacDown to do this.  If you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.  Add docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.  Run  julia make.jl; mkdocs build  in the  docs  directory to generate the documentation from the Markdown.   WARNING : You should not include any pages that are generated in the git repository.  So, make sure that your .gitignore file contains the line  docs/build .    Once you like the documentation, you can upload it with    mkdocs gh-deploy --clean -b gh-pages   Warning:  mkdocs deletes everying in gh-pages that it does not put there itself.  If you create a Julia notebook that you would like to include as documentation.   You should  put it in the notebooks directory (.julia/v0.5/Laplacians/notebooks) and then link to it's page on GitHub.  While it seems that one should convert it to html (and one can), and then include it in MkDocs, MkDocs does something funny to the resulting html that does not look nice.", 
            "title": "Documentation"
        }, 
        {
            "location": "/Developing/index.html#parametric-types", 
            "text": "A sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,  shortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)  Tv , sometimes written  Tval  denotes the types of the values, and  Ti  or  Tind  denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths,  start  is of type  Ti .  Inside the code, whenever we write something like  pArray = zeros(Ti,n) , it creates an array of zeros of type Ti.  Using these parameteric types is  much  faster than leaving the types unfixed.", 
            "title": "Parametric Types"
        }, 
        {
            "location": "/Developing/index.html#data-structures", 
            "text": "IntHeap  a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.", 
            "title": "Data structures:"
        }, 
        {
            "location": "/Developing/index.html#interface-issue", 
            "text": "There are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.", 
            "title": "Interface issue:"
        }, 
        {
            "location": "/Developing/index.html#integration-with-other-packages", 
            "text": "There are other graph packages that we might want to sometimes use.   Graphs.jl  : I found this one to be too slow and awkward to be useful.  LightGraphs.jl  : this looks more promising.  We will have to check it out.  It is reasonably fast, and the code looks pretty.", 
            "title": "Integration with other packages."
        }, 
        {
            "location": "/graphGenerators/index.html", 
            "text": "Generators\n\n\nLaplacians.jl\n implements generators for many standard graphs. The \nchimera\n and [\nwtedChimera\n])@ref) generators are designed to stress code by combining these standard graphs in tricky ways.  While no one of these graphs need be a hard case for any application, the goal is for these generators to explore the space of graphs in such a way that running on many of them should exercise your code.\n\n\nchimera(n)\n generates a random chimera graph. \nchimera(n,k)\n first sets the seed of the psrg to k. In this way, it generates the kth chimera graph, and messes with your psrg. \nwtedChimera\n is similar, but it generates weighted graphs.\n\n\n\n\nFunction list\n\n\n\n\nBase.Random.randperm\n\n\nLaplacians.ErdosRenyi\n\n\nLaplacians.ErdosRenyiCluster\n\n\nLaplacians.ErdosRenyiClusterFix\n\n\nLaplacians.chimera\n\n\nLaplacians.chimera\n\n\nLaplacians.completeBinaryTree\n\n\nLaplacians.completeGraph\n\n\nLaplacians.generalizedRing\n\n\nLaplacians.grid2\n\n\nLaplacians.grid2coords\n\n\nLaplacians.grid3\n\n\nLaplacians.grownGraph\n\n\nLaplacians.grownGraphD\n\n\nLaplacians.hyperCube\n\n\nLaplacians.pathGraph\n\n\nLaplacians.prefAttach\n\n\nLaplacians.pureRandomGraph\n\n\nLaplacians.randGenRing\n\n\nLaplacians.randMatching\n\n\nLaplacians.randRegular\n\n\nLaplacians.randWeight\n\n\nLaplacians.ringGraph\n\n\nLaplacians.semiWtedChimera\n\n\nLaplacians.wGrid2\n\n\nLaplacians.wGrid3\n\n\nLaplacians.wtedChimera\n\n\nLaplacians.wtedChimera\n\n\n\n\n#\n\n\nBase.Random.randperm\n \n \nMethod\n.\n\n\ngraph = randperm(mat::AbstractMatrix)\n        randperm(f::Expr)\n\n\n\n\nRandomly permutes the vertex indices\n\n\nsource\n\n\n#\n\n\nLaplacians.ErdosRenyi\n \n \nMethod\n.\n\n\ngraph = ErdosRenyi(n::Integer, m::Integer)\n\n\n\n\nGenerate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement\n\n\nsource\n\n\n#\n\n\nLaplacians.ErdosRenyiCluster\n \n \nMethod\n.\n\n\ngraph = ErdosRenyiCluster(n::Integer, k::Integer)\n\n\n\n\nGenerate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.\n\n\nsource\n\n\n#\n\n\nLaplacians.ErdosRenyiClusterFix\n \n \nMethod\n.\n\n\ngraph = ErdosRenyiClusterFix(n::Integer, k::Integer)\n\n\n\n\nLike an Erdos-Renyi cluster, but add back a tree so it has n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.chimera\n \n \nMethod\n.\n\n\ngraph = chimera(n::Integer, k::Integer)\n\n\n\n\nBuilds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nsource\n\n\n#\n\n\nLaplacians.chimera\n \n \nMethod\n.\n\n\ngraph = chimera(n::Integer)\n\n\n\n\nBuilds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace\n\n\nsource\n\n\n#\n\n\nLaplacians.completeBinaryTree\n \n \nMethod\n.\n\n\ngraph = completeBinaryTree(n::Int64)\n\n\n\n\nThe complete binary tree on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.completeGraph\n \n \nMethod\n.\n\n\ngraph = completeGraph(n::Int64)\n\n\n\n\nThe complete graph\n\n\nsource\n\n\n#\n\n\nLaplacians.generalizedRing\n \n \nMethod\n.\n\n\ngraph = generalizedRing(n::Int64, gens)\n\n\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example, \n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.grid2\n \n \nMethod\n.\n\n\ngraph = grid2(n::Int64, m::Int64; isotropy=1)\n\n\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\nsource\n\n\n#\n\n\nLaplacians.grid2coords\n \n \nMethod\n.\n\n\ngraph = grid2coords(n::Int64, m::Int64)\ngraph = grid2coords(n::Int64)\n\n\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\nsource\n\n\n#\n\n\nLaplacians.grid3\n \n \nMethod\n.\n\n\ngraph = grid3{Ti}(n1::Ti, n2::Ti, n3::Ti)\ngraph = grid3(n)\n\n\n\n\nAn n1-by-n2-by-n3 grid graph.\n\n\nsource\n\n\n#\n\n\nLaplacians.grownGraph\n \n \nMethod\n.\n\n\ngraph = grownGraph(n::Int64, k::Int64)\n\n\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.    \n\n\nsource\n\n\n#\n\n\nLaplacians.grownGraphD\n \n \nMethod\n.\n\n\ngraph = grownGraphD(n::Int64, k::Int64)\n\n\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.hyperCube\n \n \nMethod\n.\n\n\ngraph = hyperCube(d::Int64)\n\n\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.pathGraph\n \n \nMethod\n.\n\n\ngraph = pathGraph(n::Int64)\n\n\n\n\nThe path graph on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.prefAttach\n \n \nMethod\n.\n\n\ngraph = prefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.\n\n\nsource\n\n\n#\n\n\nLaplacians.pureRandomGraph\n \n \nMethod\n.\n\n\ngraph = pureRandomGraph(n::Integer)\n\n\n\n\nGenerate a random graph with n vertices from one of our natural distributions\n\n\nsource\n\n\n#\n\n\nLaplacians.randGenRing\n \n \nMethod\n.\n\n\ngraph = randGenRing(n::Int64, k::Integer)\n\n\n\n\nA random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution\n\n\nsource\n\n\n#\n\n\nLaplacians.randMatching\n \n \nMethod\n.\n\n\ngraph = randMatching(n::Int64)\n\n\n\n\nA random matching on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.randRegular\n \n \nMethod\n.\n\n\ngraph = randRegular(n::Int64, k::Int64)\n\n\n\n\nA sum of k random matchings on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.randWeight\n \n \nMethod\n.\n\n\ngraph = randWeight(graph)\n\n\n\n\nApplies one of a number of random weighting schemes to the edges of the graph\n\n\nsource\n\n\n#\n\n\nLaplacians.ringGraph\n \n \nMethod\n.\n\n\ngraph = ringGraph(n::Int64)\n\n\n\n\nThe simple ring on n vertices\n\n\nsource\n\n\n#\n\n\nLaplacians.semiWtedChimera\n \n \nMethod\n.\n\n\ngraph = semiWtedChimera(n::Integer)\n\n\n\n\nA Chimera graph with some weights.  The weights just appear when graphs are combined. For more interesting weights, use \nwtedChimera\n\n\nsource\n\n\n#\n\n\nLaplacians.wGrid2\n \n \nMethod\n.\n\n\ngraph = wGrid2(n::Int64; weightGen::Function=rand)\n\n\n\n\nAn n by n grid with random weights. User can specify the weighting scheme. \n\n\nsource\n\n\n#\n\n\nLaplacians.wGrid3\n \n \nMethod\n.\n\n\ngraph = wGrid3(n::Int64; weightGen::Function=rand)\n\n\n\n\nAn n^3 grid with random weights. User can specify the weighting scheme. \n\n\nsource\n\n\n#\n\n\nLaplacians.wtedChimera\n \n \nMethod\n.\n\n\ngraph = wtedChimera(n::Integer, k::Integer)\n\n\n\n\nBuilds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nsource\n\n\n#\n\n\nLaplacians.wtedChimera\n \n \nMethod\n.\n\n\ngraph = wtedChimera(n::Integer)\n\n\n\n\nGenerate a chimera, and then apply a random weighting scheme\n\n\nsource", 
            "title": "generators"
        }, 
        {
            "location": "/graphGenerators/index.html#generators", 
            "text": "Laplacians.jl  implements generators for many standard graphs. The  chimera  and [ wtedChimera ])@ref) generators are designed to stress code by combining these standard graphs in tricky ways.  While no one of these graphs need be a hard case for any application, the goal is for these generators to explore the space of graphs in such a way that running on many of them should exercise your code.  chimera(n)  generates a random chimera graph.  chimera(n,k)  first sets the seed of the psrg to k. In this way, it generates the kth chimera graph, and messes with your psrg.  wtedChimera  is similar, but it generates weighted graphs.", 
            "title": "Generators"
        }, 
        {
            "location": "/graphGenerators/index.html#function-list", 
            "text": "Base.Random.randperm  Laplacians.ErdosRenyi  Laplacians.ErdosRenyiCluster  Laplacians.ErdosRenyiClusterFix  Laplacians.chimera  Laplacians.chimera  Laplacians.completeBinaryTree  Laplacians.completeGraph  Laplacians.generalizedRing  Laplacians.grid2  Laplacians.grid2coords  Laplacians.grid3  Laplacians.grownGraph  Laplacians.grownGraphD  Laplacians.hyperCube  Laplacians.pathGraph  Laplacians.prefAttach  Laplacians.pureRandomGraph  Laplacians.randGenRing  Laplacians.randMatching  Laplacians.randRegular  Laplacians.randWeight  Laplacians.ringGraph  Laplacians.semiWtedChimera  Laplacians.wGrid2  Laplacians.wGrid3  Laplacians.wtedChimera  Laplacians.wtedChimera   #  Base.Random.randperm     Method .  graph = randperm(mat::AbstractMatrix)\n        randperm(f::Expr)  Randomly permutes the vertex indices  source  #  Laplacians.ErdosRenyi     Method .  graph = ErdosRenyi(n::Integer, m::Integer)  Generate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement  source  #  Laplacians.ErdosRenyiCluster     Method .  graph = ErdosRenyiCluster(n::Integer, k::Integer)  Generate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.  source  #  Laplacians.ErdosRenyiClusterFix     Method .  graph = ErdosRenyiClusterFix(n::Integer, k::Integer)  Like an Erdos-Renyi cluster, but add back a tree so it has n vertices  source  #  Laplacians.chimera     Method .  graph = chimera(n::Integer, k::Integer)  Builds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  source  #  Laplacians.chimera     Method .  graph = chimera(n::Integer)  Builds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace  source  #  Laplacians.completeBinaryTree     Method .  graph = completeBinaryTree(n::Int64)  The complete binary tree on n vertices  source  #  Laplacians.completeGraph     Method .  graph = completeGraph(n::Int64)  The complete graph  source  #  Laplacians.generalizedRing     Method .  graph = generalizedRing(n::Int64, gens)  A generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example,   generalizedRing(17, [1 5])  source  #  Laplacians.grid2     Method .  graph = grid2(n::Int64, m::Int64; isotropy=1)  An n-by-m grid graph.  iostropy is the weighting on edges in one direction.  source  #  Laplacians.grid2coords     Method .  graph = grid2coords(n::Int64, m::Int64)\ngraph = grid2coords(n::Int64)  Coordinates for plotting the vertices of the n-by-m grid graph  source  #  Laplacians.grid3     Method .  graph = grid3{Ti}(n1::Ti, n2::Ti, n3::Ti)\ngraph = grid3(n)  An n1-by-n2-by-n3 grid graph.  source  #  Laplacians.grownGraph     Method .  graph = grownGraph(n::Int64, k::Int64)  Create a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.      source  #  Laplacians.grownGraphD     Method .  graph = grownGraphD(n::Int64, k::Int64)  Like a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices  source  #  Laplacians.hyperCube     Method .  graph = hyperCube(d::Int64)  The d dimensional hypercube.  Has 2^d vertices  source  #  Laplacians.pathGraph     Method .  graph = pathGraph(n::Int64)  The path graph on n vertices  source  #  Laplacians.prefAttach     Method .  graph = prefAttach(n::Int64, k::Int64, p::Float64)  A preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.  source  #  Laplacians.pureRandomGraph     Method .  graph = pureRandomGraph(n::Integer)  Generate a random graph with n vertices from one of our natural distributions  source  #  Laplacians.randGenRing     Method .  graph = randGenRing(n::Int64, k::Integer)  A random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution  source  #  Laplacians.randMatching     Method .  graph = randMatching(n::Int64)  A random matching on n vertices  source  #  Laplacians.randRegular     Method .  graph = randRegular(n::Int64, k::Int64)  A sum of k random matchings on n vertices  source  #  Laplacians.randWeight     Method .  graph = randWeight(graph)  Applies one of a number of random weighting schemes to the edges of the graph  source  #  Laplacians.ringGraph     Method .  graph = ringGraph(n::Int64)  The simple ring on n vertices  source  #  Laplacians.semiWtedChimera     Method .  graph = semiWtedChimera(n::Integer)  A Chimera graph with some weights.  The weights just appear when graphs are combined. For more interesting weights, use  wtedChimera  source  #  Laplacians.wGrid2     Method .  graph = wGrid2(n::Int64; weightGen::Function=rand)  An n by n grid with random weights. User can specify the weighting scheme.   source  #  Laplacians.wGrid3     Method .  graph = wGrid3(n::Int64; weightGen::Function=rand)  An n^3 grid with random weights. User can specify the weighting scheme.   source  #  Laplacians.wtedChimera     Method .  graph = wtedChimera(n::Integer, k::Integer)  Builds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  source  #  Laplacians.wtedChimera     Method .  graph = wtedChimera(n::Integer)  Generate a chimera, and then apply a random weighting scheme  source", 
            "title": "Function list"
        }, 
        {
            "location": "/operators/index.html", 
            "text": "Operators\n\n\nOperators transform graphs to produce new graphs.\n\n\n\n\nFunction list\n\n\n\n\nBase.Random.randperm\n\n\nLaplacians.ErdosRenyi\n\n\nLaplacians.ErdosRenyiCluster\n\n\nLaplacians.ErdosRenyiClusterFix\n\n\nLaplacians.chimera\n\n\nLaplacians.chimera\n\n\nLaplacians.completeBinaryTree\n\n\nLaplacians.completeGraph\n\n\nLaplacians.generalizedRing\n\n\nLaplacians.grid2\n\n\nLaplacians.grid2coords\n\n\nLaplacians.grid3\n\n\nLaplacians.grownGraph\n\n\nLaplacians.grownGraphD\n\n\nLaplacians.hyperCube\n\n\nLaplacians.pathGraph\n\n\nLaplacians.prefAttach\n\n\nLaplacians.pureRandomGraph\n\n\nLaplacians.randGenRing\n\n\nLaplacians.randMatching\n\n\nLaplacians.randRegular\n\n\nLaplacians.randWeight\n\n\nLaplacians.ringGraph\n\n\nLaplacians.semiWtedChimera\n\n\nLaplacians.wGrid2\n\n\nLaplacians.wGrid3\n\n\nLaplacians.wtedChimera\n\n\nLaplacians.wtedChimera\n\n\n\n\n#\n\n\nLaplacians.adj\n \n \nMethod\n.\n\n\nCreate an adjacency matrix and a diagonal vector from a Laplacian with added diagonal weights\n\n\nsource\n\n\n#\n\n\nLaplacians.diagmat\n \n \nMethod\n.\n\n\nReturns the diagonal matrix(as a sparse matrix) of a graph\n\n\nsource\n\n\n#\n\n\nLaplacians.disjoin\n \n \nMethod\n.\n\n\nCreate a disjoint union of graphs a and b,   with no edges between them.\n\n\nsource\n\n\n#\n\n\nLaplacians.edgeVertexMat\n \n \nMethod\n.\n\n\nThe signed edge-vertex adjacency matrix\n\n\nsource\n\n\n#\n\n\nLaplacians.floatGraph\n \n \nMethod\n.\n\n\nConvert the nonzero entries in a graph to Float64\n\n\nsource\n\n\n#\n\n\nLaplacians.generalizedNecklace\n \n \nMethod\n.\n\n\ngeneralizedNecklace{Tv, Ti}(A::SparseMatrixCSC{Tv, Ti}, H::SparseMatrixCSC, k::Int64)\n\n\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.\n\n\nsource\n\n\n#\n\n\nLaplacians.joinGraphs\n \n \nMethod\n.\n\n\nCreate a disjoint union of graphs a and b,  and then put k random edges between them\n\n\nsource\n\n\n#\n\n\nLaplacians.lap\n \n \nMethod\n.\n\n\nCreate a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry\n\n\nsource\n\n\n#\n\n\nLaplacians.mapweight\n \n \nMethod\n.\n\n\nCreate a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write\n\n\nb = mapweight(a, x-\nrand(1)[1])\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.plotGraph\n \n \nFunction\n.\n\n\nPlots graph gr with coordinates (x,y)\n\n\nsource\n\n\n#\n\n\nLaplacians.productGraph\n \n \nMethod\n.\n\n\nThe Cartesian product of two graphs.  When applied to two paths, it gives a grid.\n\n\nsource\n\n\n#\n\n\nLaplacians.shortIntGraph\n \n \nMethod\n.\n\n\nConvert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much\n\n\nsource\n\n\n#\n\n\nLaplacians.spectralCoords\n \n \nMethod\n.\n\n\nComputes the spectral coordinates of a graph\n\n\nsource\n\n\n#\n\n\nLaplacians.spectralDrawing\n \n \nMethod\n.\n\n\nComputes spectral coordinates, and then uses plotGraph to draw\n\n\nsource\n\n\n#\n\n\nLaplacians.subsampleEdges\n \n \nMethod\n.\n\n\nCreate a new graph from the old, but keeping edge edge with probability \np\n\n\nsource\n\n\n#\n\n\nLaplacians.twoLift\n \n \nMethod\n.\n\n\nCreats a 2-lift of a.  \nflip\n is a boolean indicating which edges cross\n\n\nsource\n\n\n#\n\n\nLaplacians.uniformWeight!\n \n \nMethod\n.\n\n\nSet the weight of every edge to random uniform [0,1]\n\n\nsource\n\n\n#\n\n\nLaplacians.uniformWeight\n \n \nMethod\n.\n\n\nPut a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.\n\n\nsource\n\n\n#\n\n\nLaplacians.unweight!\n \n \nMethod\n.\n\n\nChange the weight of every edge in a to 1\n\n\nsource\n\n\n#\n\n\nLaplacians.unweight\n \n \nMethod\n.\n\n\nCreate a new graph in that is the same as the original, but with all edge weights 1\n\n\nsource", 
            "title": "operators"
        }, 
        {
            "location": "/operators/index.html#operators", 
            "text": "Operators transform graphs to produce new graphs.", 
            "title": "Operators"
        }, 
        {
            "location": "/operators/index.html#function-list", 
            "text": "Base.Random.randperm  Laplacians.ErdosRenyi  Laplacians.ErdosRenyiCluster  Laplacians.ErdosRenyiClusterFix  Laplacians.chimera  Laplacians.chimera  Laplacians.completeBinaryTree  Laplacians.completeGraph  Laplacians.generalizedRing  Laplacians.grid2  Laplacians.grid2coords  Laplacians.grid3  Laplacians.grownGraph  Laplacians.grownGraphD  Laplacians.hyperCube  Laplacians.pathGraph  Laplacians.prefAttach  Laplacians.pureRandomGraph  Laplacians.randGenRing  Laplacians.randMatching  Laplacians.randRegular  Laplacians.randWeight  Laplacians.ringGraph  Laplacians.semiWtedChimera  Laplacians.wGrid2  Laplacians.wGrid3  Laplacians.wtedChimera  Laplacians.wtedChimera   #  Laplacians.adj     Method .  Create an adjacency matrix and a diagonal vector from a Laplacian with added diagonal weights  source  #  Laplacians.diagmat     Method .  Returns the diagonal matrix(as a sparse matrix) of a graph  source  #  Laplacians.disjoin     Method .  Create a disjoint union of graphs a and b,   with no edges between them.  source  #  Laplacians.edgeVertexMat     Method .  The signed edge-vertex adjacency matrix  source  #  Laplacians.floatGraph     Method .  Convert the nonzero entries in a graph to Float64  source  #  Laplacians.generalizedNecklace     Method .  generalizedNecklace{Tv, Ti}(A::SparseMatrixCSC{Tv, Ti}, H::SparseMatrixCSC, k::Int64)  Constructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.  source  #  Laplacians.joinGraphs     Method .  Create a disjoint union of graphs a and b,  and then put k random edges between them  source  #  Laplacians.lap     Method .  Create a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry  source  #  Laplacians.mapweight     Method .  Create a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write  b = mapweight(a, x- rand(1)[1])  source  #  Laplacians.plotGraph     Function .  Plots graph gr with coordinates (x,y)  source  #  Laplacians.productGraph     Method .  The Cartesian product of two graphs.  When applied to two paths, it gives a grid.  source  #  Laplacians.shortIntGraph     Method .  Convert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much  source  #  Laplacians.spectralCoords     Method .  Computes the spectral coordinates of a graph  source  #  Laplacians.spectralDrawing     Method .  Computes spectral coordinates, and then uses plotGraph to draw  source  #  Laplacians.subsampleEdges     Method .  Create a new graph from the old, but keeping edge edge with probability  p  source  #  Laplacians.twoLift     Method .  Creats a 2-lift of a.   flip  is a boolean indicating which edges cross  source  #  Laplacians.uniformWeight!     Method .  Set the weight of every edge to random uniform [0,1]  source  #  Laplacians.uniformWeight     Method .  Put a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.  source  #  Laplacians.unweight!     Method .  Change the weight of every edge in a to 1  source  #  Laplacians.unweight     Method .  Create a new graph in that is the same as the original, but with all edge weights 1  source", 
            "title": "Function list"
        }, 
        {
            "location": "/graphUtils/index.html", 
            "text": "Graph Utilities\n\n\nThese are utilities to facilitate the use of sparse matrices as graphs.\n\n\n\n\nLaplacians.backIndices\n\n\nLaplacians.backIndices\n\n\nLaplacians.compConductance\n\n\nLaplacians.findEntries\n\n\nLaplacians.flipIndex\n\n\nLaplacians.getObound\n\n\nLaplacians.getVolume\n\n\nLaplacians.setValue\n\n\nLaplacians.wdeg\n\n\n\n\n#\n\n\nLaplacians.backIndices\n \n \nMethod\n.\n\n\nSame as the above, but now the graph is in adjacency list form \n\n\nsource\n\n\n#\n\n\nLaplacians.backIndices\n \n \nMethod\n.\n\n\nComputes the back indices in a graph in O(M+N). works if for every edge (u,v), (v,u) is also in the graph \n\n\nsource\n\n\n#\n\n\nLaplacians.compConductance\n \n \nMethod\n.\n\n\nReturns the quality of the cut for a given graph and a given cut set s.   the result will be |outgoing edges| / min(|vertices in set|, |N - vertices in set|)\n\n\nsource\n\n\n#\n\n\nLaplacians.findEntries\n \n \nMethod\n.\n\n\nSimilar to findnz, but also returns 0 entries that have an edge in the sparse matrix \n\n\nsource\n\n\n#\n\n\nLaplacians.flipIndex\n \n \nMethod\n.\n\n\nFor a symmetric matrix, this gives the correspondance between pairs of entries in an ijv. So, ai[ind] = aj[flip[ind]].  For example, \n\n\n(ai,aj,av) = findnz(a);\nfl = flipIndex(a)\nind = 10\n@show backind = fl[10]\n@show [ai[ind], aj[ind], av[ind]]\n@show [ai[backind], aj[backind], av[backind]];\n\nbackind = fl[10] = 4\n[ai[ind],aj[ind],av[ind]] = [2.0,4.0,0.7]\n[ai[backind],aj[backind],av[backind]] = [4.0,2.0,0.7]\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.getObound\n \n \nMethod\n.\n\n\nComputes the number of edges leaving s \n\n\nsource\n\n\n#\n\n\nLaplacians.getVolume\n \n \nMethod\n.\n\n\nComputes the volume of subset s in an unweighted graph G \n\n\nsource\n\n\n#\n\n\nLaplacians.setValue\n \n \nMethod\n.\n\n\nSets the value of a certain edge in a sparse graph; value can be 0 without the edges dissapearing \n\n\nsource\n\n\n#\n\n\nLaplacians.wdeg\n \n \nMethod\n.\n\n\nFinds the weighted degree of a vertex in the graph \n\n\nsource", 
            "title": "graphUtils"
        }, 
        {
            "location": "/graphUtils/index.html#graph-utilities", 
            "text": "These are utilities to facilitate the use of sparse matrices as graphs.   Laplacians.backIndices  Laplacians.backIndices  Laplacians.compConductance  Laplacians.findEntries  Laplacians.flipIndex  Laplacians.getObound  Laplacians.getVolume  Laplacians.setValue  Laplacians.wdeg   #  Laplacians.backIndices     Method .  Same as the above, but now the graph is in adjacency list form   source  #  Laplacians.backIndices     Method .  Computes the back indices in a graph in O(M+N). works if for every edge (u,v), (v,u) is also in the graph   source  #  Laplacians.compConductance     Method .  Returns the quality of the cut for a given graph and a given cut set s.   the result will be |outgoing edges| / min(|vertices in set|, |N - vertices in set|)  source  #  Laplacians.findEntries     Method .  Similar to findnz, but also returns 0 entries that have an edge in the sparse matrix   source  #  Laplacians.flipIndex     Method .  For a symmetric matrix, this gives the correspondance between pairs of entries in an ijv. So, ai[ind] = aj[flip[ind]].  For example,   (ai,aj,av) = findnz(a);\nfl = flipIndex(a)\nind = 10\n@show backind = fl[10]\n@show [ai[ind], aj[ind], av[ind]]\n@show [ai[backind], aj[backind], av[backind]];\n\nbackind = fl[10] = 4\n[ai[ind],aj[ind],av[ind]] = [2.0,4.0,0.7]\n[ai[backind],aj[backind],av[backind]] = [4.0,2.0,0.7]  source  #  Laplacians.getObound     Method .  Computes the number of edges leaving s   source  #  Laplacians.getVolume     Method .  Computes the volume of subset s in an unweighted graph G   source  #  Laplacians.setValue     Method .  Sets the value of a certain edge in a sparse graph; value can be 0 without the edges dissapearing   source  #  Laplacians.wdeg     Method .  Finds the weighted degree of a vertex in the graph   source", 
            "title": "Graph Utilities"
        }, 
        {
            "location": "/graphAlgs/index.html", 
            "text": "Graph Algorithms\n\n\nThese are basic graph algorithms.\n\n\n\n\nFunction list\n\n\n\n\nLaplacians.biggestComp\n\n\nLaplacians.components\n\n\nLaplacians.isConnected\n\n\nLaplacians.kruskal\n\n\nLaplacians.prim\n\n\nLaplacians.shortestPathTree\n\n\nLaplacians.shortestPaths\n\n\nLaplacians.vecToComps\n\n\n\n\n#\n\n\nLaplacians.biggestComp\n \n \nMethod\n.\n\n\nReturn the biggest component in a graph, as a graph\n\n\nsource\n\n\n#\n\n\nLaplacians.components\n \n \nMethod\n.\n\n\nComputes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,\n\n\ngr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.isConnected\n \n \nMethod\n.\n\n\nReturns true if graph is connected.  Calls components.\n\n\nsource\n\n\n#\n\n\nLaplacians.kruskal\n \n \nMethod\n.\n\n\n(kruskal::SparseMatrixCSC; kind=:max)\n Uses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:min if you want the min spanning tree. It returns it a a graph\n\n\nsource\n\n\n#\n\n\nLaplacians.prim\n \n \nMethod\n.\n\n\nprim(mat::SparseMatrixCSC; kind=:max)\n Compute a maximum spanning tree of the matrix \nmat\n.   If \nkind=:min\n, computes a minimum spanning tree.\n\n\nsource\n\n\n#\n\n\nLaplacians.shortestPathTree\n \n \nMethod\n.\n\n\nComputes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:\n\n\na = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.shortestPaths\n \n \nMethod\n.\n\n\nComputes the lenghts of shortest paths from \nstart\n. Returns both a vector of the lenghts, and the parent array in the shortest path tree.\n\n\nThis algorithm treats edge weights as reciprocals of distances. DOC BETTER\n\n\nsource\n\n\n#\n\n\nLaplacians.vecToComps\n \n \nMethod\n.\n\n\nThis turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,\n\n\ncomps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]\n\n\n\n\nsource", 
            "title": "graphAlgs"
        }, 
        {
            "location": "/graphAlgs/index.html#graph-algorithms", 
            "text": "These are basic graph algorithms.", 
            "title": "Graph Algorithms"
        }, 
        {
            "location": "/graphAlgs/index.html#function-list", 
            "text": "Laplacians.biggestComp  Laplacians.components  Laplacians.isConnected  Laplacians.kruskal  Laplacians.prim  Laplacians.shortestPathTree  Laplacians.shortestPaths  Laplacians.vecToComps   #  Laplacians.biggestComp     Method .  Return the biggest component in a graph, as a graph  source  #  Laplacians.components     Method .  Computes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,  gr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2  source  #  Laplacians.isConnected     Method .  Returns true if graph is connected.  Calls components.  source  #  Laplacians.kruskal     Method .  (kruskal::SparseMatrixCSC; kind=:max)  Uses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:min if you want the min spanning tree. It returns it a a graph  source  #  Laplacians.prim     Method .  prim(mat::SparseMatrixCSC; kind=:max)  Compute a maximum spanning tree of the matrix  mat .   If  kind=:min , computes a minimum spanning tree.  source  #  Laplacians.shortestPathTree     Method .  Computes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:  a = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0  source  #  Laplacians.shortestPaths     Method .  Computes the lenghts of shortest paths from  start . Returns both a vector of the lenghts, and the parent array in the shortest path tree.  This algorithm treats edge weights as reciprocals of distances. DOC BETTER  source  #  Laplacians.vecToComps     Method .  This turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,  comps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]  source", 
            "title": "Function list"
        }, 
        {
            "location": "/IO/index.html", 
            "text": "IO\n\n\n#\n\n\nLaplacians.readIJ\n \n \nFunction\n.\n\n\nTo read a simple edge list, each line being an (i, j) pair\n\n\nsource\n\n\n#\n\n\nLaplacians.readIJV\n \n \nMethod\n.\n\n\nTo read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this\n\n\n [ai,aj,av] = find(triu(a));\n\n dlmwrite('graph.txt',[ai,aj,av],'precision',9);\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.writeIJV\n \n \nMethod\n.\n\n\nWrites the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:\n\n\n dl = dlmread('graph.txt');\n\n a = sparse(dl(:,1),dl(:,2),dl(:,3));\n\n n = max(size(a))\n\n a(n,n) = 0;\n\n a = a + a';\n\n\n\n\nsource", 
            "title": "IO"
        }, 
        {
            "location": "/IO/index.html#io", 
            "text": "#  Laplacians.readIJ     Function .  To read a simple edge list, each line being an (i, j) pair  source  #  Laplacians.readIJV     Method .  To read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this   [ai,aj,av] = find(triu(a));  dlmwrite('graph.txt',[ai,aj,av],'precision',9);  source  #  Laplacians.writeIJV     Method .  Writes the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:   dl = dlmread('graph.txt');  a = sparse(dl(:,1),dl(:,2),dl(:,3));  n = max(size(a))  a(n,n) = 0;  a = a + a';  source", 
            "title": "IO"
        }, 
        {
            "location": "/pcg/index.html", 
            "text": "CG and PCG\n\n\nThese are custom implementations of CG and PCG, and variants specialized for Laplacians.  They satisfy the interfaces of all solvers in this package.  They call BLAS when they can. But, they also handle more general input types.\n\n\n#\n\n\nLaplacians.cg\n \n \nFunction\n.\n\n\ncg(mat, b; tol, maxits, maxtime, verbose)\n solves a symmetric linear system. \ntol\n is set to 1e-6 by default, \nmaxits\n defaults to Inf \nmaxtime\n defaults to Inf.  It measures seconds. \nverbose\n defaults to false\n\n\nsource\n\n\n#\n\n\nLaplacians.cgSolver\n \n \nFunction\n.\n\n\ncgSolver(mat; tol, maxits, maxtime, verbose)\n creates a solver for a PSD system \nmat\n. The parameters are as described in cg.\n\n\nsource\n\n\n#\n\n\nLaplacians.pcg\n \n \nFunction\n.\n\n\npcg(mat, b, pre; tol, maxits, maxtime, verbose)\n solves a symmetric linear system using preconditioner \npre\n. \npre\n can be a function or a matrix.  If a matrix, a function to solve it is created with cholFact. \ntol\n is set to 1e-6 by default, \nmaxits\n defaults to Inf \nmaxtime\n defaults to Inf.  It measures seconds. \nverbose\n defaults to false\n\n\nsource\n\n\n#\n\n\nLaplacians.pcgLapSolver\n \n \nMethod\n.\n\n\nCreate a solver that uses cg to solve Laplacian systems in mat. Specialized for the case when pre is a Laplacian matrix.  Fix the default parameters of the solver as given\n\n\nsource\n\n\n#\n\n\nLaplacians.pcgSolver\n \n \nFunction\n.\n\n\npcgSolver(mat, pre; tol, maxits, maxtime, verbose)\n creates a solver for a PSD system using preconditioner \npre\n. The parameters are as described in pcg.\n\n\nsource", 
            "title": "pcg"
        }, 
        {
            "location": "/pcg/index.html#cg-and-pcg", 
            "text": "These are custom implementations of CG and PCG, and variants specialized for Laplacians.  They satisfy the interfaces of all solvers in this package.  They call BLAS when they can. But, they also handle more general input types.  #  Laplacians.cg     Function .  cg(mat, b; tol, maxits, maxtime, verbose)  solves a symmetric linear system.  tol  is set to 1e-6 by default,  maxits  defaults to Inf  maxtime  defaults to Inf.  It measures seconds.  verbose  defaults to false  source  #  Laplacians.cgSolver     Function .  cgSolver(mat; tol, maxits, maxtime, verbose)  creates a solver for a PSD system  mat . The parameters are as described in cg.  source  #  Laplacians.pcg     Function .  pcg(mat, b, pre; tol, maxits, maxtime, verbose)  solves a symmetric linear system using preconditioner  pre .  pre  can be a function or a matrix.  If a matrix, a function to solve it is created with cholFact.  tol  is set to 1e-6 by default,  maxits  defaults to Inf  maxtime  defaults to Inf.  It measures seconds.  verbose  defaults to false  source  #  Laplacians.pcgLapSolver     Method .  Create a solver that uses cg to solve Laplacian systems in mat. Specialized for the case when pre is a Laplacian matrix.  Fix the default parameters of the solver as given  source  #  Laplacians.pcgSolver     Function .  pcgSolver(mat, pre; tol, maxits, maxtime, verbose)  creates a solver for a PSD system using preconditioner  pre . The parameters are as described in pcg.  source", 
            "title": "CG and PCG"
        }, 
        {
            "location": "/treeAlgs/index.html", 
            "text": "Tree Algorithms\n\n\n\n\nLaplacians.compStretches\n\n\n\n\n#\n\n\nLaplacians.compStretches\n \n \nMethod\n.\n\n\nCompute the stretched of every edge in \nmat\n with respect to the tree \ntree\n. Returns the answer as a sparse matrix with the same nonzero structure as \nmat\n. Assumes that \nmat\n is symmetric. \ntree\n should be the adjacency matrix of a spanning tree.\n\n\nsource", 
            "title": "treeAlgs"
        }, 
        {
            "location": "/treeAlgs/index.html#tree-algorithms", 
            "text": "Laplacians.compStretches   #  Laplacians.compStretches     Method .  Compute the stretched of every edge in  mat  with respect to the tree  tree . Returns the answer as a sparse matrix with the same nonzero structure as  mat . Assumes that  mat  is symmetric.  tree  should be the adjacency matrix of a spanning tree.  source", 
            "title": "Tree Algorithms"
        }, 
        {
            "location": "/akpw/index.html", 
            "text": "AKPW\n\n\nAlso see the page on \nLow Stretch Spanning Trees\n\n\n\n\nLaplacians.akpw\n\n\nLaplacians.akpwU\n\n\n\n\n#\n\n\nLaplacians.akpw\n \n \nMethod\n.\n\n\ntree = akpw(graph; ver=0)\n\n\n\n\nComputes a low stretch spanning tree of \ngraph\n, and returns it as a graph. The default version is 0.  In event of emergency, one can try \nver=2\n.  It is usually slower, but might have slightly better stretch.\n\n\nsource\n\n\n#\n\n\nLaplacians.akpwU\n \n \nMethod\n.\n\n\ntree = akpwU(graph)\n\n\n\n\nComputes a low stretch spanning tree of an unweighted \ngraph\n, and returns it as a graph.\n\n\nsource", 
            "title": "akpw"
        }, 
        {
            "location": "/akpw/index.html#akpw", 
            "text": "Also see the page on  Low Stretch Spanning Trees   Laplacians.akpw  Laplacians.akpwU   #  Laplacians.akpw     Method .  tree = akpw(graph; ver=0)  Computes a low stretch spanning tree of  graph , and returns it as a graph. The default version is 0.  In event of emergency, one can try  ver=2 .  It is usually slower, but might have slightly better stretch.  source  #  Laplacians.akpwU     Method .  tree = akpwU(graph)  Computes a low stretch spanning tree of an unweighted  graph , and returns it as a graph.  source", 
            "title": "AKPW"
        }, 
        {
            "location": "/solvers/index.html", 
            "text": "Solvers\n\n\nThese are a collection of relatively elementary solvers. For more, see the page \non using solvers\n.\n\n\n\n\nLaplacians.AMGLapSolver\n\n\nLaplacians.AMGSolver\n\n\nLaplacians.augTreeLapPrecon\n\n\nLaplacians.augTreeLapSolver\n\n\nLaplacians.augTreePrecon\n\n\nLaplacians.augTreeSolver\n\n\nLaplacians.augmentTree\n\n\nLaplacians.lapChol\n\n\nLaplacians.lapWrapSolver\n\n\n\n\n#\n\n\nLaplacians.AMGLapSolver\n \n \nMethod\n.\n\n\nA wrapper for the PyAMG solver. In line with our other solvers, takes in an adjacency matrix.\n\n\n amgSolver{Tv,Ti}(a::SparseMatrixCSC{Tv,Ti}; tol::Float64=1e-6, maxits=Inf, maxtime=Inf, verbose=false)\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.AMGSolver\n \n \nMethod\n.\n\n\nA wrapper for the PyAMG solver.\n\n\n amgSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti}; tol::Float64=1e-6, maxits=Inf, maxtime=Inf, verbose=false)\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.augTreeLapPrecon\n \n \nMethod\n.\n\n\nThis is an augmented spanning tree preconditioner for Laplacians. It takes as optional input a tree growing algorithm. It adds back 2sqrt(n) edges via \naugmentTree\n: the sqrt(n) of highest stretch and another sqrt(n) sampled according to stretch. For most purposes, one should directly call \naugTreeLapSolver\n.\n\n\nsource\n\n\n#\n\n\nLaplacians.augTreeLapSolver\n \n \nMethod\n.\n\n\nAn \"augmented spanning tree\" solver for Laplacian matrices. It works by adding edges to a low stretch spanning tree.  It calls \naugTreeLapPrecon\n to form the preconditioner. In line with other solver, it takes as input the adjacency matrix of the system.\n\n\n augTreeLapSolver{Tv,Ti}(a::SparseMatrixCSC{Tv,Ti}; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, treeAlg=akpw)\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.augTreePrecon\n \n \nMethod\n.\n\n\nThis is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. It adds back 2sqrt(n) edges via augmentTree: the sqrt(n) of highest stretch and another sqrt(n) sampled according to stretch. For most purposes, one should directly call \naugTreeSolver\n.\n\n\nsource\n\n\n#\n\n\nLaplacians.augTreeSolver\n \n \nMethod\n.\n\n\nAn \"augmented spanning tree\" solver for positive definite diagonally dominant matrices. It works by adding edges to a low stretch spanning tree.  It calls \naugTreePrecon\n to form the preconditioner.\n\n\n augTreeSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti}; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, treeAlg=akpw)\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.augmentTree\n \n \nMethod\n.\n\n\nTakes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch\n\n\nsource\n\n\n#\n\n\nLaplacians.lapChol\n \n \nMethod\n.\n\n\nlapChol uses a Cholesky Factorization to solver systems in Laplacians\n\n\nsource\n\n\n#\n\n\nLaplacians.lapWrapSolver\n \n \nMethod\n.\n\n\nTakes a solver for solving nonsingular sdd systems, and returns a solver for solving Laplacian systems. The optional args tol and maxits are not necessarily taken by all solvers.  But, if they are, one can pass them here\n\n\nsource", 
            "title": "solvers"
        }, 
        {
            "location": "/solvers/index.html#solvers", 
            "text": "These are a collection of relatively elementary solvers. For more, see the page  on using solvers .   Laplacians.AMGLapSolver  Laplacians.AMGSolver  Laplacians.augTreeLapPrecon  Laplacians.augTreeLapSolver  Laplacians.augTreePrecon  Laplacians.augTreeSolver  Laplacians.augmentTree  Laplacians.lapChol  Laplacians.lapWrapSolver   #  Laplacians.AMGLapSolver     Method .  A wrapper for the PyAMG solver. In line with our other solvers, takes in an adjacency matrix.   amgSolver{Tv,Ti}(a::SparseMatrixCSC{Tv,Ti}; tol::Float64=1e-6, maxits=Inf, maxtime=Inf, verbose=false)  source  #  Laplacians.AMGSolver     Method .  A wrapper for the PyAMG solver.   amgSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti}; tol::Float64=1e-6, maxits=Inf, maxtime=Inf, verbose=false)  source  #  Laplacians.augTreeLapPrecon     Method .  This is an augmented spanning tree preconditioner for Laplacians. It takes as optional input a tree growing algorithm. It adds back 2sqrt(n) edges via  augmentTree : the sqrt(n) of highest stretch and another sqrt(n) sampled according to stretch. For most purposes, one should directly call  augTreeLapSolver .  source  #  Laplacians.augTreeLapSolver     Method .  An \"augmented spanning tree\" solver for Laplacian matrices. It works by adding edges to a low stretch spanning tree.  It calls  augTreeLapPrecon  to form the preconditioner. In line with other solver, it takes as input the adjacency matrix of the system.   augTreeLapSolver{Tv,Ti}(a::SparseMatrixCSC{Tv,Ti}; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, treeAlg=akpw)  source  #  Laplacians.augTreePrecon     Method .  This is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. It adds back 2sqrt(n) edges via augmentTree: the sqrt(n) of highest stretch and another sqrt(n) sampled according to stretch. For most purposes, one should directly call  augTreeSolver .  source  #  Laplacians.augTreeSolver     Method .  An \"augmented spanning tree\" solver for positive definite diagonally dominant matrices. It works by adding edges to a low stretch spanning tree.  It calls  augTreePrecon  to form the preconditioner.   augTreeSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti}; tol::Real=1e-6, maxits=Inf, maxtime=Inf, verbose=false, treeAlg=akpw)  source  #  Laplacians.augmentTree     Method .  Takes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch  source  #  Laplacians.lapChol     Method .  lapChol uses a Cholesky Factorization to solver systems in Laplacians  source  #  Laplacians.lapWrapSolver     Method .  Takes a solver for solving nonsingular sdd systems, and returns a solver for solving Laplacian systems. The optional args tol and maxits are not necessarily taken by all solvers.  But, if they are, one can pass them here  source", 
            "title": "Solvers"
        }, 
        {
            "location": "/kmp/index.html", 
            "text": "KMP Solver\n\n\n\n\nLaplacians.KMPParams\n\n\nLaplacians.KMPLapSolver\n\n\nLaplacians.KMPSDDSolver\n\n\n\n\n#\n\n\nLaplacians.KMPParams\n \n \nType\n.\n\n\nParameters for the KMP solver\n\n\nsource\n\n\n#\n\n\nLaplacians.KMPLapSolver\n \n \nMethod\n.\n\n\nSolves linear equations in the Laplacian of graph with adjacency matrix \na\n.\n\n\nsource\n\n\n#\n\n\nLaplacians.KMPSDDSolver\n \n \nMethod\n.\n\n\nSolves linear equations in symmetric, diagonally dominant matrices with non-positive off-diagonals.\n\n\nsource", 
            "title": "KMP"
        }, 
        {
            "location": "/kmp/index.html#kmp-solver", 
            "text": "Laplacians.KMPParams  Laplacians.KMPLapSolver  Laplacians.KMPSDDSolver   #  Laplacians.KMPParams     Type .  Parameters for the KMP solver  source  #  Laplacians.KMPLapSolver     Method .  Solves linear equations in the Laplacian of graph with adjacency matrix  a .  source  #  Laplacians.KMPSDDSolver     Method .  Solves linear equations in symmetric, diagonally dominant matrices with non-positive off-diagonals.  source", 
            "title": "KMP Solver"
        }, 
        {
            "location": "/localClustering/index.html", 
            "text": "Local Clustering\n\n\nThis is a collection of clustering related algorithms,   based on Approximate Personal PageRank, and improvement by local   flow computations.   It needs more documentation.\n\n\nFor now, see the \nLocal Clustering Notebook\n\n\n\n\nLaplacians.apr\n\n\nLaplacians.dumbRefineCut\n\n\nLaplacians.localImprove\n\n\nLaplacians.prn\n\n\nLaplacians.refineCut\n\n\n\n\n#\n\n\nLaplacians.dumbRefineCut\n \n \nMethod\n.\n\n\nModify a cluster by passing through all the vertices exactly once and \nadding/removing them based on the value of (Deg_external - Deg_Internal).\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.refineCut\n \n \nMethod\n.\n\n\nModify a cluster by adding or removing vertices by picking at each step \nthe vertex that has the maximum value of (Deg_external - Deg_Internal).\nEach vertex can be added in/removed only once.\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.apr\n \n \nMethod\n.\n\n\nComputes an approximate page rank vector from a starting set s, an alpha and an epsilon The algorithm follows the Anderson,Chung,Lang paper and Dan Spielman's lecture notes\n\n\nsource\n\n\n#\n\n\nLaplacians.localImprove\n \n \nMethod\n.\n\n\nlocalImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1}; epsSigma=-1.0, err=1e-10, maxSize = max(G.n, G.m)\n\n\nThe LocalImprove function, from the Orrechia-Zhu paper. Given a graph and an initial set, finds a set of smaller conductance based on the starting set using a localized version of max-flow.\n\n\nSmall discussion: When adding in the neighbors of the initial component, if the resulting  conductance is worse than the initial one,  the algorithm will add more and more vertices until hitting a better conductance. However, if we fix a certain  maximum size for our component,  it might be the case that this new conductance will always be worse than what we had initially. Thus, if we run the algorithm with a small maxSize,  our initial conductance might be the best solution we can raech.\n\n\n\n\nG is the given graph, A is the initial set\n\n\nepsSigma is a measure of the quality of the returning set (the smaller the better). It's defaulted to volume(A) / volume(VA)\n\n\nerr is the numerical error considered throughout the algorithm. It's defaulted to 1e-10\n\n\nmaxSize is the maximum allowed size for the flow graph at any iteration of the algorithm. It's defaulted to |V|\n\n\n\n\nsource\n\n\n#\n\n\nLaplacians.prn\n \n \nMethod\n.\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)\n\n\nThe PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper\n\n\ns is a set of starting vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]\n\n\nphi is a bound on the quality of the conductance of the cut - the smaller the phi, the higher the quality.  b is used to handle precision throughout the algorithm - the higher the b, the greater the precision.\n\n\nsource", 
            "title": "localClustering"
        }, 
        {
            "location": "/localClustering/index.html#local-clustering", 
            "text": "This is a collection of clustering related algorithms,   based on Approximate Personal PageRank, and improvement by local   flow computations.   It needs more documentation.  For now, see the  Local Clustering Notebook   Laplacians.apr  Laplacians.dumbRefineCut  Laplacians.localImprove  Laplacians.prn  Laplacians.refineCut   #  Laplacians.dumbRefineCut     Method .  Modify a cluster by passing through all the vertices exactly once and \nadding/removing them based on the value of (Deg_external - Deg_Internal).  source  #  Laplacians.refineCut     Method .  Modify a cluster by adding or removing vertices by picking at each step \nthe vertex that has the maximum value of (Deg_external - Deg_Internal).\nEach vertex can be added in/removed only once.  source  #  Laplacians.apr     Method .  Computes an approximate page rank vector from a starting set s, an alpha and an epsilon The algorithm follows the Anderson,Chung,Lang paper and Dan Spielman's lecture notes  source  #  Laplacians.localImprove     Method .  localImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1}; epsSigma=-1.0, err=1e-10, maxSize = max(G.n, G.m)  The LocalImprove function, from the Orrechia-Zhu paper. Given a graph and an initial set, finds a set of smaller conductance based on the starting set using a localized version of max-flow.  Small discussion: When adding in the neighbors of the initial component, if the resulting  conductance is worse than the initial one,  the algorithm will add more and more vertices until hitting a better conductance. However, if we fix a certain  maximum size for our component,  it might be the case that this new conductance will always be worse than what we had initially. Thus, if we run the algorithm with a small maxSize,  our initial conductance might be the best solution we can raech.   G is the given graph, A is the initial set  epsSigma is a measure of the quality of the returning set (the smaller the better). It's defaulted to volume(A) / volume(VA)  err is the numerical error considered throughout the algorithm. It's defaulted to 1e-10  maxSize is the maximum allowed size for the flow graph at any iteration of the algorithm. It's defaulted to |V|   source  #  Laplacians.prn     Method .  prn{Tv, Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)  The PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper  s is a set of starting vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]  phi is a bound on the quality of the conductance of the cut - the smaller the phi, the higher the quality.  b is used to handle precision throughout the algorithm - the higher the b, the greater the precision.  source", 
            "title": "Local Clustering"
        }, 
        {
            "location": "/privateFuncs/index.html", 
            "text": "Unexported (Private) functions.\n\n\nThis is a list of all unexported functions and types from Laplacians.\n\n\n\n\nLaplacians.addToGPrime\n\n\nLaplacians.blockSolver\n\n\nLaplacians.compStretchesDFS\n\n\nLaplacians.getCutSet\n\n\nLaplacians.initGPrime\n\n\nLaplacians.localBlockFlow\n\n\nLaplacians.localFlow\n\n\nLaplacians.sampleByWeight\n\n\nLaplacians.sortSet\n\n\nLaplacians.treeDepthDFS\n\n\n\n\n#\n\n\nLaplacians.addToGPrime\n \n \nMethod\n.\n\n\nAdd a new vertex to GPrime \n\n\nsource\n\n\n#\n\n\nLaplacians.blockSolver\n \n \nMethod\n.\n\n\nApply the ith solver on the ith component\n\n\nsource\n\n\n#\n\n\nLaplacians.compStretchesDFS\n \n \nMethod\n.\n\n\nCompute the stretched of every edge in \nmat\n with respect to the tree \ntree\n. Returns the answer as a sparse matrix with the same nonzero structure as \nmat\n. Assumes that \nmat\n is symmetric. \ntree\n should be the adjacency matrix of a spanning tree,  \nordered by DFS so that every parent comes before its children in the order\n\n\nsource\n\n\n#\n\n\nLaplacians.getCutSet\n \n \nMethod\n.\n\n\nGet the min cut from the source - return all vertices in the cut besides the source \n\n\nsource\n\n\n#\n\n\nLaplacians.initGPrime\n \n \nMethod\n.\n\n\nInitialize GPrime with the set A and edges of type s-\nu\n\n\nsource\n\n\n#\n\n\nLaplacians.localBlockFlow\n \n \nMethod\n.\n\n\nCompute block flow between s and t\n\n\nsource\n\n\n#\n\n\nLaplacians.localFlow\n \n \nFunction\n.\n\n\nThe LocalFlow function, from the Orecchia-Zhu paper \n\n\nsource\n\n\n#\n\n\nLaplacians.sampleByWeight\n \n \nMethod\n.\n\n\nind = sampleByWeight(wt)\n\n\n\n\nsample an index with probability proportional to its weight given here\n\n\nsource\n\n\n#\n\n\nLaplacians.sortSet\n \n \nMethod\n.\n\n\nGiven a set of integers, \nset\n between 1 and n, return a sorted version of them\n\n\nsource\n\n\n#\n\n\nLaplacians.treeDepthDFS\n \n \nMethod\n.\n\n\nCompute the vector of depths in a tree that is in DFS order, \nwith the root at the first position, and the leaves at the end\n\n\nsource", 
            "title": "Private Functions"
        }, 
        {
            "location": "/privateFuncs/index.html#unexported-private-functions", 
            "text": "This is a list of all unexported functions and types from Laplacians.   Laplacians.addToGPrime  Laplacians.blockSolver  Laplacians.compStretchesDFS  Laplacians.getCutSet  Laplacians.initGPrime  Laplacians.localBlockFlow  Laplacians.localFlow  Laplacians.sampleByWeight  Laplacians.sortSet  Laplacians.treeDepthDFS   #  Laplacians.addToGPrime     Method .  Add a new vertex to GPrime   source  #  Laplacians.blockSolver     Method .  Apply the ith solver on the ith component  source  #  Laplacians.compStretchesDFS     Method .  Compute the stretched of every edge in  mat  with respect to the tree  tree . Returns the answer as a sparse matrix with the same nonzero structure as  mat . Assumes that  mat  is symmetric.  tree  should be the adjacency matrix of a spanning tree,   ordered by DFS so that every parent comes before its children in the order  source  #  Laplacians.getCutSet     Method .  Get the min cut from the source - return all vertices in the cut besides the source   source  #  Laplacians.initGPrime     Method .  Initialize GPrime with the set A and edges of type s- u  source  #  Laplacians.localBlockFlow     Method .  Compute block flow between s and t  source  #  Laplacians.localFlow     Function .  The LocalFlow function, from the Orecchia-Zhu paper   source  #  Laplacians.sampleByWeight     Method .  ind = sampleByWeight(wt)  sample an index with probability proportional to its weight given here  source  #  Laplacians.sortSet     Method .  Given a set of integers,  set  between 1 and n, return a sorted version of them  source  #  Laplacians.treeDepthDFS     Method .  Compute the vector of depths in a tree that is in DFS order,  with the root at the first position, and the leaves at the end  source", 
            "title": "Unexported (Private) functions."
        }, 
        {
            "location": "/indexOfAll/index.html", 
            "text": "Index of all exported\n\n\nThis is an index of all the exported methods. We would include the docstrings, but Documenter.jl doesn't let us.\n\n\n\n\nLaplacians.KMPParams\n\n\nBase.Random.randperm\n\n\nLaplacians.AMGLapSolver\n\n\nLaplacians.AMGSolver\n\n\nLaplacians.ErdosRenyi\n\n\nLaplacians.ErdosRenyiCluster\n\n\nLaplacians.ErdosRenyiClusterFix\n\n\nLaplacians.KMPLapSolver\n\n\nLaplacians.KMPSDDSolver\n\n\nLaplacians.addToGPrime\n\n\nLaplacians.adj\n\n\nLaplacians.akpw\n\n\nLaplacians.akpwU\n\n\nLaplacians.apr\n\n\nLaplacians.augTreeLapPrecon\n\n\nLaplacians.augTreeLapSolver\n\n\nLaplacians.augTreePrecon\n\n\nLaplacians.augTreeSolver\n\n\nLaplacians.augmentTree\n\n\nLaplacians.backIndices\n\n\nLaplacians.backIndices\n\n\nLaplacians.biggestComp\n\n\nLaplacians.blockSolver\n\n\nLaplacians.cg\n\n\nLaplacians.cgSolver\n\n\nLaplacians.chimera\n\n\nLaplacians.chimera\n\n\nLaplacians.compConductance\n\n\nLaplacians.compStretches\n\n\nLaplacians.compStretchesDFS\n\n\nLaplacians.completeBinaryTree\n\n\nLaplacians.completeGraph\n\n\nLaplacians.components\n\n\nLaplacians.diagmat\n\n\nLaplacians.disjoin\n\n\nLaplacians.dumbRefineCut\n\n\nLaplacians.edgeVertexMat\n\n\nLaplacians.findEntries\n\n\nLaplacians.flipIndex\n\n\nLaplacians.floatGraph\n\n\nLaplacians.generalizedNecklace\n\n\nLaplacians.generalizedRing\n\n\nLaplacians.getCutSet\n\n\nLaplacians.getObound\n\n\nLaplacians.getVolume\n\n\nLaplacians.grid2\n\n\nLaplacians.grid2coords\n\n\nLaplacians.grid3\n\n\nLaplacians.grownGraph\n\n\nLaplacians.grownGraphD\n\n\nLaplacians.hyperCube\n\n\nLaplacians.initGPrime\n\n\nLaplacians.isConnected\n\n\nLaplacians.joinGraphs\n\n\nLaplacians.kruskal\n\n\nLaplacians.lap\n\n\nLaplacians.lapChol\n\n\nLaplacians.lapWrapSolver\n\n\nLaplacians.localBlockFlow\n\n\nLaplacians.localFlow\n\n\nLaplacians.localImprove\n\n\nLaplacians.mapweight\n\n\nLaplacians.pathGraph\n\n\nLaplacians.pcg\n\n\nLaplacians.pcgLapSolver\n\n\nLaplacians.pcgSolver\n\n\nLaplacians.plotGraph\n\n\nLaplacians.prefAttach\n\n\nLaplacians.prim\n\n\nLaplacians.prn\n\n\nLaplacians.productGraph\n\n\nLaplacians.pureRandomGraph\n\n\nLaplacians.randGenRing\n\n\nLaplacians.randMatching\n\n\nLaplacians.randRegular\n\n\nLaplacians.randWeight\n\n\nLaplacians.randishKruskal\n\n\nLaplacians.randishPrim\n\n\nLaplacians.readIJ\n\n\nLaplacians.readIJV\n\n\nLaplacians.refineCut\n\n\nLaplacians.ringGraph\n\n\nLaplacians.sampleByWeight\n\n\nLaplacians.semiWtedChimera\n\n\nLaplacians.setValue\n\n\nLaplacians.shortIntGraph\n\n\nLaplacians.shortestPathTree\n\n\nLaplacians.shortestPaths\n\n\nLaplacians.sortSet\n\n\nLaplacians.spectralCoords\n\n\nLaplacians.spectralDrawing\n\n\nLaplacians.subsampleEdges\n\n\nLaplacians.treeDepthDFS\n\n\nLaplacians.twoLift\n\n\nLaplacians.uniformWeight\n\n\nLaplacians.uniformWeight!\n\n\nLaplacians.unweight\n\n\nLaplacians.unweight!\n\n\nLaplacians.vecToComps\n\n\nLaplacians.wGrid2\n\n\nLaplacians.wGrid3\n\n\nLaplacians.wdeg\n\n\nLaplacians.writeIJV\n\n\nLaplacians.wtedChimera\n\n\nLaplacians.wtedChimera", 
            "title": "All of the above"
        }, 
        {
            "location": "/indexOfAll/index.html#index-of-all-exported", 
            "text": "This is an index of all the exported methods. We would include the docstrings, but Documenter.jl doesn't let us.   Laplacians.KMPParams  Base.Random.randperm  Laplacians.AMGLapSolver  Laplacians.AMGSolver  Laplacians.ErdosRenyi  Laplacians.ErdosRenyiCluster  Laplacians.ErdosRenyiClusterFix  Laplacians.KMPLapSolver  Laplacians.KMPSDDSolver  Laplacians.addToGPrime  Laplacians.adj  Laplacians.akpw  Laplacians.akpwU  Laplacians.apr  Laplacians.augTreeLapPrecon  Laplacians.augTreeLapSolver  Laplacians.augTreePrecon  Laplacians.augTreeSolver  Laplacians.augmentTree  Laplacians.backIndices  Laplacians.backIndices  Laplacians.biggestComp  Laplacians.blockSolver  Laplacians.cg  Laplacians.cgSolver  Laplacians.chimera  Laplacians.chimera  Laplacians.compConductance  Laplacians.compStretches  Laplacians.compStretchesDFS  Laplacians.completeBinaryTree  Laplacians.completeGraph  Laplacians.components  Laplacians.diagmat  Laplacians.disjoin  Laplacians.dumbRefineCut  Laplacians.edgeVertexMat  Laplacians.findEntries  Laplacians.flipIndex  Laplacians.floatGraph  Laplacians.generalizedNecklace  Laplacians.generalizedRing  Laplacians.getCutSet  Laplacians.getObound  Laplacians.getVolume  Laplacians.grid2  Laplacians.grid2coords  Laplacians.grid3  Laplacians.grownGraph  Laplacians.grownGraphD  Laplacians.hyperCube  Laplacians.initGPrime  Laplacians.isConnected  Laplacians.joinGraphs  Laplacians.kruskal  Laplacians.lap  Laplacians.lapChol  Laplacians.lapWrapSolver  Laplacians.localBlockFlow  Laplacians.localFlow  Laplacians.localImprove  Laplacians.mapweight  Laplacians.pathGraph  Laplacians.pcg  Laplacians.pcgLapSolver  Laplacians.pcgSolver  Laplacians.plotGraph  Laplacians.prefAttach  Laplacians.prim  Laplacians.prn  Laplacians.productGraph  Laplacians.pureRandomGraph  Laplacians.randGenRing  Laplacians.randMatching  Laplacians.randRegular  Laplacians.randWeight  Laplacians.randishKruskal  Laplacians.randishPrim  Laplacians.readIJ  Laplacians.readIJV  Laplacians.refineCut  Laplacians.ringGraph  Laplacians.sampleByWeight  Laplacians.semiWtedChimera  Laplacians.setValue  Laplacians.shortIntGraph  Laplacians.shortestPathTree  Laplacians.shortestPaths  Laplacians.sortSet  Laplacians.spectralCoords  Laplacians.spectralDrawing  Laplacians.subsampleEdges  Laplacians.treeDepthDFS  Laplacians.twoLift  Laplacians.uniformWeight  Laplacians.uniformWeight!  Laplacians.unweight  Laplacians.unweight!  Laplacians.vecToComps  Laplacians.wGrid2  Laplacians.wGrid3  Laplacians.wdeg  Laplacians.writeIJV  Laplacians.wtedChimera  Laplacians.wtedChimera", 
            "title": "Index of all exported"
        }
    ]
}