{
    "docs": [
        {
            "location": "/about/index.html", 
            "text": "About Laplacians.jl\n\n\nLaplacians is a package containing graph algorithms, with an emphsasis on tasks related to spectral and algebraic graph theory.  It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.\n\n\nAll graphs are represented by sparse adjacency matrices.\nThis is both for speed, and because our main concerns are algebraic tasks.  it does \nnot\n handle dynamic graphs.  It would be very slow to implement dynamic graphs this way.\n\n\nLaplacians.jl was started by Daniel A. Spielman.  Other contributors include Xiao Shi, Serban Stan and Jackson Thea.\n\n\nIn this directory, you will find:\n\n\n\n\nThis documentation, in the site directory.  \n\n\nThe source code, in src\n\n\nExample sessions of using Julia in jupyter notebooks, in notebooks.\n\n\nTests, in the directory tests", 
            "title": "About"
        }, 
        {
            "location": "/about/index.html#about-laplaciansjl", 
            "text": "Laplacians is a package containing graph algorithms, with an emphsasis on tasks related to spectral and algebraic graph theory.  It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.  All graphs are represented by sparse adjacency matrices.\nThis is both for speed, and because our main concerns are algebraic tasks.  it does  not  handle dynamic graphs.  It would be very slow to implement dynamic graphs this way.  Laplacians.jl was started by Daniel A. Spielman.  Other contributors include Xiao Shi, Serban Stan and Jackson Thea.  In this directory, you will find:   This documentation, in the site directory.    The source code, in src  Example sessions of using Julia in jupyter notebooks, in notebooks.  Tests, in the directory tests", 
            "title": "About Laplacians.jl"
        }, 
        {
            "location": "/Julia/index.html", 
            "text": "Using Julia\n\n\nConverting to Julia 0.4\n\n\nSmall details\n\n\n\n\n\n\nJulia Notebooks\n\n\nWorkflows\n\n\nDan's current workflow:\n\n\nAdd your current workflow here:\n\n\n\n\n\n\nThings to be careful of (common bugs)\n\n\nUseful Julia functions\n\n\nOptimizing code in Julia\n\n\nVectorization is Bad.\n\n\n\n\n\n\nHow should Julia packages be organized?\n\n\nHow should notebooks play with Git?\n\n\n\n\n\n\n\n\n\n\nUsing Julia\n\n\nTo use the julia notebooks, you will need ipython and the IJulia package.  You should also get \njupyter\n, which you should be able to install from ipython.\nTo install IJulia, you type \nPkg.add(\"IJulia\")\n from Julia.\nThen, you just need to type \nusing IJulia\n once.  This will tell jupyter about the Julia kernel.  To run Julia notebooks, you now type \njupyter notebook\n.  You can select the kernel your new notebook is using.\n\n\nEach time you upgrade Julia (by installing verison 0.4.x+1), you should run \nPkg.build(\"IJulia\")\n so that the notebooks will be able to access the new kernel.\n\n\nDan recommends installing the anaconda distribution of python.\nYou will then need to install some things from that, like\nconda install jupyter (the new notebooks package)\nconda install mathjax\nconda install matplotlib\n\n\nThis repository contains projects implemented in Julia by Dan Spielman's group.  While organizing by language is strange, we are trying it to help us learn the language.\n\n\nThere are two projects in here so far.  One, yinsGraph, is for doing graph theory and solving Laplacian systems.  It's documentation is at \nyinsGraph.md\n\n\nI would like to use the documentation pages in this root directory to discuss issues with how to get Julia to work well.  For now, I'll just ask some questions and write a little that I've figured out.  If you figure some out, please write it here.\n\n\nConverting to Julia 0.4\n\n\nA description of the changes made in Julia 0.4 appears to be here\n[https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md]\n(https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md)\n\n\nThe first problem you will encounter when using Julia 0.4 is that it is stingier about its load path.  It won't load something from the current directory unless it is in the load path.  You can add a directory to the load path like\n\n\npush!(LOAD_PATH,\n.\n)\n\n\n\n\nor\n\n\npush!(LOAD_PATH,\n/Users/[your_username]/git/julia/yinsGraph\n)\n\n\n\n\nTo overcome this issue, you can add the above line to the end of the julia.rc file, found in\n\n\n/Applications/Julia-0.4.2.app/Contents/Resources/julia/etc/julia\n\n\n\n\nJulia 0.4 lets you take advantage of docstrings.\nFor example, \n?ringGraph\n produces\n\n\nThe simple ring on n vertices\n\n\n\n\nWhen having a multiline comment, make sure that lines don't have starting and trailing spaces.\nThis will mess up the indentation when calling '?func_name'.\n\n\nSmall details\n\n\n\n\nJulia 0.4 is trying to wean you off Matlab-like notation.  You should no longer create vectors like \n[1:n]\n.  Instead, you should type \ncollect(1:n)\n\n\n\n\nJulia Notebooks\n\n\nTo get the Julia notebooks working, I presently type \njupyter notebook\n.\nI then select the kernel to be Julia-0.4.2.\nIt seems important to run this command from a directory that contains all the directories\nthat have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks\nfrom other directories.  That has only given me trouble.\n\n\nThe calico extensions that seem to be hosted at Brynmawr seem interesting.\nI haven't yet figured out how to get them to work.\nHere are the relevent links:\n\n\n\n\nhttp://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Help.ipynb\n\n\nhttp://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Notebook%20Users%20Manual.ipynb\n\n\n\n\nTo turn a notebook into html, you type something like\n\n\nipython nbconvert Laplacians.ipynb\n\n\n\n\nor\n\n\nipython nbconvert --to markdown --stdout Sampler.ipynb \n SamplerNotebook.md\n\n\n\n\nWorkflows\n\n\nJulia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:\n\n\nDan's current workflow:\n\n\n\n\nI use emacs (which has a mode for Julia) and the notebooks.\n\n\nI develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.\n\n\n\n\nAfter modifying code, I reload it with \ninclude(\"develX.jl\")\n.  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.\n\n\n\n\n\n\nI am writing this documention in MacDown.\n\n\n\n\n\n\nAdd your current workflow here:\n\n\nThings to be careful of (common bugs)\n\n\n\n\n\n\nJulia passes vectors and matrices to routines by reference, rather than by copying them.  If you type \nx = y\n when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type \nx = copy(y)\n.  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.\n\n\n\n\n\n\nOn the other hand, if you type \nx = x + y\n, then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type \nx += y\n.  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.\n\n\n\n\n\n\n\n\nAdds b in to a\n\nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend\n\n\nFails to add b in to a\n\nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2\n\n\n\n\n\n\n\n\n\nIf you are used to programming in Matlab, you might be tempted to type a line like \nfor i in 1:10,\n.  \nDo not put extra commas in Julia!\n  It will cause bad things to happen.\n\n\n\n\n\n\nJulia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the \nsetValue\n function. \nsetValue(G, u, i, 0)\n will set \nweighti(G, u, i)\n to 0 while also leaving \n(u, nbri(G, u, i))\n in the matrix.\n\n\n\n\n\n\nUseful Julia functions\n\n\nI am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.\n\n\n\n\n\n\ndocstrings: in the above example, I used a docstring to document each function.  You can get these by typing \n?addA2B\n.  You can also  \nwrite longer docstrings and use markdown\n.  I suggest putting them in front of every function.\n\n\n\n\n\n\nmethods(foo)\n lists all methods with the name foo.\n\n\n\n\nfieldnames(footype)\n tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type \nnames(footype)\n\n\n\n\njulia\n a = sparse(rand(3,3));\njulia\n fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval\n\n\n\n\nOptimizing code in Julia\n\n\nThe best way that I've found of figuring out what's slowing down my code has been to use \n@code_warntype\n.  It only exists in version 4 of Julia.  For this reason, I keep one of those around.\n\n\nNote that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use \n@time\n to time your code.\n\n\nI recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.\n\n\nVectorization is Bad.\n\n\nJulia is the anti-matlab in that vectorization is slow.\nStill it is a good way to write your code the first time.\nHere are some examples of code that adds one vector into another.\nThe first is vectorized, the second turns that into a loop, and the fastest uses BLAS.  Note that this was done in Julia 0.3.11.  The vectorized code is much faster, but still not fast, in 0.4.\n\nAlso note that you have to run each routine once before it will be fast.  This is because it compiles it the first time your run it\n\n\nn = 10^7\na = rand(n)\nb = rand(n)\n@time a += b;\n\nelapsed time: 0.155296017 seconds (80000312 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time add2(a,b);\n\nelapsed time: 0.021190554 seconds (80 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time BLAS.axpy!(1.0,b,a);\n\nelapsed time: 0.015894922 seconds (80 bytes allocated)\n\n\n\n\n\nOne reason that \na += b\n was slow was that it seems to allocate a lot of memory.\n\n\nHow should Julia packages be organized?\n\n\nIn yinsGraph, I decided to just make one big module called yinsGraph.jl.  It then includes a bunch of individual files, most of which contain many functions and types.  I think this is much nicer than making one file per functions, as some functions are very short.\n\n\nI put the export statements in the main module.  The reason for this is that while developing code in a file, I don't include that in the module.  This way I can reload it as I change it without having to restart the kernel.  This does not seem to work as well for types.  I'm not sure why.\n\n\nHow should notebooks play with Git?\n\n\nThe great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#using-julia", 
            "text": "To use the julia notebooks, you will need ipython and the IJulia package.  You should also get  jupyter , which you should be able to install from ipython.\nTo install IJulia, you type  Pkg.add(\"IJulia\")  from Julia.\nThen, you just need to type  using IJulia  once.  This will tell jupyter about the Julia kernel.  To run Julia notebooks, you now type  jupyter notebook .  You can select the kernel your new notebook is using.  Each time you upgrade Julia (by installing verison 0.4.x+1), you should run  Pkg.build(\"IJulia\")  so that the notebooks will be able to access the new kernel.  Dan recommends installing the anaconda distribution of python.\nYou will then need to install some things from that, like\nconda install jupyter (the new notebooks package)\nconda install mathjax\nconda install matplotlib  This repository contains projects implemented in Julia by Dan Spielman's group.  While organizing by language is strange, we are trying it to help us learn the language.  There are two projects in here so far.  One, yinsGraph, is for doing graph theory and solving Laplacian systems.  It's documentation is at  yinsGraph.md  I would like to use the documentation pages in this root directory to discuss issues with how to get Julia to work well.  For now, I'll just ask some questions and write a little that I've figured out.  If you figure some out, please write it here.", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#converting-to-julia-04", 
            "text": "A description of the changes made in Julia 0.4 appears to be here\n[https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md]\n(https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md)  The first problem you will encounter when using Julia 0.4 is that it is stingier about its load path.  It won't load something from the current directory unless it is in the load path.  You can add a directory to the load path like  push!(LOAD_PATH, . )  or  push!(LOAD_PATH, /Users/[your_username]/git/julia/yinsGraph )  To overcome this issue, you can add the above line to the end of the julia.rc file, found in  /Applications/Julia-0.4.2.app/Contents/Resources/julia/etc/julia  Julia 0.4 lets you take advantage of docstrings.\nFor example,  ?ringGraph  produces  The simple ring on n vertices  When having a multiline comment, make sure that lines don't have starting and trailing spaces.\nThis will mess up the indentation when calling '?func_name'.  Small details   Julia 0.4 is trying to wean you off Matlab-like notation.  You should no longer create vectors like  [1:n] .  Instead, you should type  collect(1:n)", 
            "title": "Converting to Julia 0.4"
        }, 
        {
            "location": "/Julia/index.html#julia-notebooks", 
            "text": "To get the Julia notebooks working, I presently type  jupyter notebook .\nI then select the kernel to be Julia-0.4.2.\nIt seems important to run this command from a directory that contains all the directories\nthat have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks\nfrom other directories.  That has only given me trouble.  The calico extensions that seem to be hosted at Brynmawr seem interesting.\nI haven't yet figured out how to get them to work.\nHere are the relevent links:   http://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Help.ipynb  http://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Notebook%20Users%20Manual.ipynb   To turn a notebook into html, you type something like  ipython nbconvert Laplacians.ipynb  or  ipython nbconvert --to markdown --stdout Sampler.ipynb   SamplerNotebook.md", 
            "title": "Julia Notebooks"
        }, 
        {
            "location": "/Julia/index.html#workflows", 
            "text": "Julia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:  Dan's current workflow:   I use emacs (which has a mode for Julia) and the notebooks.  I develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.   After modifying code, I reload it with  include(\"develX.jl\") .  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.    I am writing this documention in MacDown.    Add your current workflow here:", 
            "title": "Workflows"
        }, 
        {
            "location": "/Julia/index.html#things-to-be-careful-of-common-bugs", 
            "text": "Julia passes vectors and matrices to routines by reference, rather than by copying them.  If you type  x = y  when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type  x = copy(y) .  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.    On the other hand, if you type  x = x + y , then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type  x += y .  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.     Adds b in to a \nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend Fails to add b in to a \nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2    If you are used to programming in Matlab, you might be tempted to type a line like  for i in 1:10, .   Do not put extra commas in Julia!   It will cause bad things to happen.    Julia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the  setValue  function.  setValue(G, u, i, 0)  will set  weighti(G, u, i)  to 0 while also leaving  (u, nbri(G, u, i))  in the matrix.", 
            "title": "Things to be careful of (common bugs)"
        }, 
        {
            "location": "/Julia/index.html#useful-julia-functions", 
            "text": "I am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.    docstrings: in the above example, I used a docstring to document each function.  You can get these by typing  ?addA2B .  You can also   write longer docstrings and use markdown .  I suggest putting them in front of every function.    methods(foo)  lists all methods with the name foo.   fieldnames(footype)  tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type  names(footype)   julia  a = sparse(rand(3,3));\njulia  fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval", 
            "title": "Useful Julia functions"
        }, 
        {
            "location": "/Julia/index.html#optimizing-code-in-julia", 
            "text": "The best way that I've found of figuring out what's slowing down my code has been to use  @code_warntype .  It only exists in version 4 of Julia.  For this reason, I keep one of those around.  Note that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use  @time  to time your code.  I recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.  Vectorization is Bad.  Julia is the anti-matlab in that vectorization is slow.\nStill it is a good way to write your code the first time.\nHere are some examples of code that adds one vector into another.\nThe first is vectorized, the second turns that into a loop, and the fastest uses BLAS.  Note that this was done in Julia 0.3.11.  The vectorized code is much faster, but still not fast, in 0.4. Also note that you have to run each routine once before it will be fast.  This is because it compiles it the first time your run it  n = 10^7\na = rand(n)\nb = rand(n)\n@time a += b;\n\nelapsed time: 0.155296017 seconds (80000312 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time add2(a,b);\n\nelapsed time: 0.021190554 seconds (80 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time BLAS.axpy!(1.0,b,a);\n\nelapsed time: 0.015894922 seconds (80 bytes allocated)  One reason that  a += b  was slow was that it seems to allocate a lot of memory.", 
            "title": "Optimizing code in Julia"
        }, 
        {
            "location": "/Julia/index.html#how-should-julia-packages-be-organized", 
            "text": "In yinsGraph, I decided to just make one big module called yinsGraph.jl.  It then includes a bunch of individual files, most of which contain many functions and types.  I think this is much nicer than making one file per functions, as some functions are very short.  I put the export statements in the main module.  The reason for this is that while developing code in a file, I don't include that in the module.  This way I can reload it as I change it without having to restart the kernel.  This does not seem to work as well for types.  I'm not sure why.", 
            "title": "How should Julia packages be organized?"
        }, 
        {
            "location": "/Julia/index.html#how-should-notebooks-play-with-git", 
            "text": "The great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "How should notebooks play with Git?"
        }, 
        {
            "location": "/Laplacians/index.html", 
            "text": "Laplacians\n\n\nInstallation\n\n\nTo use Laplacians\n\n\n\n\n\n\n\n\n\n\nLaplacians\n\n\nInstallation\n\n\nFirst, you will need Julia.\nYou will also need a number of Julia packages.\n\nYou install these like\n\n\nPkg.add(\nPyPlot\n)\nPkg.add(\nDataStructures\n)\n\n\n\n\nOnce these are installed, you can use Laplacians.\nRight now, Laplacians is a module, not a package.\nSo, we will need to do a little more to get it started.\nIn the directory where Laplacians resides, type the following:\n\n\npush!(LOAD_PATH,\nsrc\n)\nusing Laplacians\n\n\n\n\nInstead of adding to the load path every time you use Julia, you could put the following line in a file called .juliarc.jl that lives in your home directory:\n\n\npush!(LOAD_PATH,\n[path_to_laplacians]/Laplacians.jl/src\n)\n\n\n\n\nIn my case, the path to laplacians is \n/Users/spielman/git/\n.\nThen, when you want to use the module, you just need to type \nusing Laplacians\n.\n\n\nYou may need to install matplotlib in python before PyPlot.\nLook at this page for more information: https://github.com/stevengj/PyPlot.jl\n\n\nIf you discover that you need any other packages, please list them above.\n\n\nOther recommended (but not necessary) packages are:\n\n\n\n\nOptim\n\n\n\n\nTo see if it is working, try something like this:\n\n\na = chimera(100,6)\nspectralDrawing(a)\n\n\n\n\nor\n\n\na = generalizedNecklace(grid2(6),grid2(3),2)\nspectralDrawing(a)\n\n\n\n\nTo use Laplacians\n\n\nExamples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.\n\n\nRight now, the notebooks worth looking at are:\n\n\n\n\nyinsGraph\n - usage, demo, and speed tests (Laplacians was previously called yinsGraph)\n\n\nSolvers\n - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.\n\n\n\n\n(I suggest that you open the html in your browser)", 
            "title": "Introduction"
        }, 
        {
            "location": "/Laplacians/index.html#laplacians", 
            "text": "", 
            "title": "Laplacians"
        }, 
        {
            "location": "/Laplacians/index.html#installation", 
            "text": "First, you will need Julia.\nYou will also need a number of Julia packages. \nYou install these like  Pkg.add( PyPlot )\nPkg.add( DataStructures )  Once these are installed, you can use Laplacians.\nRight now, Laplacians is a module, not a package.\nSo, we will need to do a little more to get it started.\nIn the directory where Laplacians resides, type the following:  push!(LOAD_PATH, src )\nusing Laplacians  Instead of adding to the load path every time you use Julia, you could put the following line in a file called .juliarc.jl that lives in your home directory:  push!(LOAD_PATH, [path_to_laplacians]/Laplacians.jl/src )  In my case, the path to laplacians is  /Users/spielman/git/ .\nThen, when you want to use the module, you just need to type  using Laplacians .  You may need to install matplotlib in python before PyPlot.\nLook at this page for more information: https://github.com/stevengj/PyPlot.jl  If you discover that you need any other packages, please list them above.  Other recommended (but not necessary) packages are:   Optim   To see if it is working, try something like this:  a = chimera(100,6)\nspectralDrawing(a)  or  a = generalizedNecklace(grid2(6),grid2(3),2)\nspectralDrawing(a)", 
            "title": "Installation"
        }, 
        {
            "location": "/Laplacians/index.html#to-use-laplacians", 
            "text": "Examples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.  Right now, the notebooks worth looking at are:   yinsGraph  - usage, demo, and speed tests (Laplacians was previously called yinsGraph)  Solvers  - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.   (I suggest that you open the html in your browser)", 
            "title": "To use Laplacians"
        }, 
        {
            "location": "/solvers/index.html", 
            "text": "Solving linear equations in Laplacians\n\n\nDirect Solvers\n\n\nIterative Solvers\n\n\nLow-Stretch Spanning Trees\n\n\nAugmented spanning tree preconditioners\n\n\n\n\n\n\n\n\n\n\nSolving linear equations in Laplacians\n\n\nRight now, our solver code is in \nsolvers.jl\n, but not included in yinsGraph.  So, you should include this directly.  Implementations of cg and pcg have been automatically included in yinsGraph.  They are in the file \npcg.jl\n\n\nFor some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.\n\n\nDirect Solvers\n\n\nYou can compute a cholesky factor directly with \ncholfact\n.  It does  more than just compute the factor, and it saves its result in a data structure that implements \n\\\n.  It uses SuiteSparse by Tim Davis.\n\n\nHere is an example of how you would use it to solve a general non-singular linear system.\n\n\na = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14\n\n\n\n\nLaplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.\n\n\nla = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15\n\n\n\n\nHere are two other ways of using the wrapper:\n\n\nlapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n);\nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15\n\n\n\n\nIterative Solvers\n\n\nThe first, of course, is the Conjugate Gradient (cg).\n\n\nOur implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance \ntol\n and the maximum number of iterations, \nmaxits\n.  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.\n\n\nn = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100)\nnorm(a*x - b)\n    1.2191649497921835e-6\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\nnorm(a*xbig - bbig)\n    1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33\n\n\n\n\nAs a sanity check, we do two speed tests against Matlab.\n\n\nla = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064\n\n\n\n\nAnd, in Matlab:\n\n\n a = grid2(200);\n\n la = lap(a);\n\n b = randn(length(a),1); b = b - mean(b);\n\n tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.\n\n norm(la*x-b)\n\nans =\n\n   1.9730e-04\n\n\n\n\nPCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.\n\n\na = mapweight(grid2(200),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000)\n    3.322035 seconds (42.21 k allocations: 1.194 GB, 5.11% gc time)\nnorm(la*x-b)\n    0.008508746034886803\n\n\n\n\nIf our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.\n\n\n@time x = pcg(la,b,pre,tol=1e-1,maxits=10^5)\n    0.747042 seconds (9.65 k allocations: 275.819 MB, 4.87% gc time)\nnorm(la*x-b)\n    19.840756251253442\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5)\n    6.509665 seconds (22.55 k allocations: 1.680 GB, 3.68% gc time)\nnorm(la*x-b)\n    19.222483530605043\n\n\n\n\nLow-Stretch Spanning Trees\n\n\nIn order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm.\n\nrandishKruskall\n samples the remaining edges with probability proportional to their weight.  \nrandishPrim\n samples edges on the boundary while using the same rule.\n\n\nBoth use a data structure called \nSampler\n that allows you to store integers with real values, and to sample according to those real values.\n\n\nWe also have code for computing the stretches.\nHere are some examples.\n\n\na = grid2(1000)\nt = randishKruskal(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    43.410262262262265\n\nt = randishPrim(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    33.14477077077077\n\n\n\n\n\nAugmented spanning tree preconditioners\n\n\nHere is code that will invoke one.\nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.\n\n\na = mapweight(grid2(1000),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nla[1,1] = la[1,1] + 1\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000)\n    6.529052 seconds (4.00 M allocations: 1.858 GB, 15.34% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.058915 seconds (9.74 k allocations: 23.209 GB, 6.84% gc time)\n\nnorm(la*x - b)\n    99.74452367765869\n\n# Now, let's contrast with using CG\n\n@time y = cg(la,b,tol=1e-1,maxits=1000)\n    28.719631 seconds (4.01 k allocations: 7.473 GB, 3.74% gc time)\n\nnorm(la*y-b)\n    3243.6014713600766\n\n\n\n\n\nThat was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.\n\n\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000,treeAlg=randishPrim);\n    6.319489 seconds (4.00 M allocations: 2.030 GB, 18.81% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.503484 seconds (9.76 k allocations: 23.268 GB, 7.31% gc time)\n\nnorm(la*x - b)\n    99.29610874176991\n\n\n\n\nTo solve systems in a Laplacian, we could wrap it.\n\n\nn = 40000\nla = lap(randRegular(n,3))\nf = lapWrapSolver(augTreeSolver,la,tol=1e-6,maxits=1000)\nb = randn(n); b = b - mean(b)\nx = f(b)\nnorm(la*x-b)\n    0.00019304778073388\n\n\n\n\nAs you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Solvers"
        }, 
        {
            "location": "/solvers/index.html#solving-linear-equations-in-laplacians", 
            "text": "Right now, our solver code is in  solvers.jl , but not included in yinsGraph.  So, you should include this directly.  Implementations of cg and pcg have been automatically included in yinsGraph.  They are in the file  pcg.jl  For some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.", 
            "title": "Solving linear equations in Laplacians"
        }, 
        {
            "location": "/solvers/index.html#direct-solvers", 
            "text": "You can compute a cholesky factor directly with  cholfact .  It does  more than just compute the factor, and it saves its result in a data structure that implements  \\ .  It uses SuiteSparse by Tim Davis.  Here is an example of how you would use it to solve a general non-singular linear system.  a = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14  Laplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.  la = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15  Here are two other ways of using the wrapper:  lapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n);\nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15", 
            "title": "Direct Solvers"
        }, 
        {
            "location": "/solvers/index.html#iterative-solvers", 
            "text": "The first, of course, is the Conjugate Gradient (cg).  Our implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance  tol  and the maximum number of iterations,  maxits .  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.  n = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100)\nnorm(a*x - b)\n    1.2191649497921835e-6\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\nnorm(a*xbig - bbig)\n    1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33  As a sanity check, we do two speed tests against Matlab.  la = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064  And, in Matlab:   a = grid2(200);  la = lap(a);  b = randn(length(a),1); b = b - mean(b);  tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.  norm(la*x-b)\n\nans =\n\n   1.9730e-04  PCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.  a = mapweight(grid2(200),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000)\n    3.322035 seconds (42.21 k allocations: 1.194 GB, 5.11% gc time)\nnorm(la*x-b)\n    0.008508746034886803  If our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.  @time x = pcg(la,b,pre,tol=1e-1,maxits=10^5)\n    0.747042 seconds (9.65 k allocations: 275.819 MB, 4.87% gc time)\nnorm(la*x-b)\n    19.840756251253442\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5)\n    6.509665 seconds (22.55 k allocations: 1.680 GB, 3.68% gc time)\nnorm(la*x-b)\n    19.222483530605043", 
            "title": "Iterative Solvers"
        }, 
        {
            "location": "/solvers/index.html#low-stretch-spanning-trees", 
            "text": "In order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm. randishKruskall  samples the remaining edges with probability proportional to their weight.   randishPrim  samples edges on the boundary while using the same rule.  Both use a data structure called  Sampler  that allows you to store integers with real values, and to sample according to those real values.  We also have code for computing the stretches.\nHere are some examples.  a = grid2(1000)\nt = randishKruskal(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    43.410262262262265\n\nt = randishPrim(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    33.14477077077077", 
            "title": "Low-Stretch Spanning Trees"
        }, 
        {
            "location": "/solvers/index.html#augmented-spanning-tree-preconditioners", 
            "text": "Here is code that will invoke one.\nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.  a = mapweight(grid2(1000),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nla[1,1] = la[1,1] + 1\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000)\n    6.529052 seconds (4.00 M allocations: 1.858 GB, 15.34% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.058915 seconds (9.74 k allocations: 23.209 GB, 6.84% gc time)\n\nnorm(la*x - b)\n    99.74452367765869\n\n# Now, let's contrast with using CG\n\n@time y = cg(la,b,tol=1e-1,maxits=1000)\n    28.719631 seconds (4.01 k allocations: 7.473 GB, 3.74% gc time)\n\nnorm(la*y-b)\n    3243.6014713600766  That was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.  @time F = augTreeSolver(la,tol=1e-1,maxits=1000,treeAlg=randishPrim);\n    6.319489 seconds (4.00 M allocations: 2.030 GB, 18.81% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.503484 seconds (9.76 k allocations: 23.268 GB, 7.31% gc time)\n\nnorm(la*x - b)\n    99.29610874176991  To solve systems in a Laplacian, we could wrap it.  n = 40000\nla = lap(randRegular(n,3))\nf = lapWrapSolver(augTreeSolver,la,tol=1e-6,maxits=1000)\nb = randn(n); b = b - mean(b)\nx = f(b)\nnorm(la*x-b)\n    0.00019304778073388  As you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Augmented spanning tree preconditioners"
        }, 
        {
            "location": "/Examples/index.html", 
            "text": "Examples\n\n\nThe following are links to html files of Julia notebooks.\nThese notebooks are also in the notebook directory, and can be open there so that you can run the code live.\n\n\n\n\nFirstNotebook\n\n\nSolvers\n\n\nSampler\n\n\nLightGraphs\n - an example, with a speed test, of using LightGraphs.  At least it is easy.", 
            "title": "Examples"
        }, 
        {
            "location": "/Examples/index.html#examples", 
            "text": "The following are links to html files of Julia notebooks.\nThese notebooks are also in the notebook directory, and can be open there so that you can run the code live.   FirstNotebook  Solvers  Sampler  LightGraphs  - an example, with a speed test, of using LightGraphs.  At least it is easy.", 
            "title": "Examples"
        }, 
        {
            "location": "/Developing/index.html", 
            "text": "Developing Laplacians.jl\n\n\nJust go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"\n\n\nUsing sparse matrices as graphs\n\n\nThe routines \ndeg\n, \nnbri\n and \nweighti\n will let you treat a sparse matrix like a graph.\n\n\ndeg(graph, u)\n is the degree of node u.\n\nnbri(graph, u, i)\n is the ith neighbor of node u.\n\nweighti(graph, u, i)\n is the weight of the edge to the ith neighbor of node u.\n\n\nNote that we start indexing from 1.\n\n\nFor example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:\n\n\n  for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end\n\n\n\n\nBut, this turns out to be much slower than working with the structure directly, like\n\n\n  for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end\n\n\n\n\n\n\n[ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.\n\n\n\n\nThe SparseMatrixCSC data structure\n\n\nYou can explore what is going on with the data structure by looking at some examples.  For example, here is a randomly weighted complete graph on 4 vertices, first displayed as a matrix:\n\n\ngr = round(10*uniformWeight(completeGraph(4)))\n\n4x4 sparse matrix with 12 Float64 entries:\n    [2, 1]  =  3.0\n    [3, 1]  =  3.0\n    [4, 1]  =  6.0\n    [1, 2]  =  3.0\n    [3, 2]  =  1.0\n    [4, 2]  =  2.0\n    [1, 3]  =  3.0\n    [2, 3]  =  1.0\n    [4, 3]  =  7.0\n    [1, 4]  =  6.0\n    [2, 4]  =  2.0\n    [3, 4]  =  7.0\n\nfull(gr)\n\n4x4 Array{Float64,2}:\n 0.0  3.0  3.0  6.0\n 3.0  0.0  1.0  2.0\n 3.0  1.0  0.0  7.0\n 6.0  2.0  7.0  0.0\n\n\n\n\nTo see the underlying data structure, use \nfieldnames\n.\n\n\nfieldnames(gr)\n\n5-element Array{Symbol,1}:\n :m     \n :n     \n :colptr\n :rowval\n :nzval \n\n\n\n\nm\n and \nn\n are the dimensions of the matrix.\nThe entries of the matrix are stored in nzval.\ncolptr[i] is the index in nzval of the first nonzero entry\nin column i.  rowval tells you which rows in each column are nonzero.\nThe indices of the nonzero entries in column i are stored in \nrowval[colptr[i]] through rowval[colptr[i+1]-1].\n\n\ngr.colptr \n\n5-element Array{Int64,1}:\n  1\n  4\n  7\n 10\n 13\n\n [gr.rowval gr.nzval]\n\n 12x2 Array{Float64,2}:\n 2.0  3.0\n 3.0  3.0\n 4.0  6.0\n 1.0  3.0\n 3.0  1.0\n 4.0  2.0\n 1.0  3.0\n 2.0  1.0\n 4.0  7.0\n 1.0  6.0\n 2.0  2.0\n 3.0  7.0\n\n\n\n\nDocumentation\n\n\nThis documentation is still very rough.\nIt is generated by a combination of Markdown and semi-automatic generation.  The steps to generate and improve it are:\n\n\n\n\nEdit Markdown files in the \ndocs\n directory.  For example, you could use MacDown to do this.\n\n\nIf you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.\n\n\nAdd docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.  To build the API, type\n\n\n\n\ninclude(\ndocs/build.jl\n)\n\n\n\n\n\n\nRun \nmkdocs build\n in the root directory to regenerate the documentation from the Markdown.\n\n\n\n\nParametric Types\n\n\nA sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\n\n\n\n\nTv\n, sometimes written \nTval\n denotes the types of the values, and \nTi\n or \nTind\n denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths, \nstart\n is of type \nTi\n.  Inside the code, whenever we write something like \npArray = zeros(Ti,n)\n, it creates an array of zeros of type Ti.  Using these parameteric types is \nmuch\n faster than leaving the types unfixed.\n\n\nData structures:\n\n\n\n\nIntHeap\n a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.\n\n\n\n\nInterface issue:\n\n\nThere are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.\n\n\nWriting tests:\n\n\nI haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.\n\n\nIntegration with other packages.\n\n\nThere are other graph packages that we might want to sometimes use.\n\n\n\n\nGraphs.jl\n : I found this one to be too slow and awkward to be useful.\n\n\nLightGraphs.jl\n : this looks more promising.  We will have to check it out.", 
            "title": "Devleoping Laplacians"
        }, 
        {
            "location": "/Developing/index.html#developing-laplaciansjl", 
            "text": "Just go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"", 
            "title": "Developing Laplacians.jl"
        }, 
        {
            "location": "/Developing/index.html#using-sparse-matrices-as-graphs", 
            "text": "The routines  deg ,  nbri  and  weighti  will let you treat a sparse matrix like a graph.  deg(graph, u)  is the degree of node u. nbri(graph, u, i)  is the ith neighbor of node u. weighti(graph, u, i)  is the weight of the edge to the ith neighbor of node u.  Note that we start indexing from 1.  For example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:    for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end  But, this turns out to be much slower than working with the structure directly, like    for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end   [ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.   The SparseMatrixCSC data structure  You can explore what is going on with the data structure by looking at some examples.  For example, here is a randomly weighted complete graph on 4 vertices, first displayed as a matrix:  gr = round(10*uniformWeight(completeGraph(4)))\n\n4x4 sparse matrix with 12 Float64 entries:\n    [2, 1]  =  3.0\n    [3, 1]  =  3.0\n    [4, 1]  =  6.0\n    [1, 2]  =  3.0\n    [3, 2]  =  1.0\n    [4, 2]  =  2.0\n    [1, 3]  =  3.0\n    [2, 3]  =  1.0\n    [4, 3]  =  7.0\n    [1, 4]  =  6.0\n    [2, 4]  =  2.0\n    [3, 4]  =  7.0\n\nfull(gr)\n\n4x4 Array{Float64,2}:\n 0.0  3.0  3.0  6.0\n 3.0  0.0  1.0  2.0\n 3.0  1.0  0.0  7.0\n 6.0  2.0  7.0  0.0  To see the underlying data structure, use  fieldnames .  fieldnames(gr)\n\n5-element Array{Symbol,1}:\n :m     \n :n     \n :colptr\n :rowval\n :nzval   m  and  n  are the dimensions of the matrix.\nThe entries of the matrix are stored in nzval.\ncolptr[i] is the index in nzval of the first nonzero entry\nin column i.  rowval tells you which rows in each column are nonzero.\nThe indices of the nonzero entries in column i are stored in \nrowval[colptr[i]] through rowval[colptr[i+1]-1].  gr.colptr \n\n5-element Array{Int64,1}:\n  1\n  4\n  7\n 10\n 13\n\n [gr.rowval gr.nzval]\n\n 12x2 Array{Float64,2}:\n 2.0  3.0\n 3.0  3.0\n 4.0  6.0\n 1.0  3.0\n 3.0  1.0\n 4.0  2.0\n 1.0  3.0\n 2.0  1.0\n 4.0  7.0\n 1.0  6.0\n 2.0  2.0\n 3.0  7.0", 
            "title": "Using sparse matrices as graphs"
        }, 
        {
            "location": "/Developing/index.html#documentation", 
            "text": "This documentation is still very rough.\nIt is generated by a combination of Markdown and semi-automatic generation.  The steps to generate and improve it are:   Edit Markdown files in the  docs  directory.  For example, you could use MacDown to do this.  If you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.  Add docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.  To build the API, type   include( docs/build.jl )   Run  mkdocs build  in the root directory to regenerate the documentation from the Markdown.   Parametric Types  A sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,  shortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)  Tv , sometimes written  Tval  denotes the types of the values, and  Ti  or  Tind  denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths,  start  is of type  Ti .  Inside the code, whenever we write something like  pArray = zeros(Ti,n) , it creates an array of zeros of type Ti.  Using these parameteric types is  much  faster than leaving the types unfixed.  Data structures:   IntHeap  a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.   Interface issue:  There are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.  Writing tests:  I haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.", 
            "title": "Documentation"
        }, 
        {
            "location": "/Developing/index.html#integration-with-other-packages", 
            "text": "There are other graph packages that we might want to sometimes use.   Graphs.jl  : I found this one to be too slow and awkward to be useful.  LightGraphs.jl  : this looks more promising.  We will have to check it out.", 
            "title": "Integration with other packages."
        }, 
        {
            "location": "/wholeAPI/index.html", 
            "text": "ErdosRenyi\n\n\nGenerate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement\n\n\nErdosRenyi(n::Integer, m::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:337\n\n\nErdosRenyiCluster\n\n\nGenerate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.\n\n\nErdosRenyiCluster(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:351\n\n\nErdosRenyiClusterFix\n\n\nLike an Erdos-Renyi cluster, but add back a tree so it has n vertices\n\n\nErdosRenyiClusterFix(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:364\n\n\nLaplacians\n\n\nA package for graph computations related to graph Laplacians\n\n\nGraphs are represented by sparse adjacency matrices, etc.\n\n\nRootedTree\n\n\nSummary:\n\n\ntype Laplacians.RootedTree{Tval,Tind} \n: Any\n\n\n\n\nFields:\n\n\nroot     :: Tind\nparent   :: Array{Tind,1}\nchildren :: Array{Tind,1}\nweights  :: Array{Tval,1}\nnumKids  :: Array{Tind,1}\nkidsPtr  :: Array{Tind,1}\n\n\n\n\nakpw\n\n\nThis is a wrapper for akpw!. It's slower, but won't modify the original graph. See akpw! documentation for more details.\n\n\nakpw(origMat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/akpwWeighted.jl:818\n\n\nakpw!\n\n\nConstructs a low stretch tree using the Alon, Karp, Peleg, West algorithm. This version (akpw! instead of akpw) modifies the graph slightly changing the edges weights, then changing them back, which may lead to floating point imprecisions. akpw! is faster (about 10-20%), but akpw doesn't have float imprecisions.\n\n\nThe function has a few options:\n\n\nkind: default is :max, which regards each edge weight as the inverse of its length (just like kruskal). If this is   set to anything else (e.g. :min), it will regard edge weight as length\n\n\nrandomClusters: default is false. This means the partition function searches for the beginning of the next cluster   in node order, rather than randomly choosing nodes. If this is set to false, it will   randomly choose the next node. This slows down akpw, but may produce better stretch.\n\n\nmetisClustering: default is false. If this is set to false, the graph will be partitioned   each time by metis, rather than by the akpw partitioning method.\n\n\nshuffleClusters: default is true. This preserves the \"reshuffleClusters\" method after each each graph is   partitioned into clusters. If set to false, the function will skip this step. May be faster   but have worse stretch.\n\n\nexponentialX: default is true, where the funciton exp(sqrt(log(nVertices) * log(log(nVertices)))) is used for X.   If set fo false, the function log(nVertices+1)/log(2) will be used for X instead. \n\n\nEXAMPLE:\n\n\n[2, 1]  =  0.631273 [3, 1]  =  0.40103 [1, 2]  =  0.631273 [4, 2]  =  0.147018 [1, 3]  =  0.40103 [4, 3]  =  0.772661 [2, 4]  =  0.147018 [3, 4]  =  0.772661\n\n\n  |\n  |\n  V\n\n\n\n\n[2, 1]  =  0.631273 [3, 1]  =  0.40103 [1, 2]  =  0.631273 [1, 3]  =  0.40103 [4, 3]  =  0.772661 [3, 4]  =  0.772661\n\n\nakpw!(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/akpwWeighted.jl:733\n\n\napr\n\n\ncomputes an approximate page rank vector from a starting set s, an alpha and an epsilon   algorithm follows the Anderson,Chung,Lang paper and Dan Spielman's notes\n\n\napr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, alpha::Float64, eps::Float64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/localClustering.jl:432\n\n\naugTreePrecon\n\n\nThis is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. The default is a randomized variant of Kruskal. It adds back 2sqrt(n) edges via augmentTree. With the right tree, it should never be too bad.\n\n\naugTreePrecon{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:188\n\n\naugTreeSolver\n\n\nThis is the solver that calls augTreePrecon\n\n\naugTreeSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:210\n\n\naugmentTree\n\n\nTakes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch\n\n\naugmentTree{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, k::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:138\n\n\nbackIndices\n\n\nsame as the above, but now the graph is in adjacency list form \n\n\ncomputes the back indices in a graph in O(M+N). works if for every edge (u,v), (v,u) is also in the graph \n\n\nbackIndices{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\nbackIndices{Tv1,Tv2}(G::Array{Array{Tuple{Tv1,Tv2},1},1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:35\n\n\nbiggestComp\n\n\nReturn the biggest component in a graph, as a graph\n\n\nbiggestComp(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:159\n\n\ncg\n\n\ncg(mat, b::Array{Float64,1})\ncg(mat, b::Array{Float32,1})\ncg(mat, b)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/pcg.jl:29\n\n\nchimera\n\n\nBuilds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nBuilds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace\n\n\nchimera(n::Integer)\nchimera(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:443\n\n\ncompConductance\n\n\nreturns the quality of the cut for a given graph and a given cut set s.   the result will be |outgoing edges| / min(|vertices in set|, |N - vertices in set|)\n\n\ncompConductance{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:136\n\n\ncompDepth\n\n\ncompDepth{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:311\n\n\ncompStretches\n\n\nCompute the stretched of every edge in \nmat\n with respect to the tree \ntree\n. Returns the answer as a sparse matrix with the same nonzero structure as \nmat\n. Assumes that \nmat\n is symmetric. \ntree\n should be the adjacency matrix of a spanning tree.\n\n\ncompStretches{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti})\ncompStretches{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:393\n\n\ncompleteBinaryTree\n\n\nThe complete binary tree on n vertices\n\n\ncompleteBinaryTree(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:139\n\n\ncompleteGraph\n\n\nThe complete graph\n\n\ncompleteGraph(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:17\n\n\ncomponents\n\n\nComputes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,\n\n\ngr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2\n\n\n\n\ncomponents{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:65\n\n\ndeg\n\n\ndeg{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:11\n\n\ndiagmat\n\n\nreturns the diagonal matrix(as a sparse matrix) of a graph\n\n\ndiagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:194\n\n\ndirEdgeVertexMat\n\n\nThe signed edge-vertex adjacency matrix\n\n\ndirEdgeVertexMat(A::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/toposort.jl:49\n\n\ndumb\n\n\ndumb{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/cutHeuristics.jl:102\n\n\nedgeVertexMat\n\n\nThe signed edge-vertex adjacency matrix\n\n\nedgeVertexMat(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:56\n\n\nfindEntries\n\n\nsimilar to findnz, but also returns 0 entries that have an edge in the sparse matrix \n\n\nfindEntries{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:114\n\n\nfloatGraph\n\n\nConvert the nonzero entries in a graph to Float64\n\n\nfloatGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:6\n\n\ngeneralizedNecklace\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.\n\n\ngeneralizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti\n:Integer}, k::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:214\n\n\ngeneralizedRing\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example, \n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\ngeneralizedRing(n::Int64, gens)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:38\n\n\ngetObound\n\n\ncomputes the number of edges leaving s \n\n\ngetObound{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:167\n\n\ngetVolume\n\n\ncomputes the volume of subset s in an unweighted graph G \n\n\ngetVolume{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:149\n\n\ngrid2\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\ngrid2(n::Int64)\ngrid2(n::Int64, m::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:159\n\n\ngrid2coords\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\ngrid2coords(n::Int64, m::Int64)\ngrid2coords(n)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:163\n\n\ngrownGraph\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.    \n\n\ngrownGraph(n::Int64, k::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:202\n\n\ngrownGraphD\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\ngrownGraphD(n::Int64, k::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:234\n\n\nhyperCube\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\nhyperCube(d::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:125\n\n\nisConnected\n\n\nReturns true if graph is connected.  Calls components.\n\n\nisConnected(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:113\n\n\njoinGraphs\n\n\ncreate a disjoint union of graphs a and b,  and then put k random edges between them\n\n\njoinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:100\n\n\nkruskal\n\n\nUses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:max if you want the max spanning tree. It returns it a a graph\n\n\nkruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:407\n\n\nlap\n\n\nCreate a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry\n\n\nlap(a)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:12\n\n\nlapChol\n\n\nlapWrapSolver\n\n\nTakes a solver for solving nonsingular sdd systems, and returns a solver for solving Laplacian systems. The optional args tol and maxits are not necessarily taken by all solvers.  But, if they are, one can pass them here\n\n\nlapWrapSolver(solver)\nlapWrapSolver(solver, la::AbstractArray{T,N})\nlapWrapSolver(solver, la::AbstractArray{T,N}, b)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/solvers.jl:108\n\n\nlocalImprove\n\n\nlocalImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1}; epsSigma=-1.0, err=1e-10, maxSize = max(G.n, G.m)\n\n\nThe LocalImprove function, from the Orrechia-Zhu paper \n\n\nG is the given graph, A is the initial set    epsSigma is a measure of the quality of the returning set (the smaller the better)   err is the numerical error considered throughout the algorithm   maxSize is the maximum allowed size for the flow graph at any iteration of the algorithm\n\n\nlocalImprove{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, A::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/localClustering.jl:16\n\n\nmapweight\n\n\nCreate a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write\n\n\nb = mapweight(a, x-\nrand(1)[1])\n\n\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:29\n\n\nmatToTree\n\n\nmatToTree{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\nmatToTree{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, root::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:32\n\n\nmatToTreeDepth\n\n\nmatToTreeDepth{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\nmatToTreeDepth{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, root::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:98\n\n\nmaxflow\n\n\nimplementation of Dinic's algorithm. computes the maximum flow and min-cut in G between s and t    we consider the adjacency matrix to be the capacity matrix \n\n\nmaxflow{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Int64, t::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/flow.jl:7\n\n\nnbri\n\n\nnbri{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:12\n\n\nnbrs\n\n\nnbrs{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:14\n\n\npathFromParents\n\n\npathFromParents(parents, y)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:215\n\n\npathGraph\n\n\nThe path graph on n vertices\n\n\npathGraph(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:8\n\n\npcg\n\n\npcg(mat, b::Array{Float64,1}, pre)\npcg(mat, b::Array{Float32,1}, pre)\npcg(mat, b, pre)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/pcg.jl:42\n\n\nplotGraph\n\n\nPlots graph gr with coordinates (x,y)\n\n\nplotGraph(gr, x, y)\nplotGraph(gr, x, y, color)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:118\n\n\nprefAttach\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.\n\n\nprefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:258\n\n\nprim\n\n\nprim(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:438\n\n\nprn\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)\n\n\nThe PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper\n\n\ns is a set of starting vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]\n\n\nphi is a bound on the quality of the conductance of the cut - the smaller the phi, the higher the quality   b is used to handle precision throughout the algorithm - the higher the b, the smaller the eps\n\n\nprn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1}, phi::Float64, b::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/localClustering.jl:366\n\n\nproductGraph\n\n\nThe Cartesian product of two graphs.  When applied to two paths, it gives a grid.\n\n\nproductGraph(a0::SparseMatrixCSC{Tv,Ti\n:Integer}, a1::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:47\n\n\npureRandomGraph\n\n\nGenerate a random graph with n vertices from one of our natural distributions\n\n\npureRandomGraph(n::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:379\n\n\nrandGenRing\n\n\nA random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution\n\n\nrandGenRing(n::Int64, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:62\n\n\nrandMatching\n\n\nA random matching on n vertices\n\n\nrandMatching(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:174\n\n\nrandRegular\n\n\nA sum of k random matchings on n vertices\n\n\nrandRegular(n::Int64, k::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:187\n\n\nrandWeight\n\n\nApplies one of a number of random weighting schemes to the edges of the graph\n\n\nrandWeight(a)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:520\n\n\nrandishKruskal\n\n\nrandishKruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/randTrees.jl:10\n\n\nrandishPrim\n\n\nrandishPrim{Tval,Tind}(mat::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/randTrees.jl:47\n\n\nrandperm\n\n\n..  randperm([rng,] n)\n\nConstruct a random permutation of length ``n``. The optional ``rng`` argument\nspecifies a random number generator, see :ref:`Random Numbers \nrandom-numbers\n`.\n\n\n\n\nRandomly permutes the vertex indices\n\n\nrandperm(r::AbstractRNG, n::Integer)\nrandperm(n::Integer)\nrandperm(mat::AbstractArray{T,2})\nrandperm(f::Expr)\n\n\n\n\nat random.jl:1341\n\n\nreadIJ\n\n\nto read a simple edge list, each line being an (i, j) pair\n\n\nreadIJ(filename::AbstractString)\nreadIJ(filename::AbstractString, sep)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/IO.jl:4\n\n\nreadIJV\n\n\nto read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this\n\n\n [ai,aj,av] = find(triu(a));\n\n dlmwrite('graph.txt',[ai,aj,av],'precision',9);\n\n\n\n\nreadIJV(filename::AbstractString)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/IO.jl:25\n\n\nrefineCut\n\n\nModifies a cluster by adding/removing vertices based on Deg_external - Deg_Internal\nEach vertex can be added in/removed only once\nUses O(M + N) memory\n\n\n\n\nrefineCut{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Int64,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/cutHeuristics.jl:9\n\n\nringGraph\n\n\nThe simple ring on n vertices\n\n\nringGraph(n::Int64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:23\n\n\nsetValue\n\n\nsets the value of a certain edge in a sparse graph; value can be 0 without the edges dissapearing \n\n\nsetValue{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti, a::Tv)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:29\n\n\nshortIntGraph\n\n\nConvert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much\n\n\nshortIntGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:3\n\n\nshortestPathTree\n\n\nComputes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:\n\n\na = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0\n\n\n\n\nshortestPathTree(a, start)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:239\n\n\nshortestPaths\n\n\nComputes the lenghts of shortest paths from \nstart\n. Returns both a vector of the lenghts, and the parent array in the shortest path tree.\n\n\nThis algorithm treats edge weights as reciprocals of distances. DOC BETTER\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:175\n\n\nspectralCoords\n\n\nComputes the spectral coordinates of a graph\n\n\nspectralCoords(a)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:163\n\n\nspectralDrawing\n\n\nComputes spectral coordinates, and then uses plotGraph to draw\n\n\nspectralDrawing(a)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:155\n\n\nsubsampleEdges\n\n\nCreate a new graph from the old, but keeping edge edge with probability \np\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:65\n\n\ntarjanStretch\n\n\ntarjanStretch{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, depth::Array{Tv,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/treeAlgs.jl:334\n\n\ntoUnitVector\n\n\ncreates a unit vector of length n from a given set of integers, with weights based on the number of occurences\n\n\ntoUnitVector(a::Array{Int64,1}, n)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:172\n\n\ntoposort\n\n\ntoposort{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/toposort.jl:13\n\n\ntwoLift\n\n\nCreats a 2-lift of a.  \nflip\n is a boolean indicating which edges cross\n\n\ntwoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:88\n\n\nuniformWeight\n\n\nPut a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.\n\n\nuniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:38\n\n\nuniformWeight!\n\n\nSet the weight of every edge to 1\n\n\nuniformWeight!(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:42\n\n\nunweight\n\n\nCreate a new graph in that is the same as the original, but with all edge weights 1\n\n\nunweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphOps.jl:16\n\n\nvecToComps\n\n\nThis turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,\n\n\ncomps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]\n\n\n\n\nvecToComps{Ti}(compvec::Array{Ti,1})\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphAlgs.jl:136\n\n\nwdeg\n\n\nfinds the weighted degree of a vertex in the graph \n\n\nwdeg{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:19\n\n\nweighti\n\n\nweighti{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphUtils.jl:13\n\n\nwriteIJV\n\n\nWrites the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:\n\n\n dl = dlmread('graph.txt');\n\n a = sparse(dl(:,1),dl(:,2),dl(:,3));\n\n n = max(size(a))\n\n a(n,n) = 0;\n\n a = a + a';\n\n\n\n\nwriteIJV(filename::AbstractString, mat)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/IO.jl:52\n\n\nwtedChimera\n\n\nBuilds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nGenerate a chimera, and then apply a random weighting scheme\n\n\nwtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/.julia/v0.4/Laplacians/src/graphGenerators.jl:585", 
            "title": "Whole API"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html", 
            "text": "graphGenerators\n\n\npathGraph\n\n\nThe path graph on n vertices\n\n\npathGraph(n::Int64)\n\n\n\n\ngraphGenerators.jl:8\n\n\ncompleteGraph\n\n\nThe complete graph\n\n\ncompleteGraph(n::Int64)\n\n\n\n\ngraphGenerators.jl:17\n\n\nringGraph\n\n\nThe simple ring on n vertices\n\n\nringGraph(n::Int64)\n\n\n\n\ngraphGenerators.jl:23\n\n\ngeneralizedRing\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example, \n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\ngeneralizedRing(n::Int64, gens)\n\n\n\n\ngraphGenerators.jl:38\n\n\nrandGenRing\n\n\nA random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution\n\n\nrandGenRing(n::Int64, k::Integer)\n\n\n\n\ngraphGenerators.jl:62\n\n\nhyperCube\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\nhyperCube(d::Int64)\n\n\n\n\ngraphGenerators.jl:125\n\n\ncompleteBinaryTree\n\n\nThe complete binary tree on n vertices\n\n\ncompleteBinaryTree(n::Int64)\n\n\n\n\ngraphGenerators.jl:139\n\n\ngrid2\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\ngrid2(n::Int64)\ngrid2(n::Int64, m::Int64)\n\n\n\n\ngraphGenerators.jl:159\n\n\ngrid2coords\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\ngrid2coords(n::Int64, m::Int64)\ngrid2coords(n)\n\n\n\n\ngraphGenerators.jl:163\n\n\nrandMatching\n\n\nA random matching on n vertices\n\n\nrandMatching(n::Int64)\n\n\n\n\ngraphGenerators.jl:174\n\n\nrandRegular\n\n\nA sum of k random matchings on n vertices\n\n\nrandRegular(n::Int64, k::Int64)\n\n\n\n\ngraphGenerators.jl:187\n\n\ngrownGraph\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.    \n\n\ngrownGraph(n::Int64, k::Int64)\n\n\n\n\ngraphGenerators.jl:202\n\n\ngrownGraphD\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\ngrownGraphD(n::Int64, k::Int64)\n\n\n\n\ngraphGenerators.jl:234\n\n\nprefAttach\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.\n\n\nprefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\ngraphGenerators.jl:258\n\n\nErdosRenyi\n\n\nGenerate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement\n\n\nErdosRenyi(n::Integer, m::Integer)\n\n\n\n\ngraphGenerators.jl:337\n\n\nErdosRenyiCluster\n\n\nGenerate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.\n\n\nErdosRenyiCluster(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:351\n\n\nErdosRenyiClusterFix\n\n\nLike an Erdos-Renyi cluster, but add back a tree so it has n vertices\n\n\nErdosRenyiClusterFix(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:364\n\n\npureRandomGraph\n\n\nGenerate a random graph with n vertices from one of our natural distributions\n\n\npureRandomGraph(n::Integer)\n\n\n\n\ngraphGenerators.jl:379\n\n\nchimera\n\n\nBuilds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nBuilds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace\n\n\nchimera(n::Integer)\nchimera(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:443\n\n\nrandWeight\n\n\nApplies one of a number of random weighting schemes to the edges of the graph\n\n\nrandWeight(a)\n\n\n\n\ngraphGenerators.jl:520\n\n\nwtedChimera\n\n\nBuilds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nGenerate a chimera, and then apply a random weighting scheme\n\n\nwtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:585", 
            "title": "graphGenerators"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#graphgenerators", 
            "text": "pathGraph  The path graph on n vertices  pathGraph(n::Int64)  graphGenerators.jl:8  completeGraph  The complete graph  completeGraph(n::Int64)  graphGenerators.jl:17  ringGraph  The simple ring on n vertices  ringGraph(n::Int64)  graphGenerators.jl:23  generalizedRing  A generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example,   generalizedRing(17, [1 5])  generalizedRing(n::Int64, gens)  graphGenerators.jl:38  randGenRing  A random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution  randGenRing(n::Int64, k::Integer)  graphGenerators.jl:62  hyperCube  The d dimensional hypercube.  Has 2^d vertices  hyperCube(d::Int64)  graphGenerators.jl:125  completeBinaryTree  The complete binary tree on n vertices  completeBinaryTree(n::Int64)  graphGenerators.jl:139  grid2  An n-by-m grid graph.  iostropy is the weighting on edges in one direction.  grid2(n::Int64)\ngrid2(n::Int64, m::Int64)  graphGenerators.jl:159  grid2coords  Coordinates for plotting the vertices of the n-by-m grid graph  grid2coords(n::Int64, m::Int64)\ngrid2coords(n)  graphGenerators.jl:163  randMatching  A random matching on n vertices  randMatching(n::Int64)  graphGenerators.jl:174  randRegular  A sum of k random matchings on n vertices  randRegular(n::Int64, k::Int64)  graphGenerators.jl:187  grownGraph  Create a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.      grownGraph(n::Int64, k::Int64)  graphGenerators.jl:202  grownGraphD  Like a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices  grownGraphD(n::Int64, k::Int64)  graphGenerators.jl:234  prefAttach  A preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.  prefAttach(n::Int64, k::Int64, p::Float64)  graphGenerators.jl:258  ErdosRenyi  Generate a random graph on n vertices with m edges. The actual number of edges will probably be smaller, as we sample with replacement  ErdosRenyi(n::Integer, m::Integer)  graphGenerators.jl:337  ErdosRenyiCluster  Generate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.  ErdosRenyiCluster(n::Integer, k::Integer)  graphGenerators.jl:351  ErdosRenyiClusterFix  Like an Erdos-Renyi cluster, but add back a tree so it has n vertices  ErdosRenyiClusterFix(n::Integer, k::Integer)  graphGenerators.jl:364  pureRandomGraph  Generate a random graph with n vertices from one of our natural distributions  pureRandomGraph(n::Integer)  graphGenerators.jl:379  chimera  Builds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  Builds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace  chimera(n::Integer)\nchimera(n::Integer, k::Integer)  graphGenerators.jl:443  randWeight  Applies one of a number of random weighting schemes to the edges of the graph  randWeight(a)  graphGenerators.jl:520  wtedChimera  Builds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  Generate a chimera, and then apply a random weighting scheme  wtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)  graphGenerators.jl:585", 
            "title": "graphGenerators"
        }, 
        {
            "location": "/IOAPI/index.html", 
            "text": "IO\n\n\nreadIJ\n\n\nto read a simple edge list, each line being an (i, j) pair\n\n\nreadIJ(filename::AbstractString)\nreadIJ(filename::AbstractString, sep)\n\n\n\n\nIO.jl:4\n\n\nreadIJV\n\n\nto read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this\n\n\n [ai,aj,av] = find(triu(a));\n\n dlmwrite('graph.txt',[ai,aj,av],'precision',9);\n\n\n\n\nreadIJV(filename::AbstractString)\n\n\n\n\nIO.jl:25\n\n\nwriteIJV\n\n\nWrites the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:\n\n\n dl = dlmread('graph.txt');\n\n a = sparse(dl(:,1),dl(:,2),dl(:,3));\n\n n = max(size(a))\n\n a(n,n) = 0;\n\n a = a + a';\n\n\n\n\nwriteIJV(filename::AbstractString, mat)\n\n\n\n\nIO.jl:52", 
            "title": "IO"
        }, 
        {
            "location": "/IOAPI/index.html#io", 
            "text": "readIJ  to read a simple edge list, each line being an (i, j) pair  readIJ(filename::AbstractString)\nreadIJ(filename::AbstractString, sep)  IO.jl:4  readIJV  to read a simple edge list, each line being an (i, j, v) pair. The parens should not be there in the format, just commas separating. To generate this format in Matlab, you just need to be careful to write the vertex indices with sufficient precision.  For example, you can do this   [ai,aj,av] = find(triu(a));  dlmwrite('graph.txt',[ai,aj,av],'precision',9);  readIJV(filename::AbstractString)  IO.jl:25  writeIJV  Writes the upper portion of a matrix in ijv format, one row for each edge, separated by commas.  Only writes the upper triangular portion. The result can be read from Matlab like this:   dl = dlmread('graph.txt');  a = sparse(dl(:,1),dl(:,2),dl(:,3));  n = max(size(a))  a(n,n) = 0;  a = a + a';  writeIJV(filename::AbstractString, mat)  IO.jl:52", 
            "title": "IO"
        }, 
        {
            "location": "/graphOpsAPI/index.html", 
            "text": "graphOps\n\n\nshortIntGraph\n\n\nConvert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much\n\n\nshortIntGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:3\n\n\nfloatGraph\n\n\nConvert the nonzero entries in a graph to Float64\n\n\nfloatGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:6\n\n\nlap\n\n\nCreate a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry\n\n\nlap(a)\n\n\n\n\ngraphOps.jl:12\n\n\nunweight\n\n\nCreate a new graph in that is the same as the original, but with all edge weights 1\n\n\nunweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\ngraphOps.jl:16\n\n\nmapweight\n\n\nCreate a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write\n\n\nb = mapweight(a, x-\nrand(1)[1])\n\n\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)\n\n\n\n\ngraphOps.jl:29\n\n\nuniformWeight\n\n\nPut a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.\n\n\nuniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\ngraphOps.jl:38\n\n\nuniformWeight!\n\n\nSet the weight of every edge to 1\n\n\nuniformWeight!(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:42\n\n\nproductGraph\n\n\nThe Cartesian product of two graphs.  When applied to two paths, it gives a grid.\n\n\nproductGraph(a0::SparseMatrixCSC{Tv,Ti\n:Integer}, a1::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:47\n\n\nedgeVertexMat\n\n\nThe signed edge-vertex adjacency matrix\n\n\nedgeVertexMat(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:56\n\n\nsubsampleEdges\n\n\nCreate a new graph from the old, but keeping edge edge with probability \np\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n\n\n\ngraphOps.jl:65\n\n\ntwoLift\n\n\nCreats a 2-lift of a.  \nflip\n is a boolean indicating which edges cross\n\n\ntwoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)\n\n\n\n\ngraphOps.jl:88\n\n\njoinGraphs\n\n\ncreate a disjoint union of graphs a and b,  and then put k random edges between them\n\n\njoinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)\n\n\n\n\ngraphOps.jl:100\n\n\nplotGraph\n\n\nPlots graph gr with coordinates (x,y)\n\n\nplotGraph(gr, x, y)\nplotGraph(gr, x, y, color)\n\n\n\n\ngraphOps.jl:118\n\n\nspectralDrawing\n\n\nComputes spectral coordinates, and then uses plotGraph to draw\n\n\nspectralDrawing(a)\n\n\n\n\ngraphOps.jl:155\n\n\nspectralCoords\n\n\nComputes the spectral coordinates of a graph\n\n\nspectralCoords(a)\n\n\n\n\ngraphOps.jl:163\n\n\ntoUnitVector\n\n\ncreates a unit vector of length n from a given set of integers, with weights based on the number of occurences\n\n\ntoUnitVector(a::Array{Int64,1}, n)\n\n\n\n\ngraphOps.jl:172\n\n\ndiagmat\n\n\nreturns the diagonal matrix(as a sparse matrix) of a graph\n\n\ndiagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphOps.jl:194\n\n\ngeneralizedNecklace\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.\n\n\ngeneralizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti\n:Integer}, k::Int64)\n\n\n\n\ngraphOps.jl:214", 
            "title": "graphOps"
        }, 
        {
            "location": "/graphOpsAPI/index.html#graphops", 
            "text": "shortIntGraph  Convert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much  shortIntGraph(a::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:3  floatGraph  Convert the nonzero entries in a graph to Float64  floatGraph(a::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:6  lap  Create a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry  lap(a)  graphOps.jl:12  unweight  Create a new graph in that is the same as the original, but with all edge weights 1  unweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  graphOps.jl:16  mapweight  Create a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write  b = mapweight(a, x- rand(1)[1])  mapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)  graphOps.jl:29  uniformWeight  Put a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.  uniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  graphOps.jl:38  uniformWeight!  Set the weight of every edge to 1  uniformWeight!(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:42  productGraph  The Cartesian product of two graphs.  When applied to two paths, it gives a grid.  productGraph(a0::SparseMatrixCSC{Tv,Ti :Integer}, a1::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:47  edgeVertexMat  The signed edge-vertex adjacency matrix  edgeVertexMat(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:56  subsampleEdges  Create a new graph from the old, but keeping edge edge with probability  p  subsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)  graphOps.jl:65  twoLift  Creats a 2-lift of a.   flip  is a boolean indicating which edges cross  twoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)  graphOps.jl:88  joinGraphs  create a disjoint union of graphs a and b,  and then put k random edges between them  joinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)  graphOps.jl:100  plotGraph  Plots graph gr with coordinates (x,y)  plotGraph(gr, x, y)\nplotGraph(gr, x, y, color)  graphOps.jl:118  spectralDrawing  Computes spectral coordinates, and then uses plotGraph to draw  spectralDrawing(a)  graphOps.jl:155  spectralCoords  Computes the spectral coordinates of a graph  spectralCoords(a)  graphOps.jl:163  toUnitVector  creates a unit vector of length n from a given set of integers, with weights based on the number of occurences  toUnitVector(a::Array{Int64,1}, n)  graphOps.jl:172  diagmat  returns the diagonal matrix(as a sparse matrix) of a graph  diagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})  graphOps.jl:194  generalizedNecklace  Constructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.  generalizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti :Integer}, k::Int64)  graphOps.jl:214", 
            "title": "graphOps"
        }, 
        {
            "location": "/graphAlgsAPI/index.html", 
            "text": "graphAlgs\n\n\ncomponents\n\n\nComputes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,\n\n\ngr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2\n\n\n\n\ncomponents{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphAlgs.jl:65\n\n\nisConnected\n\n\nReturns true if graph is connected.  Calls components.\n\n\nisConnected(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphAlgs.jl:113\n\n\nvecToComps\n\n\nThis turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,\n\n\ncomps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]\n\n\n\n\nvecToComps{Ti}(compvec::Array{Ti,1})\n\n\n\n\ngraphAlgs.jl:136\n\n\nbiggestComp\n\n\nReturn the biggest component in a graph, as a graph\n\n\nbiggestComp(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphAlgs.jl:159\n\n\nshortestPaths\n\n\nComputes the lenghts of shortest paths from \nstart\n. Returns both a vector of the lenghts, and the parent array in the shortest path tree.\n\n\nThis algorithm treats edge weights as reciprocals of distances. DOC BETTER\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphAlgs.jl:175\n\n\nshortestPathTree\n\n\nComputes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:\n\n\na = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0\n\n\n\n\nshortestPathTree(a, start)\n\n\n\n\ngraphAlgs.jl:239\n\n\nkruskal\n\n\nUses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:max if you want the max spanning tree. It returns it a a graph\n\n\nkruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphAlgs.jl:407", 
            "title": "graphAlgs"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#graphalgs", 
            "text": "components  Computes the connected components of a graph. Returns them as a vector of length equal to the number of vertices. The vector numbers the components from 1 through the maximum number. For example,  gr = ErdosRenyi(10,11)\nc = components(gr)\n\n10-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 1\n 1\n 1\n 3\n 2  components{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  graphAlgs.jl:65  isConnected  Returns true if graph is connected.  Calls components.  isConnected(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphAlgs.jl:113  vecToComps  This turns a component vector, like that generated by components, into an array of arrays of indices of vertices in each component.  For example,  comps = vecToComps(c)\n\n3-element Array{Array{Int64,1},1}:\n [1,2,3,4,6,7,8]\n [5,10]\n [9]  vecToComps{Ti}(compvec::Array{Ti,1})  graphAlgs.jl:136  biggestComp  Return the biggest component in a graph, as a graph  biggestComp(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphAlgs.jl:159  shortestPaths  Computes the lenghts of shortest paths from  start . Returns both a vector of the lenghts, and the parent array in the shortest path tree.  This algorithm treats edge weights as reciprocals of distances. DOC BETTER  shortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  graphAlgs.jl:175  shortestPathTree  Computes the shortest path tree, and returns it as a sparse matrix. Treats edge weights as reciprocals of lengths. For example:  a = [0 2 1; 2 0 3; 1 3 0]\ntr = full(shortestPathTree(sparse(a),1))\n\n3x3 Array{Float64,2}:\n 0.0  2.0  0.0\n 2.0  0.0  3.0\n 0.0  3.0  0.0  shortestPathTree(a, start)  graphAlgs.jl:239  kruskal  Uses Kruskal's algorithm to compute a minimum (or maximum) spanning tree. Set kind=:max if you want the max spanning tree. It returns it a a graph  kruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})  graphAlgs.jl:407", 
            "title": "graphAlgs"
        }, 
        {
            "location": "/cutPageRankAPI/index.html", 
            "text": "cutPageRank\n\n\npr\n\n\ncomputes a page rank vector satisfying p = a/n * 1 + (1 - a) * W * p \n\n\npr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, alpha::Float64)\n\n\n\n\ncutPageRank.jl:20\n\n\nppr\n\n\ncomputes the personal page rank vector from a starting vector s and an alpha; operates with lazy walk matrix \n\n\nppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64)\nppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, niter::Int64)\n\n\n\n\ncutPageRank.jl:38\n\n\napr\n\n\ncomputes an approximate page rank vector from a starting vector s, an alpha and an epsilon \n\n\napr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, eps::Float64)\n\n\n\n\ncutPageRank.jl:71\n\n\nprn\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)\n\n\nthe PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper\n\n\nv is a set of vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]\n\n\nprn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)\n\n\n\n\ncutPageRank.jl:115", 
            "title": "cutPageRank"
        }, 
        {
            "location": "/cutPageRankAPI/index.html#cutpagerank", 
            "text": "pr  computes a page rank vector satisfying p = a/n * 1 + (1 - a) * W * p   pr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, alpha::Float64)  cutPageRank.jl:20  ppr  computes the personal page rank vector from a starting vector s and an alpha; operates with lazy walk matrix   ppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64)\nppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, niter::Int64)  cutPageRank.jl:38  apr  computes an approximate page rank vector from a starting vector s, an alpha and an epsilon   apr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, eps::Float64)  cutPageRank.jl:71  prn  prn{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)  the PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper  v is a set of vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]  prn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)  cutPageRank.jl:115", 
            "title": "cutPageRank"
        }, 
        {
            "location": "/solversAPI/index.html", 
            "text": "solvers\n\n\nlapWrapSolver\n\n\nTakes a solver for solving nonsingular sdd systems, and returns a solver for solving Laplacian systems. The optional args tol and maxits are not necessarily taken by all solvers.  But, if they are, one can pass them here\n\n\nlapWrapSolver(solver)\nlapWrapSolver(solver, la::AbstractArray{T,N})\nlapWrapSolver(solver, la::AbstractArray{T,N}, b)\n\n\n\n\nsolvers.jl:108\n\n\naugmentTree\n\n\nTakes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch\n\n\naugmentTree{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, k::Ti)\n\n\n\n\nsolvers.jl:138\n\n\naugTreePrecon\n\n\nThis is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. The default is a randomized variant of Kruskal. It adds back 2sqrt(n) edges via augmentTree. With the right tree, it should never be too bad.\n\n\naugTreePrecon{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nsolvers.jl:188\n\n\naugTreeSolver\n\n\nThis is the solver that calls augTreePrecon\n\n\naugTreeSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nsolvers.jl:210", 
            "title": "solvers"
        }, 
        {
            "location": "/solversAPI/index.html#solvers", 
            "text": "lapWrapSolver  Takes a solver for solving nonsingular sdd systems, and returns a solver for solving Laplacian systems. The optional args tol and maxits are not necessarily taken by all solvers.  But, if they are, one can pass them here  lapWrapSolver(solver)\nlapWrapSolver(solver, la::AbstractArray{T,N})\nlapWrapSolver(solver, la::AbstractArray{T,N}, b)  solvers.jl:108  augmentTree  Takes as input a tree and an adjacency matrix of a graph. It then computes the stretch of every edge of the graph wrt the tree.  It then adds back the k edges of highest stretch, and k edges sampled according to stretch  augmentTree{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, k::Ti)  solvers.jl:138  augTreePrecon  This is an augmented spanning tree preconditioner for diagonally dominant linear systems.  It takes as optional input a tree growing algorithm. The default is a randomized variant of Kruskal. It adds back 2sqrt(n) edges via augmentTree. With the right tree, it should never be too bad.  augTreePrecon{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})  solvers.jl:188  augTreeSolver  This is the solver that calls augTreePrecon  augTreeSolver{Tv,Ti}(ddmat::SparseMatrixCSC{Tv,Ti})  solvers.jl:210", 
            "title": "solvers"
        }
    ]
}