{
    "docs": [
        {
            "location": "/yinsGraph/", 
            "text": "yinsGraph\n\n\nTo install yinsGraph\n\n\nTo use yinsGraph\n\n\nGraph generators:\n\n\nOperations on Graphs:\n\n\nFundamental Graph Algorithms:\n\n\nSolving Linear equations:\n\n\n\n\n\n\nTo develop yinsGraph\n\n\nUsing sparse matrices as graphs\n\n\nParametric Types\n\n\nData structures:\n\n\nInterface issue:\n\n\nWriting tests:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyinsGraph\n\n\nyinsGraph is a package that I (Dan) am writing to explore and manipulate graphs in Julia.  The graphs are represented as sparse matrices.  The particular class in Julia is called a SparseMatrixCSC.  The reasons for this are:\n\n\n\n\nThey are fast, and\n\n\nWe want to do linear algebra with them, so matrices help.\n\n\n\n\nYou can probably learn more about the CSC (Compressed Sparse Column) format by googling it.  \n\n\nSo far, speed tests of code that I've written for connected components, shorest paths, and minimum spanning trees have been as fast or faster than the previous routines I could call from Matlab.  \n\n\nTo install yinsGraph\n\n\nyou will need a number of packages.\nYou install these like\n\n\nPkg.add(\nPyCall\n)\nPkg.add(\nPyPlot\n)\nPkg.add(\nDataStructures\n)\n\n\n\n\n\nI also recommend the Optim package.\n\n\nI think you need to install matplotlib in python before PyPlot.\n\nLook at this page for more information: https://github.com/stevengj/PyPlot.jl\n\n\nI'm not sure if there are any others.  If you find that there are, please list them above.\n\n\nTo use yinsGraph\n\n\nExamples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.  \n\n\nRight now, the notebooks worth looking at are:\n\n\n\n\nyinsGraph\n - usage, demo, and speed tests\n\n\n\n\nSolvers\n - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.\n\n\n\n\n\n\n[ ] The implementation of CG in IterativeSolvers sort of sucks, as I now see in the tests.  It is allocating way to much memory.  It should be fixed either by devectorizing (see Julia Performance Tips), or by using BLAS routines.\n\n\n\n\n\n\n(I suggest that you open the html in your browser)\n\n\nGraph generators:\n\n\n \nreadIJ\n(\nfilename\n::\nString\n)\n\n \nreadIJV\n(\nfilename\n::\nString\n)\n\n \nwriteIJV\n(\nfilename\n::\nString\n,\n \nmat\n)\n\n \nringGraph\n(\nn\n::\nInt64\n)\n\n \ngeneralizedRing\n(\nn\n::\nInt64\n,\n \ngens\n)\n\n \nrandMatching\n(\nn\n::\nInt64\n)\n\n \nrandRegular\n(\nn\n::\nInt64\n,\n \nk\n::\nInt64\n)\n\n \ngrownGraph\n(\nn\n::\nInt64\n,\n \nk\n::\nInt64\n)\n\n \ngrownGraphD\n(\nn\n::\nInt64\n,\n \nk\n::\nInt64\n)\n\n \nprefAttach\n(\nn\n::\nInt64\n,\n \nk\n::\nInt64\n,\n \np\n::\nFloat64\n)\n\n \nhyperCube\n(\nd\n::\nInt64\n)\n\n \ncompleteBinaryTree\n(\nn\n::\nInt64\n)\n\n \ngrid2\n(\nn\n::\nInt64\n)\n\n \ngrid2\n(\nn\n::\nInt64\n,\n \nm\n::\nInt64\n;\n \nisotropy\n=\n1\n)\n\n \ngrid2coords\n(\nn\n::\nInt64\n,\n \nm\n::\nInt64\n)\n\n\n\n\n\n\n\n\n[ ] The types in the arguments of those should probably be more flexible/general.\n\n\n\n\nFor example, to generate a 4-by-5 grid, you type\n\n\ngraph\n \n=\n \ngrid2\n(\n4\n,\n5\n)\n\n\n\n\n\n\nOperations on Graphs:\n\n\n\n\nshortIntGraph\n  for converting the index type of a graph to an Int32.  \n\n\nlap\n  to produce the laplacian of a graph\n\n\n[ ] Maybe this should grab the upper triangular part, and symmetrize first.     \n\n\nunweight\n - change all the weights to 1\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind},f)\n  to apply the function f to the weight of every edge.\n\n\nuniformWeight\n  an example of mapweight.  It ignores the weight, and maps every weight to a random in [0,1]\n\n\nproductGraph(a0::SparseMatrixCSC, a1::SparseMatrixCSC)\n the cartesian product.  Given two paths it makes a grid.\n\n\nedgeVertexMat(mat::SparseMatrixCSC)\n  signed edge vertex matrix\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n  produce a new graph that keeps each edge with probability p.\n\n\ntwoLift(a, k)\n create a 2-lift of a with k flipped edges.  If k is unspecified, this generates a random 2-lift. \n\n\njoinGraphs(a, b, k)\n create a disjoint union of a and b, and add k random edges between them\n\n\nplotGraph(gr,x,y,color=[0,0,1];dots=true,setaxis=true,number=false)\n\n\nspectralDrawing(graph)\n\n\n\n\nFundamental Graph Algorithms:\n\n\n\n\ncomponents\n computes connected components, returns as a vector\n\n\nvecToComps\n turns into an array with a list of vertices in each component\n\n\nshortestPaths(mat, start)\n  returns an array of distances,\n    and pointers to the node closest (parent array)\n\n\nkruskal(mat; kind=:min)\n  to get a max tree, use \nkind = :max\n\n    returns it as a sparse matrix.\n\n\n\n\nSolving Linear equations:\n\n\nWe have implemented Conjugate Gradient (cg) and the Preconditioned Conjugate Gradient (pcg).  These implementations use BLAS when they can, and a slower routine for data types like BigFloat.\n\n\nTo learn more, read \nsolvers.md\n.\n\n\nTo develop yinsGraph\n\n\nJust go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"\n\n\nI think that each file should contain a manifest up top listing the functions and types that it provides.  They should be divided up into those that are for internal use only, and those that should be exported.  Old code that didn't work well, but which you want to keep for reference should go at the end.\n\n\nUsing sparse matrices as graphs\n\n\nThe routines \ndeg\n, \nnbri\n and \nweighti\n will let you treat a sparse matrix like a graph.\n\n\ndeg(graph, u)\n is the degree of node u.\n\nnbri(graph, u, i)\n is the ith neighbor of node u.\n\nweighti(graph, u, i)\n is the weight of the edge to the ith neighbor of node u.\n\n\nNote that we start indexing from 1.\n\n\nFor example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:\n\n\n  \nfor\n \ni\n \nin\n \n1\n:\ndeg\n(\nmat\n,\n \nv\n)\n\n     \nnbr\n \n=\n \nnbri\n(\nmat\n,\n \nv\n,\n \ni\n)\n\n     \nwt\n \n=\n \nweighti\n(\nmat\n,\n \nv\n,\n \ni\n)\n\n     \nfoo\n(\nv\n,\n \nnbr\n,\n \nwt\n)\n\n  \nend\n\n\n\n\n\n\nBut, this turns out to be much slower than working with the structure directly, like\n\n\n  \nfor\n \nind\n \nin\n \nmat\n.\ncolptr\n[\nv\n]:(\nmat\n.\ncolptr\n[\nv\n+\n1\n]\n-\n1\n)\n\n      \nnbr\n \n=\n \nmat\n.\nrowval\n[\nind\n]\n\n      \nwt\n \n=\n \nmat\n.\nnzval\n[\nind\n]\n\n      \nfoo\n(\nv\n,\n \nnbr\n,\n \nwt\n)\n\n  \nend\n\n\n\n\n\n\n\n\n[ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.\n\n\n\n\nParametric Types\n\n\nA sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,\n\n\nshortestPaths\n{\nTv\n,\nTi\n}(\nmat\n::\nSparseMatrixCSC\n{\nTv\n,\nTi\n},\n \nstart\n::\nTi\n)\n\n\n\n\n\n\nTv\n, sometimes written \nTval\n denotes the types of the values, and \nTi\n or \nTind\n denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths, \nstart\n is of type \nTi\n.  Inside the code, whenever we write something like \npArray = zeros(Ti,n)\n, it creates an array of zeros of type Ti.  Using these parameteric types is \nmuch\n faster than leaving the types unfixed.\n\n\nData structures:\n\n\n\n\nIntHeap\n a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.\n\n\n\n\nInterface issue:\n\n\nThere are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.\n\n\nWriting tests:\n\n\nI haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.", 
            "title": "Overview"
        }, 
        {
            "location": "/yinsGraph/#yinsgraph", 
            "text": "yinsGraph is a package that I (Dan) am writing to explore and manipulate graphs in Julia.  The graphs are represented as sparse matrices.  The particular class in Julia is called a SparseMatrixCSC.  The reasons for this are:   They are fast, and  We want to do linear algebra with them, so matrices help.   You can probably learn more about the CSC (Compressed Sparse Column) format by googling it.    So far, speed tests of code that I've written for connected components, shorest paths, and minimum spanning trees have been as fast or faster than the previous routines I could call from Matlab.", 
            "title": "yinsGraph"
        }, 
        {
            "location": "/yinsGraph/#to-install-yinsgraph", 
            "text": "you will need a number of packages.\nYou install these like  Pkg.add( PyCall )\nPkg.add( PyPlot )\nPkg.add( DataStructures )  I also recommend the Optim package.  I think you need to install matplotlib in python before PyPlot. \nLook at this page for more information: https://github.com/stevengj/PyPlot.jl  I'm not sure if there are any others.  If you find that there are, please list them above.", 
            "title": "To install yinsGraph"
        }, 
        {
            "location": "/yinsGraph/#to-use-yinsgraph", 
            "text": "Examples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.    Right now, the notebooks worth looking at are:   yinsGraph  - usage, demo, and speed tests   Solvers  - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.    [ ] The implementation of CG in IterativeSolvers sort of sucks, as I now see in the tests.  It is allocating way to much memory.  It should be fixed either by devectorizing (see Julia Performance Tips), or by using BLAS routines.    (I suggest that you open the html in your browser)  Graph generators:    readIJ ( filename :: String ) \n  readIJV ( filename :: String ) \n  writeIJV ( filename :: String ,   mat ) \n  ringGraph ( n :: Int64 ) \n  generalizedRing ( n :: Int64 ,   gens ) \n  randMatching ( n :: Int64 ) \n  randRegular ( n :: Int64 ,   k :: Int64 ) \n  grownGraph ( n :: Int64 ,   k :: Int64 ) \n  grownGraphD ( n :: Int64 ,   k :: Int64 ) \n  prefAttach ( n :: Int64 ,   k :: Int64 ,   p :: Float64 ) \n  hyperCube ( d :: Int64 ) \n  completeBinaryTree ( n :: Int64 ) \n  grid2 ( n :: Int64 ) \n  grid2 ( n :: Int64 ,   m :: Int64 ;   isotropy = 1 ) \n  grid2coords ( n :: Int64 ,   m :: Int64 )    [ ] The types in the arguments of those should probably be more flexible/general.   For example, to generate a 4-by-5 grid, you type  graph   =   grid2 ( 4 , 5 )   Operations on Graphs:   shortIntGraph   for converting the index type of a graph to an Int32.    lap   to produce the laplacian of a graph  [ ] Maybe this should grab the upper triangular part, and symmetrize first.       unweight  - change all the weights to 1  mapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind},f)   to apply the function f to the weight of every edge.  uniformWeight   an example of mapweight.  It ignores the weight, and maps every weight to a random in [0,1]  productGraph(a0::SparseMatrixCSC, a1::SparseMatrixCSC)  the cartesian product.  Given two paths it makes a grid.  edgeVertexMat(mat::SparseMatrixCSC)   signed edge vertex matrix  subsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64) \n  produce a new graph that keeps each edge with probability p.  twoLift(a, k)  create a 2-lift of a with k flipped edges.  If k is unspecified, this generates a random 2-lift.   joinGraphs(a, b, k)  create a disjoint union of a and b, and add k random edges between them  plotGraph(gr,x,y,color=[0,0,1];dots=true,setaxis=true,number=false)  spectralDrawing(graph)   Fundamental Graph Algorithms:   components  computes connected components, returns as a vector  vecToComps  turns into an array with a list of vertices in each component  shortestPaths(mat, start)   returns an array of distances,\n    and pointers to the node closest (parent array)  kruskal(mat; kind=:min)   to get a max tree, use  kind = :max \n    returns it as a sparse matrix.   Solving Linear equations:  We have implemented Conjugate Gradient (cg) and the Preconditioned Conjugate Gradient (pcg).  These implementations use BLAS when they can, and a slower routine for data types like BigFloat.  To learn more, read  solvers.md .", 
            "title": "To use yinsGraph"
        }, 
        {
            "location": "/yinsGraph/#to-develop-yinsgraph", 
            "text": "Just go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"  I think that each file should contain a manifest up top listing the functions and types that it provides.  They should be divided up into those that are for internal use only, and those that should be exported.  Old code that didn't work well, but which you want to keep for reference should go at the end.  Using sparse matrices as graphs  The routines  deg ,  nbri  and  weighti  will let you treat a sparse matrix like a graph.  deg(graph, u)  is the degree of node u. nbri(graph, u, i)  is the ith neighbor of node u. weighti(graph, u, i)  is the weight of the edge to the ith neighbor of node u.  Note that we start indexing from 1.  For example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:     for   i   in   1 : deg ( mat ,   v ) \n      nbr   =   nbri ( mat ,   v ,   i ) \n      wt   =   weighti ( mat ,   v ,   i ) \n      foo ( v ,   nbr ,   wt ) \n   end   But, this turns out to be much slower than working with the structure directly, like     for   ind   in   mat . colptr [ v ]:( mat . colptr [ v + 1 ] - 1 ) \n       nbr   =   mat . rowval [ ind ] \n       wt   =   mat . nzval [ ind ] \n       foo ( v ,   nbr ,   wt ) \n   end    [ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.   Parametric Types  A sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,  shortestPaths { Tv , Ti }( mat :: SparseMatrixCSC { Tv , Ti },   start :: Ti )   Tv , sometimes written  Tval  denotes the types of the values, and  Ti  or  Tind  denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths,  start  is of type  Ti .  Inside the code, whenever we write something like  pArray = zeros(Ti,n) , it creates an array of zeros of type Ti.  Using these parameteric types is  much  faster than leaving the types unfixed.  Data structures:   IntHeap  a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.   Interface issue:  There are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.  Writing tests:  I haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.", 
            "title": "To develop yinsGraph"
        }, 
        {
            "location": "/solvers/", 
            "text": "Solving linear equations in Laplacians\n\n\nDirect Solvers\n\n\nIterative Solvers\n\n\nLow-Stretch Spanning Trees\n\n\nAugmented spanning tree preconditioners\n\n\n\n\n\n\n\n\n\n\nSolving linear equations in Laplacians\n\n\nRight now, our solver code is in \nsolvers.jl\n, but not included in yinsGraph.  So, you should include this directly.  Implementations of cg and pcg have been automatically included in yinsGraph.  They are in the file \npcg.jl\n\n\nFor some experiments with solvers, including some of those below, look at the notebook \nSolvers.ipynb\n.\n\n\nDirect Solvers\n\n\nYou can compute a cholesky factor directly with \ncholfact\n.  It does  more than just compute the factor, and it saves its result in a data structure that implements \n\\\n.  It uses SuiteSparse by Tim Davis.\n\n\nHere is an example of how you would use it to solve a general non-singular linear system.\n\n\na\n \n=\n \ngrid2\n(\n5\n)\n\n\nla\n \n=\n \nlap\n(\na\n)\n\n\nla\n[\n1\n,\n1\n]\n \n=\n \nla\n[\n1\n,\n1\n]\n \n+\n \n1\n\n\nF\n \n=\n \ncholfact\n(\nla\n)\n\n\n\nn\n \n=\n \nsize\n(\na\n)[\n1\n]\n\n\nb\n \n=\n \nrandn\n(\nn\n)\n\n\nx\n \n=\n \nF\n \\ \nb\n\n\nnorm\n(\nla\n*\nx\n-\nb\n)\n\n\n    \n1.0598778281116327e-14\n\n\n\n\n\n\nLaplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.\n\n\nla\n \n=\n \nlap\n(\na\n)\n\n\nf\n \n=\n \nlapWrapSolver\n(\ncholfact\n,\nla\n)\n\n\nb\n \n=\n \nrandn\n(\nn\n);\n \nb\n \n=\n \nb\n \n-\n \nmean\n(\nb\n);\n\n\nnorm\n(\nla\n*\nf\n(\nb\n)\n \n-\n \nb\n)\n\n    \n2.0971536951312585e-15\n\n\n\n\n\n\nHere are two other ways of using the wrapper:\n\n\nlapChol\n \n=\n \nlapWrapSolver\n(\ncholfact\n)\n\n\nf\n \n=\n \nlapChol\n(\nla\n)\n\n\nb\n \n=\n \nrandn\n(\nn\n);\n \n\nb\n \n=\n \nb\n \n-\n \nmean\n(\nb\n);\n\n\nnorm\n(\nla\n*\nf\n(\nb\n)\n \n-\n \nb\n)\n\n    \n2.6924696662484416e-15\n\n\n\nx\n \n=\n \nlapWrapSolver\n(\ncholfact\n,\nla\n,\nb\n)\n\n\nnorm\n(\nla\n*\nx\n \n-\n \nb\n)\n\n    \n2.6924696662484416e-15\n\n\n\n\n\n\nIterative Solvers\n\n\nThe first, of course, is the Conjugate Gradient (cg).\n\n\nOur implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance \ntol\n and the maximum number of iterations, \nmaxits\n.  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.\n\n\nn\n \n=\n \n50\n\n\na\n \n=\n \nrandn\n(\nn\n,\nn\n);\n \na\n \n=\n \na\n \n*\n \na\n;\n\n\nb\n \n=\n \nrandn\n(\nn\n)\n\n\nx\n \n=\n \ncg\n(\na\n,\nb\n,\nmaxits\n=\n100\n)\n\n\nnorm\n(\na\n*\nx\n \n-\n \nb\n)\n\n    \n1.2191649497921835e-6\n\n\n\nbbig\n \n=\n \nconvert\n(\nArray\n{\nBigFloat\n,\n1\n},\nb\n)\n\n\nxbig\n \n=\n \ncg\n(\na\n,\nbbig\n,\nmaxits\n=\n100\n)\n\n\nnorm\n(\na\n*\nxbig\n \n-\n \nbbig\n)\n\n    \n1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33\n\n\n\n\n\n\nAs a sanity check, we do two speed tests against Matlab.\n\n\nla\n \n=\n \nlap\n(\ngrid2\n(\n200\n))\n\n\nn\n \n=\n \nsize\n(\nla\n)[\n1\n]\n\n\nb\n \n=\n \nrandn\n(\nn\n)\n\n\nb\n \n=\n \nb\n \n-\n \nmean\n(\nb\n);\n\n\n@\ntime\n \nx\n \n=\n \ncg\n(\nla\n,\nb\n,\nmaxits\n=\n1000\n)\n\n    \n0.813791\n \nseconds\n \n(\n2.77\n \nk\n \nallocations\n:\n \n211.550\n \nMB\n,\n \n3.56\n%\n \ngc\n \ntime\n)\n\n\n\nnorm\n(\nla\n*\nx\n-\nb\n)\n\n    \n0.0001900620047823064\n\n\n\n\n\n\nAnd, in Matlab:\n\n\n \na\n \n=\n \ngrid2\n(\n200\n);\n\n\n \nla\n \n=\n \nlap\n(\na\n);\n\n\n \nb\n \n=\n \nrandn\n(\nlength\n(\na\n),\n1\n);\n \nb\n \n=\n \nb\n \n-\n \nmean\n(\nb\n);\n\n\n \ntic\n;\n \nx\n \n=\n \npcg\n(\nla\n,\nb\n,[],\n1000\n);\n \ntoc\n\n\npcg\n \nconverged\n \nat\n \niteration\n \n688\n \nto\n \na\n \nsolution\n \nwith\n \nrelative\n \nresidual\n \n9.8e-07\n.\n\n\nElapsed\n \ntime\n \nis\n \n1.244917\n \nseconds\n.\n\n\n \nnorm\n(\nla\n*\nx\n-\nb\n)\n\n\n\nans\n \n=\n\n\n   \n1.9730e-04\n\n\n\n\n\n\nPCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.\n\n\na\n \n=\n \nmapweight\n(\ngrid2\n(\n200\n),\nx\n-\n1\n/\n(\nrand\n(\n1\n)[\n1\n]));\n\n\nla\n \n=\n \nlap\n(\na\n)\n\n\nn\n \n=\n \nsize\n(\nla\n)[\n1\n]\n\n\nb\n \n=\n \nrandn\n(\nn\n)\n\n\nb\n \n=\n \nb\n \n-\n \nmean\n(\nb\n);\n\n\n\nd\n \n=\n \ndiag\n(\nla\n)\n\n\npre\n(\nx\n)\n \n=\n \nx\n \n./\n \nd\n\n\n@\ntime\n \nx\n \n=\n \npcg\n(\nla\n,\nb\n,\npre\n,\nmaxits\n=\n2000\n)\n\n    \n3.322035\n \nseconds\n \n(\n42.21\n \nk\n \nallocations\n:\n \n1.194\n \nGB\n,\n \n5.11\n%\n \ngc\n \ntime\n)\n\n\nnorm\n(\nla\n*\nx\n-\nb\n)\n\n    \n0.008508746034886803\n\n\n\n\n\n\nIf our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.\n\n\n@\ntime\n \nx\n \n=\n \npcg\n(\nla\n,\nb\n,\npre\n,\ntol\n=\n1e-1\n,\nmaxits\n=\n10\n^\n5\n)\n\n    \n0.747042\n \nseconds\n \n(\n9.65\n \nk\n \nallocations\n:\n \n275.819\n \nMB\n,\n \n4.87\n%\n \ngc\n \ntime\n)\n\n\nnorm\n(\nla\n*\nx\n-\nb\n)\n\n    \n19.840756251253442\n\n\n\n@\ntime\n \nx\n \n=\n \ncg\n(\nla\n,\nb\n,\ntol\n=\n1e-1\n,\nmaxits\n=\n10\n^\n5\n)\n\n    \n6.509665\n \nseconds\n \n(\n22.55\n \nk\n \nallocations\n:\n \n1.680\n \nGB\n,\n \n3.68\n%\n \ngc\n \ntime\n)\n\n\nnorm\n(\nla\n*\nx\n-\nb\n)\n\n    \n19.222483530605043\n\n\n\n\n\n\nLow-Stretch Spanning Trees\n\n\nIn order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm.\n\nrandishKruskall\n samples the remaining edges with probability proportional to their weight.  \nrandishPrim\n samples edges on the boundary while using the same rule.\n\n\nBoth use a data structure called \nSampler\n that allows you to store integers with real values, and to sample according to those real values.\n\n\nWe also have code for computing the stretches.\nHere are some examples.\n\n\na\n \n=\n \ngrid2\n(\n1000\n)\n\n\nt\n \n=\n \nrandishKruskal\n(\na\n);\n\n\nst\n \n=\n \ncompStretches\n(\nt\n,\na\n);\n\n\nsum\n(\nst\n)\n/\nnnz\n(\na\n)\n\n    \n43.410262262262265\n\n\n\nt\n \n=\n \nrandishPrim\n(\na\n);\n\n\nst\n \n=\n \ncompStretches\n(\nt\n,\na\n);\n\n\nsum\n(\nst\n)\n/\nnnz\n(\na\n)\n\n    \n33.14477077077077\n\n\n\n\n\n\nAugmented spanning tree preconditioners\n\n\nHere is code that will invoke one.\n\nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.\n\n\na\n \n=\n \nmapweight\n(\ngrid2\n(\n1000\n),\nx\n-\n1\n/\n(\nrand\n(\n1\n)[\n1\n]));\n\n\nla\n \n=\n \nlap\n(\na\n)\n\n\nn\n \n=\n \nsize\n(\nla\n)[\n1\n]\n\n\nla\n[\n1\n,\n1\n]\n \n=\n \nla\n[\n1\n,\n1\n]\n \n+\n \n1\n\n\n@\ntime\n \nF\n \n=\n \naugTreeSolver\n(\nla\n,\ntol\n=\n1e-1\n,\nmaxits\n=\n1000\n)\n\n    \n6.529052\n \nseconds\n \n(\n4.00\n \nM\n \nallocations\n:\n \n1.858\n \nGB\n,\n \n15.34\n%\n \ngc\n \ntime\n)\n\n\n\nb\n \n=\n \nrandn\n(\nn\n)\n\n\n@\ntime\n \nx\n \n=\n \nF\n(\nb\n)\n\n    \n29.058915\n \nseconds\n \n(\n9.74\n \nk\n \nallocations\n:\n \n23.209\n \nGB\n,\n \n6.84\n%\n \ngc\n \ntime\n)\n\n\n\nnorm\n(\nla\n*\nx\n \n-\n \nb\n)\n\n    \n99.74452367765869\n\n\n\n# Now, let\ns contrast with using CG\n\n\n\n@\ntime\n \ny\n \n=\n \ncg\n(\nla\n,\nb\n,\ntol\n=\n1e-1\n,\nmaxits\n=\n1000\n)\n\n    \n28.719631\n \nseconds\n \n(\n4.01\n \nk\n \nallocations\n:\n \n7.473\n \nGB\n,\n \n3.74\n%\n \ngc\n \ntime\n)\n\n\n\nnorm\n(\nla\n*\ny\n-\nb\n)\n\n    \n3243.6014713600766\n\n\n\n\n\n\nThat was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.\n\n\n@\ntime\n \nF\n \n=\n \naugTreeSolver\n(\nla\n,\ntol\n=\n1e-1\n,\nmaxits\n=\n1000\n,\ntreeAlg\n=\nrandishPrim\n);\n\n    \n6.319489\n \nseconds\n \n(\n4.00\n \nM\n \nallocations\n:\n \n2.030\n \nGB\n,\n \n18.81\n%\n \ngc\n \ntime\n)\n\n\n\nb\n \n=\n \nrandn\n(\nn\n)\n\n\n@\ntime\n \nx\n \n=\n \nF\n(\nb\n)\n\n    \n29.503484\n \nseconds\n \n(\n9.76\n \nk\n \nallocations\n:\n \n23.268\n \nGB\n,\n \n7.31\n%\n \ngc\n \ntime\n)\n\n\n\nnorm\n(\nla\n*\nx\n \n-\n \nb\n)\n  \n    \n99.29610874176991\n\n\n\n\n\n\nTo solve systems in a Laplacian, we could wrap it.\n\n\nn\n \n=\n \n40000\n\n\nla\n \n=\n \nlap\n(\nrandRegular\n(\nn\n,\n3\n))\n\n\nf\n \n=\n \nlapWrapSolver\n(\naugTreeSolver\n,\nla\n,\ntol\n=\n1e-6\n,\nmaxits\n=\n1000\n)\n\n\nb\n \n=\n \nrandn\n(\nn\n);\n \nb\n \n=\n \nb\n \n-\n \nmean\n(\nb\n)\n\n\nx\n \n=\n \nf\n(\nb\n)\n\n\nnorm\n(\nla\n*\nx\n-\nb\n)\n\n    \n0.00019304778073388\n\n\n\n\n\n\nAs you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Solvers"
        }, 
        {
            "location": "/solvers/#solving-linear-equations-in-laplacians", 
            "text": "Right now, our solver code is in  solvers.jl , but not included in yinsGraph.  So, you should include this directly.  Implementations of cg and pcg have been automatically included in yinsGraph.  They are in the file  pcg.jl  For some experiments with solvers, including some of those below, look at the notebook  Solvers.ipynb .", 
            "title": "Solving linear equations in Laplacians"
        }, 
        {
            "location": "/solvers/#direct-solvers", 
            "text": "You can compute a cholesky factor directly with  cholfact .  It does  more than just compute the factor, and it saves its result in a data structure that implements  \\ .  It uses SuiteSparse by Tim Davis.  Here is an example of how you would use it to solve a general non-singular linear system.  a   =   grid2 ( 5 )  la   =   lap ( a )  la [ 1 , 1 ]   =   la [ 1 , 1 ]   +   1  F   =   cholfact ( la )  n   =   size ( a )[ 1 ]  b   =   randn ( n )  x   =   F  \\  b  norm ( la * x - b ) \n\n     1.0598778281116327e-14   Laplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.  la   =   lap ( a )  f   =   lapWrapSolver ( cholfact , la )  b   =   randn ( n );   b   =   b   -   mean ( b );  norm ( la * f ( b )   -   b ) \n     2.0971536951312585e-15   Here are two other ways of using the wrapper:  lapChol   =   lapWrapSolver ( cholfact )  f   =   lapChol ( la )  b   =   randn ( n );   b   =   b   -   mean ( b );  norm ( la * f ( b )   -   b ) \n     2.6924696662484416e-15  x   =   lapWrapSolver ( cholfact , la , b )  norm ( la * x   -   b ) \n     2.6924696662484416e-15", 
            "title": "Direct Solvers"
        }, 
        {
            "location": "/solvers/#iterative-solvers", 
            "text": "The first, of course, is the Conjugate Gradient (cg).  Our implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance  tol  and the maximum number of iterations,  maxits .  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.  n   =   50  a   =   randn ( n , n );   a   =   a   *   a ;  b   =   randn ( n )  x   =   cg ( a , b , maxits = 100 )  norm ( a * x   -   b ) \n     1.2191649497921835e-6  bbig   =   convert ( Array { BigFloat , 1 }, b )  xbig   =   cg ( a , bbig , maxits = 100 )  norm ( a * xbig   -   bbig ) \n     1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33   As a sanity check, we do two speed tests against Matlab.  la   =   lap ( grid2 ( 200 ))  n   =   size ( la )[ 1 ]  b   =   randn ( n )  b   =   b   -   mean ( b );  @ time   x   =   cg ( la , b , maxits = 1000 ) \n     0.813791   seconds   ( 2.77   k   allocations :   211.550   MB ,   3.56 %   gc   time )  norm ( la * x - b ) \n     0.0001900620047823064   And, in Matlab:    a   =   grid2 ( 200 );    la   =   lap ( a );    b   =   randn ( length ( a ), 1 );   b   =   b   -   mean ( b );    tic ;   x   =   pcg ( la , b ,[], 1000 );   toc  pcg   converged   at   iteration   688   to   a   solution   with   relative   residual   9.8e-07 .  Elapsed   time   is   1.244917   seconds .    norm ( la * x - b )  ans   = \n\n    1.9730e-04   PCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.  a   =   mapweight ( grid2 ( 200 ), x - 1 / ( rand ( 1 )[ 1 ]));  la   =   lap ( a )  n   =   size ( la )[ 1 ]  b   =   randn ( n )  b   =   b   -   mean ( b );  d   =   diag ( la )  pre ( x )   =   x   ./   d  @ time   x   =   pcg ( la , b , pre , maxits = 2000 ) \n     3.322035   seconds   ( 42.21   k   allocations :   1.194   GB ,   5.11 %   gc   time )  norm ( la * x - b ) \n     0.008508746034886803   If our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.  @ time   x   =   pcg ( la , b , pre , tol = 1e-1 , maxits = 10 ^ 5 ) \n     0.747042   seconds   ( 9.65   k   allocations :   275.819   MB ,   4.87 %   gc   time )  norm ( la * x - b ) \n     19.840756251253442  @ time   x   =   cg ( la , b , tol = 1e-1 , maxits = 10 ^ 5 ) \n     6.509665   seconds   ( 22.55   k   allocations :   1.680   GB ,   3.68 %   gc   time )  norm ( la * x - b ) \n     19.222483530605043", 
            "title": "Iterative Solvers"
        }, 
        {
            "location": "/solvers/#low-stretch-spanning-trees", 
            "text": "In order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm. randishKruskall  samples the remaining edges with probability proportional to their weight.   randishPrim  samples edges on the boundary while using the same rule.  Both use a data structure called  Sampler  that allows you to store integers with real values, and to sample according to those real values.  We also have code for computing the stretches.\nHere are some examples.  a   =   grid2 ( 1000 )  t   =   randishKruskal ( a );  st   =   compStretches ( t , a );  sum ( st ) / nnz ( a ) \n     43.410262262262265  t   =   randishPrim ( a );  st   =   compStretches ( t , a );  sum ( st ) / nnz ( a ) \n     33.14477077077077", 
            "title": "Low-Stretch Spanning Trees"
        }, 
        {
            "location": "/solvers/#augmented-spanning-tree-preconditioners", 
            "text": "Here is code that will invoke one. \nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.  a   =   mapweight ( grid2 ( 1000 ), x - 1 / ( rand ( 1 )[ 1 ]));  la   =   lap ( a )  n   =   size ( la )[ 1 ]  la [ 1 , 1 ]   =   la [ 1 , 1 ]   +   1  @ time   F   =   augTreeSolver ( la , tol = 1e-1 , maxits = 1000 ) \n     6.529052   seconds   ( 4.00   M   allocations :   1.858   GB ,   15.34 %   gc   time )  b   =   randn ( n )  @ time   x   =   F ( b ) \n     29.058915   seconds   ( 9.74   k   allocations :   23.209   GB ,   6.84 %   gc   time )  norm ( la * x   -   b ) \n     99.74452367765869  # Now, let s contrast with using CG  @ time   y   =   cg ( la , b , tol = 1e-1 , maxits = 1000 ) \n     28.719631   seconds   ( 4.01   k   allocations :   7.473   GB ,   3.74 %   gc   time )  norm ( la * y - b ) \n     3243.6014713600766   That was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.  @ time   F   =   augTreeSolver ( la , tol = 1e-1 , maxits = 1000 , treeAlg = randishPrim ); \n     6.319489   seconds   ( 4.00   M   allocations :   2.030   GB ,   18.81 %   gc   time )  b   =   randn ( n )  @ time   x   =   F ( b ) \n     29.503484   seconds   ( 9.76   k   allocations :   23.268   GB ,   7.31 %   gc   time )  norm ( la * x   -   b )   \n     99.29610874176991   To solve systems in a Laplacian, we could wrap it.  n   =   40000  la   =   lap ( randRegular ( n , 3 ))  f   =   lapWrapSolver ( augTreeSolver , la , tol = 1e-6 , maxits = 1000 )  b   =   randn ( n );   b   =   b   -   mean ( b )  x   =   f ( b )  norm ( la * x - b ) \n     0.00019304778073388   As you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Augmented spanning tree preconditioners"
        }, 
        {
            "location": "/api/yinsGraph/", 
            "text": "yinsGraph\n\n\nExported\n\n\n\n\n\n\napr{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  s::Array{Float64, 1},  alpha::Float64,  eps::Float64) \n\u00b6\n\n\ncomputes an approximate page rank vector from a starting vector s, an alpha and an epsilon \n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:69\n\n\n\n\n\n\nbackIndices{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti}) \n\u00b6\n\n\ncomputes the back indices in a graph in O(M+N) \n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/graphUtils.jl:21\n\n\n\n\n\n\ncompleteGraph(n::Int64) \n\u00b6\n\n\nThe complete graph \n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:351\n\n\n\n\n\n\ndiagmat{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti}) \n\u00b6\n\n\nreturns the diagonal matrix(as a sparse matrix) of a graph \n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/graphOps.jl:198\n\n\n\n\n\n\ngeneralizedNecklace{Tv, Ti}(A::SparseMatrixCSC{Tv, Ti},  H::SparseMatrixCSC{Tv, Ti\n:Integer},  k::Int64) \n\u00b6\n\n\ngeneralizedNecklace{Tv, Ti}(A::SparseMatrixCSC{Tv, Ti}, H::SparseMatrixCSC, k::Int64)\n\n\n\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The\nresulting new graph will be constructed by expanding each vertex in H to an\ninstance of A. k random edges will be generated between components. Thus, the\nresulting graph may have weighted edges.\n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:310\n\n\n\n\n\n\njoinGraphs{Tval, Tind}(a::SparseMatrixCSC{Tval, Tind},  b::SparseMatrixCSC{Tval, Tind},  k::Integer) \n\u00b6\n\n\ncreate a disjoint union of graphs a and b,\n and then put k random edges between them\n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/graphOps.jl:106\n\n\n\n\n\n\nmaxflow{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  s::Int64,  t::Int64) \n\u00b6\n\n\nimplementation of Dinic's algorithm. computes the maximum flow and min-cut in G between s and t. we consider the adjacency matrix to be the capacity matrix \n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/flow.jl:2\n\n\n\n\n\n\npathGraph(n::Int64) \n\u00b6\n\n\nThe path graph on n vertices \n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:356\n\n\n\n\n\n\nppr{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  s::Array{Float64, 1},  alpha::Float64) \n\u00b6\n\n\ncomputes the personal page rank vector from a starting vector s and an alpha; operates with lazy walk matrix \n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:35\n\n\n\n\n\n\nprefAttach(n::Int64,  k::Int64,  p::Float64) \n\u00b6\n\n\nA preferential attachment graph in which each vertex has k edges to those\nthat come before.  These are chosen with probability p to be from a random vertex,\nand with probability 1-p to come from the endpoint of a random edge.\nIt begins with a k-clique on the first k+1 vertices.\n\n\nprefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:190\n\n\n\n\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  v::Array{Int64, 1},  phi::Float64,  b::Int64) \n\u00b6\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)\n\n\nthe PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper\n\n\nv is a set of vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]\n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:113\n\n\n\n\n\n\npr{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  alpha::Float64) \n\u00b6\n\n\ncomputes a page rank vector satisfying p = a/n * 1 + (1 - a) * W * p\n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:18\n\n\n\n\n\n\nringGraph(n::Int64) \n\u00b6\n\n\nThe simple ring on n vertices\n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:73\n\n\n\n\n\n\ntoUnitVector(a::Array{Int64, 1},  n) \n\u00b6\n\n\ncreates a unit vector of length n from a given set of integers, with weights based on the number of occurences \n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/graphOps.jl:176\n\n\nInternal\n\n\n\n\n\n\npushpr{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  u::Int64,  alpha::Float64,  p::Array{Float64, 1},  r::Array{Float64, 1}) \n\u00b6\n\n\nimplements the push operation for the aproximate page rank vector \n\n\nsource:\n\n\n/Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:2", 
            "title": "API"
        }, 
        {
            "location": "/api/yinsGraph/#yinsgraph", 
            "text": "", 
            "title": "yinsGraph"
        }, 
        {
            "location": "/api/yinsGraph/#exported", 
            "text": "apr{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  s::Array{Float64, 1},  alpha::Float64,  eps::Float64)  \u00b6  computes an approximate page rank vector from a starting vector s, an alpha and an epsilon   source:  /Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:69    backIndices{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti})  \u00b6  computes the back indices in a graph in O(M+N)   source:  /Users/spielman/git/GraphAnalysis.jl/src/graphUtils.jl:21    completeGraph(n::Int64)  \u00b6  The complete graph   source:  /Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:351    diagmat{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti})  \u00b6  returns the diagonal matrix(as a sparse matrix) of a graph   source:  /Users/spielman/git/GraphAnalysis.jl/src/graphOps.jl:198    generalizedNecklace{Tv, Ti}(A::SparseMatrixCSC{Tv, Ti},  H::SparseMatrixCSC{Tv, Ti :Integer},  k::Int64)  \u00b6  generalizedNecklace{Tv, Ti}(A::SparseMatrixCSC{Tv, Ti}, H::SparseMatrixCSC, k::Int64)  Constructs a generalized necklace graph starting with two graphs A and H. The\nresulting new graph will be constructed by expanding each vertex in H to an\ninstance of A. k random edges will be generated between components. Thus, the\nresulting graph may have weighted edges.  source:  /Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:310    joinGraphs{Tval, Tind}(a::SparseMatrixCSC{Tval, Tind},  b::SparseMatrixCSC{Tval, Tind},  k::Integer)  \u00b6  create a disjoint union of graphs a and b,\n and then put k random edges between them  source:  /Users/spielman/git/GraphAnalysis.jl/src/graphOps.jl:106    maxflow{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  s::Int64,  t::Int64)  \u00b6  implementation of Dinic's algorithm. computes the maximum flow and min-cut in G between s and t. we consider the adjacency matrix to be the capacity matrix   source:  /Users/spielman/git/GraphAnalysis.jl/src/flow.jl:2    pathGraph(n::Int64)  \u00b6  The path graph on n vertices   source:  /Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:356    ppr{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  s::Array{Float64, 1},  alpha::Float64)  \u00b6  computes the personal page rank vector from a starting vector s and an alpha; operates with lazy walk matrix   source:  /Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:35    prefAttach(n::Int64,  k::Int64,  p::Float64)  \u00b6  A preferential attachment graph in which each vertex has k edges to those\nthat come before.  These are chosen with probability p to be from a random vertex,\nand with probability 1-p to come from the endpoint of a random edge.\nIt begins with a k-clique on the first k+1 vertices.  prefAttach(n::Int64, k::Int64, p::Float64)  source:  /Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:190    prn{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  v::Array{Int64, 1},  phi::Float64,  b::Int64)  \u00b6  prn{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)  the PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper  v is a set of vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]  source:  /Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:113    pr{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  alpha::Float64)  \u00b6  computes a page rank vector satisfying p = a/n * 1 + (1 - a) * W * p  source:  /Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:18    ringGraph(n::Int64)  \u00b6  The simple ring on n vertices  source:  /Users/spielman/git/GraphAnalysis.jl/src/graphGenerators.jl:73    toUnitVector(a::Array{Int64, 1},  n)  \u00b6  creates a unit vector of length n from a given set of integers, with weights based on the number of occurences   source:  /Users/spielman/git/GraphAnalysis.jl/src/graphOps.jl:176", 
            "title": "Exported"
        }, 
        {
            "location": "/api/yinsGraph/#internal", 
            "text": "pushpr{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti},  u::Int64,  alpha::Float64,  p::Array{Float64, 1},  r::Array{Float64, 1})  \u00b6  implements the push operation for the aproximate page rank vector   source:  /Users/spielman/git/GraphAnalysis.jl/src/cutPageRank.jl:2", 
            "title": "Internal"
        }
    ]
}