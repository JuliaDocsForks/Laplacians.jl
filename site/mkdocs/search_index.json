{
    "docs": [
        {
            "location": "/about/index.html", 
            "text": "A package for graph computations related to graph Laplacians\n\n\nGraphs are represented by sparse adjacency matrices, etc.", 
            "title": "About"
        }, 
        {
            "location": "/yinsGraph/index.html", 
            "text": "yinsGraph\n\n\nTo install yinsGraph\n\n\nTo use yinsGraph\n\n\nGraph generators:\n\n\nOperations on Graphs:\n\n\nFundamental Graph Algorithms:\n\n\nSolving Linear equations:\n\n\n\n\n\n\nTo develop yinsGraph\n\n\nUsing sparse matrices as graphs\n\n\nParametric Types\n\n\nData structures:\n\n\nInterface issue:\n\n\nWriting tests:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyinsGraph\n\n\nyinsGraph is a package that I (Dan) am writing to explore and manipulate graphs in Julia.  The graphs are represented as sparse matrices.  The particular class in Julia is called a SparseMatrixCSC.  The reasons for this are:\n\n\n\n\nThey are fast, and\n\n\nWe want to do linear algebra with them, so matrices help.\n\n\n\n\nYou can probably learn more about the CSC (Compressed Sparse Column) format by googling it.  \n\n\nSo far, speed tests of code that I've written for connected components, shorest paths, and minimum spanning trees have been as fast or faster than the previous routines I could call from Matlab.  \n\n\nTo install yinsGraph\n\n\nyou will need a number of packages.\nYou install these like\n\n\nPkg.add(\nPyCall\n)\nPkg.add(\nPyPlot\n)\nPkg.add(\nDataStructures\n)\n\n\n\n\nI also recommend the Optim package.\n\n\nI think you need to install matplotlib in python before PyPlot.\n\nLook at this page for more information: https://github.com/stevengj/PyPlot.jl\n\n\nI'm not sure if there are any others.  If you find that there are, please list them above.\n\n\nTo use yinsGraph\n\n\nExamples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.  \n\n\nRight now, the notebooks worth looking at are:\n\n\n\n\nyinsGraph\n - usage, demo, and speed tests\n\n\n\n\nSolvers\n - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.\n\n\n\n\n\n\n[ ] The implementation of CG in IterativeSolvers sort of sucks, as I now see in the tests.  It is allocating way to much memory.  It should be fixed either by devectorizing (see Julia Performance Tips), or by using BLAS routines.\n\n\n\n\n\n\n(I suggest that you open the html in your browser)\n\n\nGraph generators:\n\n\n readIJ(filename::String)\n readIJV(filename::String)\n writeIJV(filename::String, mat)\n ringGraph(n::Int64)\n generalizedRing(n::Int64, gens)\n randMatching(n::Int64)\n randRegular(n::Int64, k::Int64)\n grownGraph(n::Int64, k::Int64)\n grownGraphD(n::Int64, k::Int64)\n prefAttach(n::Int64, k::Int64, p::Float64)\n hyperCube(d::Int64)\n completeBinaryTree(n::Int64)\n grid2(n::Int64)\n grid2(n::Int64, m::Int64; isotropy=1)\n grid2coords(n::Int64, m::Int64)\n\n\n\n\n\n\n[ ] The types in the arguments of those should probably be more flexible/general.\n\n\n\n\nFor example, to generate a 4-by-5 grid, you type\n\n\ngraph = grid2(4,5)\n\n\n\n\nOperations on Graphs:\n\n\n\n\nshortIntGraph\n  for converting the index type of a graph to an Int32.  \n\n\nlap\n  to produce the laplacian of a graph\n\n\n[ ] Maybe this should grab the upper triangular part, and symmetrize first.     \n\n\nunweight\n - change all the weights to 1\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind},f)\n  to apply the function f to the weight of every edge.\n\n\nuniformWeight\n  an example of mapweight.  It ignores the weight, and maps every weight to a random in [0,1]\n\n\nproductGraph(a0::SparseMatrixCSC, a1::SparseMatrixCSC)\n the cartesian product.  Given two paths it makes a grid.\n\n\nedgeVertexMat(mat::SparseMatrixCSC)\n  signed edge vertex matrix\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n  produce a new graph that keeps each edge with probability p.\n\n\ntwoLift(a, k)\n create a 2-lift of a with k flipped edges.  If k is unspecified, this generates a random 2-lift. \n\n\njoinGraphs(a, b, k)\n create a disjoint union of a and b, and add k random edges between them\n\n\nplotGraph(gr,x,y,color=[0,0,1];dots=true,setaxis=true,number=false)\n\n\nspectralDrawing(graph)\n\n\n\n\nFundamental Graph Algorithms:\n\n\n\n\ncomponents\n computes connected components, returns as a vector\n\n\nvecToComps\n turns into an array with a list of vertices in each component\n\n\nshortestPaths(mat, start)\n  returns an array of distances,\n    and pointers to the node closest (parent array)\n\n\nkruskal(mat; kind=:min)\n  to get a max tree, use \nkind = :max\n\n    returns it as a sparse matrix.\n\n\n\n\nSolving Linear equations:\n\n\nWe have implemented Conjugate Gradient (cg) and the Preconditioned Conjugate Gradient (pcg).  These implementations use BLAS when they can, and a slower routine for data types like BigFloat.\n\n\nTo learn more, read \nsolvers.md\n.\n\n\nTo develop yinsGraph\n\n\nJust go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"\n\n\nI think that each file should contain a manifest up top listing the functions and types that it provides.  They should be divided up into those that are for internal use only, and those that should be exported.  Old code that didn't work well, but which you want to keep for reference should go at the end.\n\n\nUsing sparse matrices as graphs\n\n\nThe routines \ndeg\n, \nnbri\n and \nweighti\n will let you treat a sparse matrix like a graph.\n\n\ndeg(graph, u)\n is the degree of node u.\n\nnbri(graph, u, i)\n is the ith neighbor of node u.\n\nweighti(graph, u, i)\n is the weight of the edge to the ith neighbor of node u.\n\n\nNote that we start indexing from 1.\n\n\nFor example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:\n\n\n  for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end\n\n\n\n\nBut, this turns out to be much slower than working with the structure directly, like\n\n\n  for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end\n\n\n\n\n\n\n[ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.\n\n\n\n\nParametric Types\n\n\nA sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\n\n\n\n\nTv\n, sometimes written \nTval\n denotes the types of the values, and \nTi\n or \nTind\n denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths, \nstart\n is of type \nTi\n.  Inside the code, whenever we write something like \npArray = zeros(Ti,n)\n, it creates an array of zeros of type Ti.  Using these parameteric types is \nmuch\n faster than leaving the types unfixed.\n\n\nData structures:\n\n\n\n\nIntHeap\n a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.\n\n\n\n\nInterface issue:\n\n\nThere are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.\n\n\nWriting tests:\n\n\nI haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.", 
            "title": "Overview"
        }, 
        {
            "location": "/yinsGraph/index.html#yinsgraph", 
            "text": "yinsGraph is a package that I (Dan) am writing to explore and manipulate graphs in Julia.  The graphs are represented as sparse matrices.  The particular class in Julia is called a SparseMatrixCSC.  The reasons for this are:   They are fast, and  We want to do linear algebra with them, so matrices help.   You can probably learn more about the CSC (Compressed Sparse Column) format by googling it.    So far, speed tests of code that I've written for connected components, shorest paths, and minimum spanning trees have been as fast or faster than the previous routines I could call from Matlab.", 
            "title": "yinsGraph"
        }, 
        {
            "location": "/yinsGraph/index.html#to-install-yinsgraph", 
            "text": "you will need a number of packages.\nYou install these like  Pkg.add( PyCall )\nPkg.add( PyPlot )\nPkg.add( DataStructures )  I also recommend the Optim package.  I think you need to install matplotlib in python before PyPlot. \nLook at this page for more information: https://github.com/stevengj/PyPlot.jl  I'm not sure if there are any others.  If you find that there are, please list them above.", 
            "title": "To install yinsGraph"
        }, 
        {
            "location": "/yinsGraph/index.html#to-use-yinsgraph", 
            "text": "Examples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.    Right now, the notebooks worth looking at are:   yinsGraph  - usage, demo, and speed tests   Solvers  - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.    [ ] The implementation of CG in IterativeSolvers sort of sucks, as I now see in the tests.  It is allocating way to much memory.  It should be fixed either by devectorizing (see Julia Performance Tips), or by using BLAS routines.    (I suggest that you open the html in your browser)  Graph generators:   readIJ(filename::String)\n readIJV(filename::String)\n writeIJV(filename::String, mat)\n ringGraph(n::Int64)\n generalizedRing(n::Int64, gens)\n randMatching(n::Int64)\n randRegular(n::Int64, k::Int64)\n grownGraph(n::Int64, k::Int64)\n grownGraphD(n::Int64, k::Int64)\n prefAttach(n::Int64, k::Int64, p::Float64)\n hyperCube(d::Int64)\n completeBinaryTree(n::Int64)\n grid2(n::Int64)\n grid2(n::Int64, m::Int64; isotropy=1)\n grid2coords(n::Int64, m::Int64)   [ ] The types in the arguments of those should probably be more flexible/general.   For example, to generate a 4-by-5 grid, you type  graph = grid2(4,5)  Operations on Graphs:   shortIntGraph   for converting the index type of a graph to an Int32.    lap   to produce the laplacian of a graph  [ ] Maybe this should grab the upper triangular part, and symmetrize first.       unweight  - change all the weights to 1  mapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind},f)   to apply the function f to the weight of every edge.  uniformWeight   an example of mapweight.  It ignores the weight, and maps every weight to a random in [0,1]  productGraph(a0::SparseMatrixCSC, a1::SparseMatrixCSC)  the cartesian product.  Given two paths it makes a grid.  edgeVertexMat(mat::SparseMatrixCSC)   signed edge vertex matrix  subsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64) \n  produce a new graph that keeps each edge with probability p.  twoLift(a, k)  create a 2-lift of a with k flipped edges.  If k is unspecified, this generates a random 2-lift.   joinGraphs(a, b, k)  create a disjoint union of a and b, and add k random edges between them  plotGraph(gr,x,y,color=[0,0,1];dots=true,setaxis=true,number=false)  spectralDrawing(graph)   Fundamental Graph Algorithms:   components  computes connected components, returns as a vector  vecToComps  turns into an array with a list of vertices in each component  shortestPaths(mat, start)   returns an array of distances,\n    and pointers to the node closest (parent array)  kruskal(mat; kind=:min)   to get a max tree, use  kind = :max \n    returns it as a sparse matrix.   Solving Linear equations:  We have implemented Conjugate Gradient (cg) and the Preconditioned Conjugate Gradient (pcg).  These implementations use BLAS when they can, and a slower routine for data types like BigFloat.  To learn more, read  solvers.md .", 
            "title": "To use yinsGraph"
        }, 
        {
            "location": "/yinsGraph/index.html#to-develop-yinsgraph", 
            "text": "Just go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"  I think that each file should contain a manifest up top listing the functions and types that it provides.  They should be divided up into those that are for internal use only, and those that should be exported.  Old code that didn't work well, but which you want to keep for reference should go at the end.  Using sparse matrices as graphs  The routines  deg ,  nbri  and  weighti  will let you treat a sparse matrix like a graph.  deg(graph, u)  is the degree of node u. nbri(graph, u, i)  is the ith neighbor of node u. weighti(graph, u, i)  is the weight of the edge to the ith neighbor of node u.  Note that we start indexing from 1.  For example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:    for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end  But, this turns out to be much slower than working with the structure directly, like    for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end   [ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.   Parametric Types  A sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,  shortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)  Tv , sometimes written  Tval  denotes the types of the values, and  Ti  or  Tind  denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths,  start  is of type  Ti .  Inside the code, whenever we write something like  pArray = zeros(Ti,n) , it creates an array of zeros of type Ti.  Using these parameteric types is  much  faster than leaving the types unfixed.  Data structures:   IntHeap  a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.   Interface issue:  There are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.  Writing tests:  I haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.", 
            "title": "To develop yinsGraph"
        }, 
        {
            "location": "/solvers/index.html", 
            "text": "Solving linear equations in Laplacians\n\n\nDirect Solvers\n\n\nIterative Solvers\n\n\nLow-Stretch Spanning Trees\n\n\nAugmented spanning tree preconditioners\n\n\n\n\n\n\n\n\n\n\nSolving linear equations in Laplacians\n\n\nRight now, our solver code is in \nsolvers.jl\n, but not included in yinsGraph.  So, you should include this directly.  Implementations of cg and pcg have been automatically included in yinsGraph.  They are in the file \npcg.jl\n\n\nFor some experiments with solvers, including some of those below, look at the notebook \nSolvers.ipynb\n.\n\n\nDirect Solvers\n\n\nYou can compute a cholesky factor directly with \ncholfact\n.  It does  more than just compute the factor, and it saves its result in a data structure that implements \n\\\n.  It uses SuiteSparse by Tim Davis.\n\n\nHere is an example of how you would use it to solve a general non-singular linear system.\n\n\na = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14\n\n\n\n\nLaplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.\n\n\nla = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15\n\n\n\n\nHere are two other ways of using the wrapper:\n\n\nlapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n); \nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15\n\n\n\n\nIterative Solvers\n\n\nThe first, of course, is the Conjugate Gradient (cg).\n\n\nOur implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance \ntol\n and the maximum number of iterations, \nmaxits\n.  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.\n\n\nn = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100)\nnorm(a*x - b)\n    1.2191649497921835e-6\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\nnorm(a*xbig - bbig)\n    1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33\n\n\n\n\nAs a sanity check, we do two speed tests against Matlab.\n\n\nla = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064\n\n\n\n\nAnd, in Matlab:\n\n\n a = grid2(200);\n\n la = lap(a);\n\n b = randn(length(a),1); b = b - mean(b);\n\n tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.\n\n norm(la*x-b)\n\nans =\n\n   1.9730e-04\n\n\n\n\nPCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.\n\n\na = mapweight(grid2(200),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000)\n    3.322035 seconds (42.21 k allocations: 1.194 GB, 5.11% gc time)\nnorm(la*x-b)\n    0.008508746034886803\n\n\n\n\nIf our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.\n\n\n@time x = pcg(la,b,pre,tol=1e-1,maxits=10^5)\n    0.747042 seconds (9.65 k allocations: 275.819 MB, 4.87% gc time)\nnorm(la*x-b)\n    19.840756251253442\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5)\n    6.509665 seconds (22.55 k allocations: 1.680 GB, 3.68% gc time)\nnorm(la*x-b)\n    19.222483530605043\n\n\n\n\nLow-Stretch Spanning Trees\n\n\nIn order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm.\n\nrandishKruskall\n samples the remaining edges with probability proportional to their weight.  \nrandishPrim\n samples edges on the boundary while using the same rule.\n\n\nBoth use a data structure called \nSampler\n that allows you to store integers with real values, and to sample according to those real values.\n\n\nWe also have code for computing the stretches.\nHere are some examples.\n\n\na = grid2(1000)\nt = randishKruskal(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    43.410262262262265\n\nt = randishPrim(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    33.14477077077077\n\n\n\n\n\nAugmented spanning tree preconditioners\n\n\nHere is code that will invoke one.\n\nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.\n\n\na = mapweight(grid2(1000),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nla[1,1] = la[1,1] + 1\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000)\n    6.529052 seconds (4.00 M allocations: 1.858 GB, 15.34% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.058915 seconds (9.74 k allocations: 23.209 GB, 6.84% gc time)\n\nnorm(la*x - b)\n    99.74452367765869\n\n# Now, let's contrast with using CG\n\n@time y = cg(la,b,tol=1e-1,maxits=1000)\n    28.719631 seconds (4.01 k allocations: 7.473 GB, 3.74% gc time)\n\nnorm(la*y-b)\n    3243.6014713600766\n\n\n\n\n\nThat was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.\n\n\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000,treeAlg=randishPrim);\n    6.319489 seconds (4.00 M allocations: 2.030 GB, 18.81% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.503484 seconds (9.76 k allocations: 23.268 GB, 7.31% gc time)\n\nnorm(la*x - b)  \n    99.29610874176991\n\n\n\n\nTo solve systems in a Laplacian, we could wrap it.\n\n\nn = 40000\nla = lap(randRegular(n,3))\nf = lapWrapSolver(augTreeSolver,la,tol=1e-6,maxits=1000)\nb = randn(n); b = b - mean(b)\nx = f(b)\nnorm(la*x-b)\n    0.00019304778073388\n\n\n\n\nAs you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Solvers"
        }, 
        {
            "location": "/solvers/index.html#solving-linear-equations-in-laplacians", 
            "text": "Right now, our solver code is in  solvers.jl , but not included in yinsGraph.  So, you should include this directly.  Implementations of cg and pcg have been automatically included in yinsGraph.  They are in the file  pcg.jl  For some experiments with solvers, including some of those below, look at the notebook  Solvers.ipynb .", 
            "title": "Solving linear equations in Laplacians"
        }, 
        {
            "location": "/solvers/index.html#direct-solvers", 
            "text": "You can compute a cholesky factor directly with  cholfact .  It does  more than just compute the factor, and it saves its result in a data structure that implements  \\ .  It uses SuiteSparse by Tim Davis.  Here is an example of how you would use it to solve a general non-singular linear system.  a = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14  Laplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.  la = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15  Here are two other ways of using the wrapper:  lapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n); \nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15", 
            "title": "Direct Solvers"
        }, 
        {
            "location": "/solvers/index.html#iterative-solvers", 
            "text": "The first, of course, is the Conjugate Gradient (cg).  Our implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance  tol  and the maximum number of iterations,  maxits .  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.  n = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100)\nnorm(a*x - b)\n    1.2191649497921835e-6\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\nnorm(a*xbig - bbig)\n    1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33  As a sanity check, we do two speed tests against Matlab.  la = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064  And, in Matlab:   a = grid2(200);  la = lap(a);  b = randn(length(a),1); b = b - mean(b);  tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.  norm(la*x-b)\n\nans =\n\n   1.9730e-04  PCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.  a = mapweight(grid2(200),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000)\n    3.322035 seconds (42.21 k allocations: 1.194 GB, 5.11% gc time)\nnorm(la*x-b)\n    0.008508746034886803  If our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.  @time x = pcg(la,b,pre,tol=1e-1,maxits=10^5)\n    0.747042 seconds (9.65 k allocations: 275.819 MB, 4.87% gc time)\nnorm(la*x-b)\n    19.840756251253442\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5)\n    6.509665 seconds (22.55 k allocations: 1.680 GB, 3.68% gc time)\nnorm(la*x-b)\n    19.222483530605043", 
            "title": "Iterative Solvers"
        }, 
        {
            "location": "/solvers/index.html#low-stretch-spanning-trees", 
            "text": "In order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm. randishKruskall  samples the remaining edges with probability proportional to their weight.   randishPrim  samples edges on the boundary while using the same rule.  Both use a data structure called  Sampler  that allows you to store integers with real values, and to sample according to those real values.  We also have code for computing the stretches.\nHere are some examples.  a = grid2(1000)\nt = randishKruskal(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    43.410262262262265\n\nt = randishPrim(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    33.14477077077077", 
            "title": "Low-Stretch Spanning Trees"
        }, 
        {
            "location": "/solvers/index.html#augmented-spanning-tree-preconditioners", 
            "text": "Here is code that will invoke one. \nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.  a = mapweight(grid2(1000),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nla[1,1] = la[1,1] + 1\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000)\n    6.529052 seconds (4.00 M allocations: 1.858 GB, 15.34% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.058915 seconds (9.74 k allocations: 23.209 GB, 6.84% gc time)\n\nnorm(la*x - b)\n    99.74452367765869\n\n# Now, let's contrast with using CG\n\n@time y = cg(la,b,tol=1e-1,maxits=1000)\n    28.719631 seconds (4.01 k allocations: 7.473 GB, 3.74% gc time)\n\nnorm(la*y-b)\n    3243.6014713600766  That was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.  @time F = augTreeSolver(la,tol=1e-1,maxits=1000,treeAlg=randishPrim);\n    6.319489 seconds (4.00 M allocations: 2.030 GB, 18.81% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.503484 seconds (9.76 k allocations: 23.268 GB, 7.31% gc time)\n\nnorm(la*x - b)  \n    99.29610874176991  To solve systems in a Laplacian, we could wrap it.  n = 40000\nla = lap(randRegular(n,3))\nf = lapWrapSolver(augTreeSolver,la,tol=1e-6,maxits=1000)\nb = randn(n); b = b - mean(b)\nx = f(b)\nnorm(la*x-b)\n    0.00019304778073388  As you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Augmented spanning tree preconditioners"
        }, 
        {
            "location": "/generators/index.html", 
            "text": "The following is a list of the graph generators.\n\n\nDeterministic\n\n\ncompleteGraph\n\n\ncompleteGraph(n::Int64)\n\n\n\n\nThe complete graph \n\n\npathGraph\n\n\npathGraph(n::Int64)\n\n\n\n\nThe path graph on n vertices \n\n\nringGraph\n\n\nringGraph(n::Int64)\n\n\n\n\nThe simple ring on n vertices\n\n\ngeneralizedRing\n\n\ngeneralizedRing(n::Int64, gens)\n\n\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example, \n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\nhyperCube\n\n\nhyperCube(d::Int64)\n\n\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\ncompleteBinaryTree\n\n\ncompleteBinaryTree(n::Int64)\n\n\n\n\nThe complete binary tree on n vertices\n\n\ngrid2\n\n\ngrid2(n::Int64)\ngrid2(n::Int64, m::Int64)\n\n\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\ngrid2coords\n\n\ngrid2coords(n::Int64, m::Int64)\ngrid2coords(n)\n\n\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\nRandom\n\n\nThese are randomized graph generators.\n            ### randMatching\n\n\nrandMatching(n::Int64)\n\n\n\n\nA random matching on n vertices\n\n\nrandRegular\n\n\nrandRegular(n::Int64, k::Int64)\n\n\n\n\nA sum of k random matchings on n vertices\n\n\ngrownGraph\n\n\ngrownGraph(n::Int64, k::Int64)\n\n\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.    \n\n\ngrownGraphD\n\n\ngrownGraphD(n::Int64, k::Int64)\n\n\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\nprefAttach\n\n\nprefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.\n\n\ngeneralizedNecklace\n\n\ngeneralizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti\n:Integer}, k::Int64)\n\n\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.", 
            "title": "Generators"
        }, 
        {
            "location": "/generators/index.html#deterministic", 
            "text": "completeGraph  completeGraph(n::Int64)  The complete graph   pathGraph  pathGraph(n::Int64)  The path graph on n vertices   ringGraph  ringGraph(n::Int64)  The simple ring on n vertices  generalizedRing  generalizedRing(n::Int64, gens)  A generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example,   generalizedRing(17, [1 5])  hyperCube  hyperCube(d::Int64)  The d dimensional hypercube.  Has 2^d vertices  completeBinaryTree  completeBinaryTree(n::Int64)  The complete binary tree on n vertices  grid2  grid2(n::Int64)\ngrid2(n::Int64, m::Int64)  An n-by-m grid graph.  iostropy is the weighting on edges in one direction.  grid2coords  grid2coords(n::Int64, m::Int64)\ngrid2coords(n)  Coordinates for plotting the vertices of the n-by-m grid graph", 
            "title": "Deterministic"
        }, 
        {
            "location": "/generators/index.html#random", 
            "text": "These are randomized graph generators.\n            ### randMatching  randMatching(n::Int64)  A random matching on n vertices  randRegular  randRegular(n::Int64, k::Int64)  A sum of k random matchings on n vertices  grownGraph  grownGraph(n::Int64, k::Int64)  Create a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.      grownGraphD  grownGraphD(n::Int64, k::Int64)  Like a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices  prefAttach  prefAttach(n::Int64, k::Int64, p::Float64)  A preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.  generalizedNecklace  generalizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti :Integer}, k::Int64)  Constructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.", 
            "title": "Random"
        }
    ]
}