{
    "docs": [
        {
            "location": "/about/index.html", 
            "text": "About Laplacians.jl\n\n\nLaplacians is a package containing graph algorithms, with an emphsasis on tasks related to spectral and algebraic graph theory.  It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.\n\n\nAll graphs are represented by sparse adjacency matrices.\nThis is both for speed, and because our main concerns are algebraic tasks.\n\n\nLaplacians.jl was started by Daniel A. Spielman.  Other contributors include Xiao Shi, Serban Stan and Jackson Thea.", 
            "title": "About"
        }, 
        {
            "location": "/about/index.html#about-laplaciansjl", 
            "text": "Laplacians is a package containing graph algorithms, with an emphsasis on tasks related to spectral and algebraic graph theory.  It contains (and will contain more) code for solving systems of linear equations in graph Laplacians, low stretch spanning trees, sparsifiation, clustering, local clustering, and optimization on graphs.  All graphs are represented by sparse adjacency matrices.\nThis is both for speed, and because our main concerns are algebraic tasks.  Laplacians.jl was started by Daniel A. Spielman.  Other contributors include Xiao Shi, Serban Stan and Jackson Thea.", 
            "title": "About Laplacians.jl"
        }, 
        {
            "location": "/Julia/index.html", 
            "text": "Using Julia\n\n\nConverting to Julia 0.4\n\n\nSmall details\n\n\n\n\n\n\nJulia Notebooks\n\n\nWorkflows\n\n\nDan's current workflow:\n\n\nAdd your current workflow here:\n\n\n\n\n\n\nThings to be careful of (common bugs)\n\n\nUseful Julia functions\n\n\nOptimizing code in Julia\n\n\nVectorization is Bad.\n\n\n\n\n\n\nHow should Julia packages be organized?\n\n\nHow should notebooks play with Git?\n\n\n\n\n\n\n\n\n\n\nUsing Julia\n\n\nTo use the julia notebooks, you will need ipython and the IJulia package.  You should also get \njupyter\n, which you should be able to install from ipython.\nTo install IJulia, you type \nPkg.add(\"IJulia\")\n from Julia.\nThen, you just need to type \nusing IJulia\n once.  This will tell jupyter about the Julia kernel.  To run Julia notebooks, you now type \njupyter notebook\n.  You can select the kernel your new notebook is using.\n\n\nDan recommends installing the anaconda distribution of python.\nYou will then need to install some things from that, like\nconda install jupyter (the new notebooks package)\nconda install mathjax\nconda install matplotlib\n\n\nThis repository contains projects implemented in Julia by Dan Spielman's group.  While organizing by language is strange, we are trying it to help us learn the language.\n\n\nThere are two projects in here so far.  One, yinsGraph, is for doing graph theory and solving Laplacian systems.  It's documentation is at \nyinsGraph.md\n\n\nI would like to use the documentation pages in this root directory to discuss issues with how to get Julia to work well.  For now, I'll just ask some questions and write a little that I've figured out.  If you figure some out, please write it here.\n\n\nConverting to Julia 0.4\n\n\nA description of the changes made in Julia 0.4 appears to be here\n[https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md]\n(https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md)\n\n\nThe first problem you will encounter when using Julia 0.4 is that it is stingier about its load path.  It won't load something from the current directory unless it is in the load path.  You can add a directory to the load path like\n\n\npush!(LOAD_PATH,\n.\n)\n\n\n\n\nor\n\n\npush!(LOAD_PATH,\n/Users/[your_username]/git/julia/yinsGraph\n)\n\n\n\n\nTo overcome this issue, you can add the above line to the end of the julia.rc file, found in\n\n\n/Applications/Julia-0.4.0-rc4.app/Contents/Resources/julia/etc/julia\n\n\n\n\nJulia 0.4 lets you take advantage of docstrings.\nFor example, \n?ringGraph\n produces\n\n\nThe simple ring on n vertices\n\n\n\n\nWhen having a multiline comment, make sure that lines don't have starting and trailing spaces.\nThis will mess up the indentation when calling '?func_name'.\n\n\nSmall details\n\n\n\n\nJulia 0.4 is trying to wean you off Matlab-like notation.  You should no longer create vectors like \n[1:n]\n.  Instead, you should type \ncollect(1:n)\n\n\n\n\nJulia Notebooks\n\n\nTo get the Julia notebooks working, I presently type \njupyter notebook\n.\nI then select the kernel to be Julia-0.3.11.\nIt seems important to run this command from a directory that contains all the directories\nthat have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks\nfrom other directories.  That has only given me trouble.\n\n\nThe calico extensions that seem to be hosted at Brynmawr seem interesting.\nI haven't yet figured out how to get them to work.\nHere are the relevent links:\n\n\n\n\nhttp://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Help.ipynb\n\n\nhttp://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Notebook%20Users%20Manual.ipynb\n\n\n\n\nTo turn a notebook into html, you type something like\n\n\nipython nbconvert Laplacians.ipynb\n\n\n\n\nWorkflows\n\n\nJulia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:\n\n\nDan's current workflow:\n\n\n\n\nI use emacs (which has a mode for Julia) and the notebooks.\n\n\nI develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.\n\n\n\n\nAfter modifying code, I reload it with \ninclude(\"develX.jl\")\n.  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.\n\n\n\n\n\n\nI am writing this documention in MacDown.\n\n\n\n\n\n\nAdd your current workflow here:\n\n\nThings to be careful of (common bugs)\n\n\n\n\n\n\nJulia passes vectors and matrices to routines by reference, rather than by copying them.  If you type \nx = y\n when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type \nx = copy(y)\n.  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.\n\n\n\n\n\n\nOn the other hand, if you type \nx = x + y\n, then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type \nx += y\n.  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.\n\n\n\n\n\n\n\n\nAdds b in to a\n\nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend\n\n\nFails to add b in to a\n\nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2\n\n\n\n\n\n\n\n\n\nIf you are used to programming in Matlab, you might be tempted to type a line like \nfor i in 1:10,\n.  \nDo not put extra commas in Julia!\n  It will cause bad things to happen.\n\n\n\n\n\n\nJulia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the \nsetValue\n function. \nsetValue(G, u, i, 0)\n will set \nweighti(G, u, i)\n to 0 while also leaving \n(u, nbri(G, u, i))\n in the matrix.\n\n\n\n\n\n\nUseful Julia functions\n\n\nI am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.\n\n\n\n\n\n\ndocstrings: in the above example, I used a docstring to document each function.  You can get these by typing \n?addA2B\n.  You can also  \nwrite longer docstrings and use markdown\n.  I suggest putting them in front of every function.\n\n\n\n\n\n\nmethods(foo)\n lists all methods with the name foo.\n\n\n\n\nfieldnames(footype)\n tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type \nnames(footype)\n\n\n\n\njulia\n a = sparse(rand(3,3));\njulia\n fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval\n\n\n\n\nOptimizing code in Julia\n\n\nThe best way that I've found of figuring out what's slowing down my code has been to use \n@code_warntype\n.  It only exists in version 4 of Julia.  For this reason, I keep one of those around.\n\n\nNote that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use \n@time\n to time your code.\n\n\nI recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.\n\n\nVectorization is Bad.\n\n\nJulia is the anti-matlab in that vectorization is slow.\nStill it is a good way to write your code the first time.\nHere are some examples of code that adds one vector into another.\nThe first is vectorized, the second turns that into a loop, and the fastest uses BLAS.  Note that this was done in Julia 0.3.11.  The vectorized code is much faster, but still not fast, in 0.4.\n\nAlso note that you have to run each routine once before it will be fast.  This is because it compiles it the first time your run it\n\n\nn = 10^7\na = rand(n)\nb = rand(n)\n@time a += b;\n\nelapsed time: 0.155296017 seconds (80000312 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time add2(a,b);\n\nelapsed time: 0.021190554 seconds (80 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time BLAS.axpy!(1.0,b,a);\n\nelapsed time: 0.015894922 seconds (80 bytes allocated)\n\n\n\n\n\nOne reason that \na += b\n was slow was that it seems to allocate a lot of memory.\n\n\nHow should Julia packages be organized?\n\n\nIn yinsGraph, I decided to just make one big module called yinsGraph.jl.  It then includes a bunch of individual files, most of which contain many functions and types.  I think this is much nicer than making one file per functions, as some functions are very short.\n\n\nI put the export statements in the main module.  The reason for this is that while developing code in a file, I don't include that in the module.  This way I can reload it as I change it without having to restart the kernel.  This does not seem to work as well for types.  I'm not sure why.\n\n\nHow should notebooks play with Git?\n\n\nThe great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#using-julia", 
            "text": "To use the julia notebooks, you will need ipython and the IJulia package.  You should also get  jupyter , which you should be able to install from ipython.\nTo install IJulia, you type  Pkg.add(\"IJulia\")  from Julia.\nThen, you just need to type  using IJulia  once.  This will tell jupyter about the Julia kernel.  To run Julia notebooks, you now type  jupyter notebook .  You can select the kernel your new notebook is using.  Dan recommends installing the anaconda distribution of python.\nYou will then need to install some things from that, like\nconda install jupyter (the new notebooks package)\nconda install mathjax\nconda install matplotlib  This repository contains projects implemented in Julia by Dan Spielman's group.  While organizing by language is strange, we are trying it to help us learn the language.  There are two projects in here so far.  One, yinsGraph, is for doing graph theory and solving Laplacian systems.  It's documentation is at  yinsGraph.md  I would like to use the documentation pages in this root directory to discuss issues with how to get Julia to work well.  For now, I'll just ask some questions and write a little that I've figured out.  If you figure some out, please write it here.", 
            "title": "Using Julia"
        }, 
        {
            "location": "/Julia/index.html#converting-to-julia-04", 
            "text": "A description of the changes made in Julia 0.4 appears to be here\n[https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md]\n(https://github.com/JuliaLang/julia/blob/release-0.4/NEWS.md)  The first problem you will encounter when using Julia 0.4 is that it is stingier about its load path.  It won't load something from the current directory unless it is in the load path.  You can add a directory to the load path like  push!(LOAD_PATH, . )  or  push!(LOAD_PATH, /Users/[your_username]/git/julia/yinsGraph )  To overcome this issue, you can add the above line to the end of the julia.rc file, found in  /Applications/Julia-0.4.0-rc4.app/Contents/Resources/julia/etc/julia  Julia 0.4 lets you take advantage of docstrings.\nFor example,  ?ringGraph  produces  The simple ring on n vertices  When having a multiline comment, make sure that lines don't have starting and trailing spaces.\nThis will mess up the indentation when calling '?func_name'.  Small details   Julia 0.4 is trying to wean you off Matlab-like notation.  You should no longer create vectors like  [1:n] .  Instead, you should type  collect(1:n)", 
            "title": "Converting to Julia 0.4"
        }, 
        {
            "location": "/Julia/index.html#julia-notebooks", 
            "text": "To get the Julia notebooks working, I presently type  jupyter notebook .\nI then select the kernel to be Julia-0.3.11.\nIt seems important to run this command from a directory that contains all the directories\nthat have notebooks that you will use.  In particular, I advise against \"uploading\" notebooks\nfrom other directories.  That has only given me trouble.  The calico extensions that seem to be hosted at Brynmawr seem interesting.\nI haven't yet figured out how to get them to work.\nHere are the relevent links:   http://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Help.ipynb  http://jupyter.cs.brynmawr.edu/hub/dblank/public/Jupyter%20Notebook%20Users%20Manual.ipynb   To turn a notebook into html, you type something like  ipython nbconvert Laplacians.ipynb", 
            "title": "Julia Notebooks"
        }, 
        {
            "location": "/Julia/index.html#workflows", 
            "text": "Julia has an IDE called Juno.  Both Dan and Serban have encountered some trouble with it: we have both found that it sometimes refuses to reload .jl code that we have written.  Please document workflows that you have found useful here:  Dan's current workflow:   I use emacs (which has a mode for Julia) and the notebooks.  I develop Julia code in a \"temporary\" file with a name like develX.jl.  While I am developing, this code is not included by the module to which it will eventually belong.   After modifying code, I reload it with  include(\"develX.jl\") .  This works fine for reloading methods.  It is not a good way to reload modules or types.  So, I usually put the types either in a separate file, or in my julia notebook.    I am writing this documention in MacDown.    Add your current workflow here:", 
            "title": "Workflows"
        }, 
        {
            "location": "/Julia/index.html#things-to-be-careful-of-common-bugs", 
            "text": "Julia passes vectors and matrices to routines by reference, rather than by copying them.  If you type  x = y  when x and y are arrays, then this will make x a pointer to y.  If you want x to be a copy of y, type  x = copy(y) .  This can really mess up matlab programmers.  I wrote many functions that were modifying their arguments without realizing it.    On the other hand, if you type  x = x + y , then x becomes a newly allocated vector and no longer refers to the original.  This is true even if you type  x += y .  Here is an example that shows two of the possible behaviors, and the difference between what happens inside functions.     Adds b in to a \nfunction addA2B(a,b)\n    for i in 1:length(a)\n        a[i] += b[i]\n    end\nend Fails to add b in to a \nfunction addA2Bfail(a,b)\n    a += b\nend\n\na = [1 0]\nb = [2 2]\naddA2B(a,b)\na\n\n1x2 Array{Int64,2}:\n 3  2\n\na = [1 0]\nb = [2 2]\naddA2Bfail(a,b)\na\n\n1x2 Array{Int64,2}:\n 1  0\n\na += b\na\n\n1x2 Array{Int64,2}:\n 3  2    If you are used to programming in Matlab, you might be tempted to type a line like  for i in 1:10, .   Do not put extra commas in Julia!   It will cause bad things to happen.    Julia sparse matrix entries dissapear if they are set to 0. In order to overcome this, use the  setValue  function.  setValue(G, u, i, 0)  will set  weighti(G, u, i)  to 0 while also leaving  (u, nbri(G, u, i))  in the matrix.", 
            "title": "Things to be careful of (common bugs)"
        }, 
        {
            "location": "/Julia/index.html#useful-julia-functions", 
            "text": "I am going to make a short list of Julia functions/features that I find useful.  Please add those that you use often as well.    docstrings: in the above example, I used a docstring to document each function.  You can get these by typing  ?addA2B .  You can also   write longer docstrings and use markdown .  I suggest putting them in front of every function.    methods(foo)  lists all methods with the name foo.   fieldnames(footype)  tells you all the fields of footype.  Note that this is 0.4.  In 0.3.11, you type  names(footype)   julia  a = sparse(rand(3,3));\njulia  fieldnames(a)\n5-element Array{Symbol,1}:\n :m\n :n\n :colptr\n :rowval\n :nzval", 
            "title": "Useful Julia functions"
        }, 
        {
            "location": "/Julia/index.html#optimizing-code-in-julia", 
            "text": "The best way that I've found of figuring out what's slowing down my code has been to use  @code_warntype .  It only exists in version 4 of Julia.  For this reason, I keep one of those around.  Note that the first time you run a piece of code in Julia, it gets compiled.  So, you should run it on a small example before trying to time it.  Then, use  @time  to time your code.  I recommend reading the Performance Tips in the Julia documentation, not that I've understood all of it yet.  Vectorization is Bad.  Julia is the anti-matlab in that vectorization is slow.\nStill it is a good way to write your code the first time.\nHere are some examples of code that adds one vector into another.\nThe first is vectorized, the second turns that into a loop, and the fastest uses BLAS.  Note that this was done in Julia 0.3.11.  The vectorized code is much faster, but still not fast, in 0.4. Also note that you have to run each routine once before it will be fast.  This is because it compiles it the first time your run it  n = 10^7\na = rand(n)\nb = rand(n)\n@time a += b;\n\nelapsed time: 0.155296017 seconds (80000312 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time add2(a,b);\n\nelapsed time: 0.021190554 seconds (80 bytes allocated)\n\na = rand(n)\nb = rand(n)\n@time BLAS.axpy!(1.0,b,a);\n\nelapsed time: 0.015894922 seconds (80 bytes allocated)  One reason that  a += b  was slow was that it seems to allocate a lot of memory.", 
            "title": "Optimizing code in Julia"
        }, 
        {
            "location": "/Julia/index.html#how-should-julia-packages-be-organized", 
            "text": "In yinsGraph, I decided to just make one big module called yinsGraph.jl.  It then includes a bunch of individual files, most of which contain many functions and types.  I think this is much nicer than making one file per functions, as some functions are very short.  I put the export statements in the main module.  The reason for this is that while developing code in a file, I don't include that in the module.  This way I can reload it as I change it without having to restart the kernel.  This does not seem to work as well for types.  I'm not sure why.", 
            "title": "How should Julia packages be organized?"
        }, 
        {
            "location": "/Julia/index.html#how-should-notebooks-play-with-git", 
            "text": "The great thing about the notebooks is that they contain live code, so that you can play with them.  But, sometimes you get a version that serves as great documentation, and you don't want to klobber it my mistake later (or evern worse, have someone else klobber it).  Presumably if someone accidently commits a messed up version we can unwind that.  But, is there a good way to keep track of this?", 
            "title": "How should notebooks play with Git?"
        }, 
        {
            "location": "/Laplacians/index.html", 
            "text": "Laplacians\n\n\nInstallation\n\n\nTo use Laplacians\n\n\nAbout this Documentation\n\n\nSome Documentation that hasn't yet made it elsewhere.\n\n\nFundamental Graph Algorithms:\n\n\nSolving Linear equations:\n\n\n\n\n\n\nTo develop Laplacians\n\n\nUsing sparse matrices as graphs\n\n\nParametric Types\n\n\nData structures:\n\n\nInterface issue:\n\n\nWriting tests:\n\n\n\n\n\n\nIntegration with other packages.\n\n\n\n\n\n\n\n\n\n\nLaplacians\n\n\nInstallation\n\n\nFirst, you will need Julia.\nYou will also need a number of Julia packages.\n\nYou install these like\n\n\nPkg.add(\nPyPlot\n)\nPkg.add(\nDataStructures\n)\n\n\n\n\nOnce these are installed, you can use Laplacians.\nRight now, Laplacians is a module, not a package.\nSo, we will need to do a little more to get it started.\nIn the directory where Laplacians resides, type the following:\n\n\npush!(LOAD_PATH,\nsrc\n)\nusing Laplacians\n\n\n\n\nYou may need to install matplotlib in python before PyPlot.\nLook at this page for more information: https://github.com/stevengj/PyPlot.jl\n\n\nIf you discover that you need any other packages, please list them above.\n\n\nOther recommended (but not necessary) packages are:\n\n\n\n\nOptim\n\n\n\n\nTo use Laplacians\n\n\nExamples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.\n\n\nRight now, the notebooks worth looking at are:\n\n\n\n\nyinsGraph\n - usage, demo, and speed tests (Laplacians was previously called yinsGraph)\n\n\nSolvers\n - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.\n\n\n\n\n(I suggest that you open the html in your browser)\n\n\nAbout this Documentation\n\n\nThis documentation is still very rough.\nIt is generated by a combination of Markdown and semi-automatic generation.  The steps to generate and improve it are:\n\n\n\n\nEdit Markdown files in the \ndocs\n directory.  For example, you could use MacDown to do this.\n\n\nIf you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.\n\n\nAdd docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.  To build the API, type\n\n\n\n\ninclude(\ndocs/build.jl\n)\n\n\n\n\n\n\nRun \nmkdocs build\n in the root directory to regenerate the documentation from the Markdown.\n\n\n\n\nSome Documentation that hasn't yet made it elsewhere.\n\n\nFundamental Graph Algorithms:\n\n\n\n\ncomponents\n computes connected components, returns as a vector\n\n\nvecToComps\n turns into an array with a list of vertices in each component\n\n\nshortestPaths(mat, start)\n  returns an array of distances,\n    and pointers to the node closest (parent array)\n\n\nkruskal(mat; kind=:min)\n  to get a max tree, use \nkind = :max\n\n    returns it as a sparse matrix.\n\n\n\n\nSolving Linear equations:\n\n\nWe have implemented Conjugate Gradient (cg) and the Preconditioned Conjugate Gradient (pcg).  These implementations use BLAS when they can, and a slower routine for data types like BigFloat.\n\n\nTo learn more, read \nsolvers.md\n.\n\n\nTo develop Laplacians\n\n\nJust go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"\n\n\nI think that each file should contain a manifest up top listing the functions and types that it provides.  They should be divided up into those that are for internal use only, and those that should be exported.  Old code that didn't work well, but which you want to keep for reference should go at the end.\n\n\nUsing sparse matrices as graphs\n\n\nThe routines \ndeg\n, \nnbri\n and \nweighti\n will let you treat a sparse matrix like a graph.\n\n\ndeg(graph, u)\n is the degree of node u.\n\nnbri(graph, u, i)\n is the ith neighbor of node u.\n\nweighti(graph, u, i)\n is the weight of the edge to the ith neighbor of node u.\n\n\nNote that we start indexing from 1.\n\n\nFor example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:\n\n\n  for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end\n\n\n\n\nBut, this turns out to be much slower than working with the structure directly, like\n\n\n  for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end\n\n\n\n\n\n\n[ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.\n\n\n\n\nParametric Types\n\n\nA sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\n\n\n\n\nTv\n, sometimes written \nTval\n denotes the types of the values, and \nTi\n or \nTind\n denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths, \nstart\n is of type \nTi\n.  Inside the code, whenever we write something like \npArray = zeros(Ti,n)\n, it creates an array of zeros of type Ti.  Using these parameteric types is \nmuch\n faster than leaving the types unfixed.\n\n\nData structures:\n\n\n\n\nIntHeap\n a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.\n\n\n\n\nInterface issue:\n\n\nThere are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.\n\n\nWriting tests:\n\n\nI haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.\n\n\nIntegration with other packages.\n\n\nThere are other graph packages that we might want to sometimes use.\n\n\n\n\nGraphs.jl\n : I found this one to be too slow and awkward to be useful.\n\n\nLightGraphs.jl\n : this looks more promising.  We will have to check it out.", 
            "title": "Overview"
        }, 
        {
            "location": "/Laplacians/index.html#laplacians", 
            "text": "", 
            "title": "Laplacians"
        }, 
        {
            "location": "/Laplacians/index.html#installation", 
            "text": "First, you will need Julia.\nYou will also need a number of Julia packages. \nYou install these like  Pkg.add( PyPlot )\nPkg.add( DataStructures )  Once these are installed, you can use Laplacians.\nRight now, Laplacians is a module, not a package.\nSo, we will need to do a little more to get it started.\nIn the directory where Laplacians resides, type the following:  push!(LOAD_PATH, src )\nusing Laplacians  You may need to install matplotlib in python before PyPlot.\nLook at this page for more information: https://github.com/stevengj/PyPlot.jl  If you discover that you need any other packages, please list them above.  Other recommended (but not necessary) packages are:   Optim", 
            "title": "Installation"
        }, 
        {
            "location": "/Laplacians/index.html#to-use-laplacians", 
            "text": "Examples of how to do many things in yinsGraph may be found in the IJulia notebooks.  These have the extensions .ipynb.  When they look nice, I think it makes sense to convert them to .html.  Right now, the notebooks worth looking at are:   yinsGraph  - usage, demo, and speed tests (Laplacians was previously called yinsGraph)  Solvers  - code for solving equations.  How to use direct methods, conjugate gradient, and a preconditioned augmented spanning tree solver.   (I suggest that you open the html in your browser)", 
            "title": "To use Laplacians"
        }, 
        {
            "location": "/Laplacians/index.html#about-this-documentation", 
            "text": "This documentation is still very rough.\nIt is generated by a combination of Markdown and semi-automatic generation.  The steps to generate and improve it are:   Edit Markdown files in the  docs  directory.  For example, you could use MacDown to do this.  If you want to add a new page to the documention, create one.  Edit the file mkdocs.yml so show where it should appear.  Add docstrings to everything that needs it, and in particular to the routines you create.  The API is built from the docstrings.  To build the API, type   include( docs/build.jl )   Run  mkdocs build  in the root directory to regenerate the documentation from the Markdown.", 
            "title": "About this Documentation"
        }, 
        {
            "location": "/Laplacians/index.html#some-documentation-that-hasnt-yet-made-it-elsewhere", 
            "text": "Fundamental Graph Algorithms:   components  computes connected components, returns as a vector  vecToComps  turns into an array with a list of vertices in each component  shortestPaths(mat, start)   returns an array of distances,\n    and pointers to the node closest (parent array)  kruskal(mat; kind=:min)   to get a max tree, use  kind = :max \n    returns it as a sparse matrix.   Solving Linear equations:  We have implemented Conjugate Gradient (cg) and the Preconditioned Conjugate Gradient (pcg).  These implementations use BLAS when they can, and a slower routine for data types like BigFloat.  To learn more, read  solvers.md .", 
            "title": "Some Documentation that hasn't yet made it elsewhere."
        }, 
        {
            "location": "/Laplacians/index.html#to-develop-laplacians", 
            "text": "Just go for it.\nDon't worry about writing fast code at first.\nJust get it to work.\nWe can speed it up later.\nThe yinsGraph.ipynb notebook contains some examples of speed tests.\nWithin some of the files, I am keeping old, unoptimized versions of code around for comparison (and for satisfaction).  I will give them the name \"XSlow\"  I think that each file should contain a manifest up top listing the functions and types that it provides.  They should be divided up into those that are for internal use only, and those that should be exported.  Old code that didn't work well, but which you want to keep for reference should go at the end.  Using sparse matrices as graphs  The routines  deg ,  nbri  and  weighti  will let you treat a sparse matrix like a graph.  deg(graph, u)  is the degree of node u. nbri(graph, u, i)  is the ith neighbor of node u. weighti(graph, u, i)  is the weight of the edge to the ith neighbor of node u.  Note that we start indexing from 1.  For example, to iterate over the neighbors of node v,\n  and play with the attached nodes, you could write code like:    for i in 1:deg(mat, v)\n     nbr = nbri(mat, v, i)\n     wt = weighti(mat, v, i)\n     foo(v, nbr, wt)\n  end  But, this turns out to be much slower than working with the structure directly, like    for ind in mat.colptr[v]:(mat.colptr[v+1]-1)\n      nbr = mat.rowval[ind]\n      wt = mat.nzval[ind]\n      foo(v, nbr, wt)\n  end   [ ] Maybe we can make a macro to replace those functions.  It could be faster and more readable.   Parametric Types  A sparse matrix has two types associated with it: the types of its indices (some sort of integer) and the types of its values (some sort of number).  Most of the code has been written so that once these types are fixed, the type of everything else in the function has been too.  This is accomplished by putting curly braces after a function name, with the names of the types that we want to use in the braces.  For example,  shortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)  Tv , sometimes written  Tval  denotes the types of the values, and  Ti  or  Tind  denotes the types of the indices.  This function will only be called if the node from which we compute the shortest paths,  start  is of type  Ti .  Inside the code, whenever we write something like  pArray = zeros(Ti,n) , it creates an array of zeros of type Ti.  Using these parameteric types is  much  faster than leaving the types unfixed.  Data structures:   IntHeap  a heap that stores small integers (like indices of nodes in a graph) and that makes deletion fast.  Was much faster than using Julia's more general heap.   Interface issue:  There are many different sorts of things that our code could be passing around.  For example, kruskal returns a graph as a sparse matrix.  But, we could use a format that is more specialized for trees, like the RootedTree type.  At some point, when we optimize code, we will need to figure out the right interfaces between routines.  For example, some routines symmetrize at the end.  This is slow, and should be skipped if not necessary.  It also doubles storage.  Writing tests:  I haven't written any yet.  I'll admit that I'm using the notebooks as tests.  If I can run all the cells, then it's all good.", 
            "title": "To develop Laplacians"
        }, 
        {
            "location": "/Laplacians/index.html#integration-with-other-packages", 
            "text": "There are other graph packages that we might want to sometimes use.   Graphs.jl  : I found this one to be too slow and awkward to be useful.  LightGraphs.jl  : this looks more promising.  We will have to check it out.", 
            "title": "Integration with other packages."
        }, 
        {
            "location": "/solvers/index.html", 
            "text": "Solving linear equations in Laplacians\n\n\nDirect Solvers\n\n\nIterative Solvers\n\n\nLow-Stretch Spanning Trees\n\n\nAugmented spanning tree preconditioners\n\n\n\n\n\n\n\n\n\n\nSolving linear equations in Laplacians\n\n\nRight now, our solver code is in \nsolvers.jl\n, but not included in yinsGraph.  So, you should include this directly.  Implementations of cg and pcg have been automatically included in yinsGraph.  They are in the file \npcg.jl\n\n\nFor some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.\n\n\nDirect Solvers\n\n\nYou can compute a cholesky factor directly with \ncholfact\n.  It does  more than just compute the factor, and it saves its result in a data structure that implements \n\\\n.  It uses SuiteSparse by Tim Davis.\n\n\nHere is an example of how you would use it to solve a general non-singular linear system.\n\n\na = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14\n\n\n\n\nLaplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.\n\n\nla = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15\n\n\n\n\nHere are two other ways of using the wrapper:\n\n\nlapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n);\nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15\n\n\n\n\nIterative Solvers\n\n\nThe first, of course, is the Conjugate Gradient (cg).\n\n\nOur implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance \ntol\n and the maximum number of iterations, \nmaxits\n.  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.\n\n\nn = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100)\nnorm(a*x - b)\n    1.2191649497921835e-6\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\nnorm(a*xbig - bbig)\n    1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33\n\n\n\n\nAs a sanity check, we do two speed tests against Matlab.\n\n\nla = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064\n\n\n\n\nAnd, in Matlab:\n\n\n a = grid2(200);\n\n la = lap(a);\n\n b = randn(length(a),1); b = b - mean(b);\n\n tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.\n\n norm(la*x-b)\n\nans =\n\n   1.9730e-04\n\n\n\n\nPCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.\n\n\na = mapweight(grid2(200),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000)\n    3.322035 seconds (42.21 k allocations: 1.194 GB, 5.11% gc time)\nnorm(la*x-b)\n    0.008508746034886803\n\n\n\n\nIf our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.\n\n\n@time x = pcg(la,b,pre,tol=1e-1,maxits=10^5)\n    0.747042 seconds (9.65 k allocations: 275.819 MB, 4.87% gc time)\nnorm(la*x-b)\n    19.840756251253442\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5)\n    6.509665 seconds (22.55 k allocations: 1.680 GB, 3.68% gc time)\nnorm(la*x-b)\n    19.222483530605043\n\n\n\n\nLow-Stretch Spanning Trees\n\n\nIn order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm.\n\nrandishKruskall\n samples the remaining edges with probability proportional to their weight.  \nrandishPrim\n samples edges on the boundary while using the same rule.\n\n\nBoth use a data structure called \nSampler\n that allows you to store integers with real values, and to sample according to those real values.\n\n\nWe also have code for computing the stretches.\nHere are some examples.\n\n\na = grid2(1000)\nt = randishKruskal(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    43.410262262262265\n\nt = randishPrim(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    33.14477077077077\n\n\n\n\n\nAugmented spanning tree preconditioners\n\n\nHere is code that will invoke one.\nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.\n\n\na = mapweight(grid2(1000),x-\n1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nla[1,1] = la[1,1] + 1\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000)\n    6.529052 seconds (4.00 M allocations: 1.858 GB, 15.34% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.058915 seconds (9.74 k allocations: 23.209 GB, 6.84% gc time)\n\nnorm(la*x - b)\n    99.74452367765869\n\n# Now, let's contrast with using CG\n\n@time y = cg(la,b,tol=1e-1,maxits=1000)\n    28.719631 seconds (4.01 k allocations: 7.473 GB, 3.74% gc time)\n\nnorm(la*y-b)\n    3243.6014713600766\n\n\n\n\n\nThat was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.\n\n\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000,treeAlg=randishPrim);\n    6.319489 seconds (4.00 M allocations: 2.030 GB, 18.81% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.503484 seconds (9.76 k allocations: 23.268 GB, 7.31% gc time)\n\nnorm(la*x - b)\n    99.29610874176991\n\n\n\n\nTo solve systems in a Laplacian, we could wrap it.\n\n\nn = 40000\nla = lap(randRegular(n,3))\nf = lapWrapSolver(augTreeSolver,la,tol=1e-6,maxits=1000)\nb = randn(n); b = b - mean(b)\nx = f(b)\nnorm(la*x-b)\n    0.00019304778073388\n\n\n\n\nAs you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Solvers"
        }, 
        {
            "location": "/solvers/index.html#solving-linear-equations-in-laplacians", 
            "text": "Right now, our solver code is in  solvers.jl , but not included in yinsGraph.  So, you should include this directly.  Implementations of cg and pcg have been automatically included in yinsGraph.  They are in the file  pcg.jl  For some experiments with solvers, including some of those below, look at the notebook Solvers.ipynb.", 
            "title": "Solving linear equations in Laplacians"
        }, 
        {
            "location": "/solvers/index.html#direct-solvers", 
            "text": "You can compute a cholesky factor directly with  cholfact .  It does  more than just compute the factor, and it saves its result in a data structure that implements  \\ .  It uses SuiteSparse by Tim Davis.  Here is an example of how you would use it to solve a general non-singular linear system.  a = grid2(5)\nla = lap(a)\nla[1,1] = la[1,1] + 1\nF = cholfact(la)\n\nn = size(a)[1]\nb = randn(n)\nx = F \\ b\nnorm(la*x-b)\n\n    1.0598778281116327e-14  Laplacians, however, are singular.  So, we need to wrap the solver inside a routine that compensates for this.  la = lap(a)\nf = lapWrapSolver(cholfact,la)\nb = randn(n); b = b - mean(b);\nnorm(la*f(b) - b)\n    2.0971536951312585e-15  Here are two other ways of using the wrapper:  lapChol = lapWrapSolver(cholfact)\nf = lapChol(la)\nb = randn(n);\nb = b - mean(b);\nnorm(la*f(b) - b)\n    2.6924696662484416e-15\n\nx = lapWrapSolver(cholfact,la,b)\nnorm(la*x - b)\n    2.6924696662484416e-15", 
            "title": "Direct Solvers"
        }, 
        {
            "location": "/solvers/index.html#iterative-solvers", 
            "text": "The first, of course, is the Conjugate Gradient (cg).  Our implementation requires 2 arguments: the matrix and the right-hand vector.  It's optional arguments are the tolerance  tol  and the maximum number of iterations,  maxits .  It has been written to use BLAS when possible, and slower routines when dealing with data types that BLAS cannot handle.  Here are examples.  n = 50\na = randn(n,n); a = a * a';\nb = randn(n)\nx = cg(a,b,maxits=100)\nnorm(a*x - b)\n    1.2191649497921835e-6\n\nbbig = convert(Array{BigFloat,1},b)\nxbig = cg(a,bbig,maxits=100)\nnorm(a*xbig - bbig)\n    1.494919244242202629856363570306545126541716514824419323325986374186529786019681e-33  As a sanity check, we do two speed tests against Matlab.  la = lap(grid2(200))\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n@time x = cg(la,b,maxits=1000)\n    0.813791 seconds (2.77 k allocations: 211.550 MB, 3.56% gc time)\n\nnorm(la*x-b)\n    0.0001900620047823064  And, in Matlab:   a = grid2(200);  la = lap(a);  b = randn(length(a),1); b = b - mean(b);  tic; x = pcg(la,b,[],1000); toc\npcg converged at iteration 688 to a solution with relative residual 9.8e-07.\nElapsed time is 1.244917 seconds.  norm(la*x-b)\n\nans =\n\n   1.9730e-04  PCG also takes as input a preconditioner.  This should be a function.  Here is an example of how one might construct and use a diagonal preonditioner.  To motivate this, I will use a grid with highly varying weights on edges.  a = mapweight(grid2(200),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nb = randn(n)\nb = b - mean(b);\n\nd = diag(la)\npre(x) = x ./ d\n@time x = pcg(la,b,pre,maxits=2000)\n    3.322035 seconds (42.21 k allocations: 1.194 GB, 5.11% gc time)\nnorm(la*x-b)\n    0.008508746034886803  If our target is just low error, and we are willing to allow many iterations, here's how cg and pcg compare on this example.  @time x = pcg(la,b,pre,tol=1e-1,maxits=10^5)\n    0.747042 seconds (9.65 k allocations: 275.819 MB, 4.87% gc time)\nnorm(la*x-b)\n    19.840756251253442\n\n@time x = cg(la,b,tol=1e-1,maxits=10^5)\n    6.509665 seconds (22.55 k allocations: 1.680 GB, 3.68% gc time)\nnorm(la*x-b)\n    19.222483530605043", 
            "title": "Iterative Solvers"
        }, 
        {
            "location": "/solvers/index.html#low-stretch-spanning-trees", 
            "text": "In order to make preconditioners, we will want low-stretch spanning trees.  We do not yet have any code in Julia that is guaranteed to produce these.  Instead, for now, we have two routines that can be thought of as randomized versions of Prim and Kruskall's algorithm. randishKruskall  samples the remaining edges with probability proportional to their weight.   randishPrim  samples edges on the boundary while using the same rule.  Both use a data structure called  Sampler  that allows you to store integers with real values, and to sample according to those real values.  We also have code for computing the stretches.\nHere are some examples.  a = grid2(1000)\nt = randishKruskal(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    43.410262262262265\n\nt = randishPrim(a);\nst = compStretches(t,a);\nsum(st)/nnz(a)\n    33.14477077077077", 
            "title": "Low-Stretch Spanning Trees"
        }, 
        {
            "location": "/solvers/index.html#augmented-spanning-tree-preconditioners", 
            "text": "Here is code that will invoke one.\nIt is designed for positive definite systems.  So, let's give it one.\nRight now, it is using a randomized version of a MST.  There is no real reason to think that this should work.  a = mapweight(grid2(1000),x- 1/(rand(1)[1]));\nla = lap(a)\nn = size(la)[1]\nla[1,1] = la[1,1] + 1\n@time F = augTreeSolver(la,tol=1e-1,maxits=1000)\n    6.529052 seconds (4.00 M allocations: 1.858 GB, 15.34% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.058915 seconds (9.74 k allocations: 23.209 GB, 6.84% gc time)\n\nnorm(la*x - b)\n    99.74452367765869\n\n# Now, let's contrast with using CG\n\n@time y = cg(la,b,tol=1e-1,maxits=1000)\n    28.719631 seconds (4.01 k allocations: 7.473 GB, 3.74% gc time)\n\nnorm(la*y-b)\n    3243.6014713600766  That was not too impressive.  We will have to investigate.  By default, it presently uses randishKruskal.  Let's try randishPrim.  You can pass the treeAlg as a parameter.  @time F = augTreeSolver(la,tol=1e-1,maxits=1000,treeAlg=randishPrim);\n    6.319489 seconds (4.00 M allocations: 2.030 GB, 18.81% gc time)\n\nb = randn(n)\n@time x = F(b)\n    29.503484 seconds (9.76 k allocations: 23.268 GB, 7.31% gc time)\n\nnorm(la*x - b)\n    99.29610874176991  To solve systems in a Laplacian, we could wrap it.  n = 40000\nla = lap(randRegular(n,3))\nf = lapWrapSolver(augTreeSolver,la,tol=1e-6,maxits=1000)\nb = randn(n); b = b - mean(b)\nx = f(b)\nnorm(la*x-b)\n    0.00019304778073388  As you can see, lapWrapSolver can pass tol and maxits arguments to its solver, if they are given to it.", 
            "title": "Augmented spanning tree preconditioners"
        }, 
        {
            "location": "/wholeAPI/index.html", 
            "text": "ErdosRenyiCluster\n\n\nGenerate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.\n\n\nErdosRenyiCluster(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:319\n\n\nErdosRenyiClusterFix\n\n\nLike an Erdos-Renyi cluster, but add back a tree so it has n vertices\n\n\nErdosRenyiClusterFix(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:332\n\n\nLaplacians\n\n\nA package for graph computations related to graph Laplacians\n\n\nGraphs are represented by sparse adjacency matrices, etc.\n\n\nRootedTree\n\n\nSummary:\n\n\ntype Laplacians.RootedTree{Tval,Tind} \n: Any\n\n\n\n\nFields:\n\n\nroot     :: Tind\nparent   :: Array{Tind,1}\nchildren :: Array{Tind,1}\nweights  :: Array{Tval,1}\nnumKids  :: Array{Tind,1}\nkidsPtr  :: Array{Tind,1}\n\n\n\n\napr\n\n\ncomputes an approximate page rank vector from a starting vector s, an alpha and an epsilon \n\n\napr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, eps::Float64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/cutPageRank.jl:71\n\n\nbackIndices\n\n\ncomputes the back indices in a graph in O(M+N) \n\n\nbackIndices{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphUtils.jl:22\n\n\nbiggestComp\n\n\nReturn the biggest component in a graph\n\n\nbiggestComp(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphAlgs.jl:111\n\n\ncg\n\n\ncg(mat, b::Array{Float64,1})\ncg(mat, b::Array{Float32,1})\ncg(mat, b)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/pcg.jl:29\n\n\nchimera\n\n\nBuilds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nBuilds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace\n\n\nchimera(n::Integer)\nchimera(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:392\n\n\ncompDepth\n\n\ncompDepth{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/treeAlgs.jl:249\n\n\ncompStretches\n\n\ncompStretches{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti})\ncompStretches{Tv,Ti}(tree::SparseMatrixCSC{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/treeAlgs.jl:319\n\n\ncompleteBinaryTree\n\n\nThe complete binary tree on n vertices\n\n\ncompleteBinaryTree(n::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:119\n\n\ncompleteGraph\n\n\nThe complete graph\n\n\ncompleteGraph(n::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:54\n\n\ncomponents\n\n\ncomponents{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphAlgs.jl:43\n\n\ndeg\n\n\ndeg{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphUtils.jl:11\n\n\ndiagmat\n\n\nreturns the diagonal matrix(as a sparse matrix) of a graph\n\n\ndiagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:214\n\n\nedgeVertexMat\n\n\nThe signed edge-vertex adjacency matrix\n\n\nedgeVertexMat(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:78\n\n\ngeneralizedNecklace\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.\n\n\ngeneralizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti\n:Integer}, k::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:234\n\n\ngeneralizedRing\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example, \n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\ngeneralizedRing(n::Int64, gens)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:75\n\n\ngrid2\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\ngrid2(n::Int64)\ngrid2(n::Int64, m::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:139\n\n\ngrid2coords\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\ngrid2coords(n::Int64, m::Int64)\ngrid2coords(n)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:143\n\n\ngrownGraph\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.    \n\n\ngrownGraph(n::Int64, k::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:182\n\n\ngrownGraphD\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\ngrownGraphD(n::Int64, k::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:214\n\n\nhyperCube\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\nhyperCube(d::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:105\n\n\njoinGraphs\n\n\ncreate a disjoint union of graphs a and b,  and then put k random edges between them\n\n\njoinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:120\n\n\nkruskal\n\n\nkruskal{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphAlgs.jl:316\n\n\nlap\n\n\nCreate a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry\n\n\nlap(a)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:39\n\n\nmapweight\n\n\nCreate a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write\n\n\nb = mapweight(a, x-\nrand(1)[1])\n\n\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:56\n\n\nmatToTree\n\n\nmatToTree{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\nmatToTree{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, root::Ti)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/treeAlgs.jl:32\n\n\nmatToTreeDepth\n\n\nmatToTreeDepth{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti})\nmatToTreeDepth{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, root::Ti)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/treeAlgs.jl:98\n\n\nmaxflow\n\n\nimplementation of Dinic's algorithm. computes the maximum flow and min-cut in G between s and t. we consider the adjacency matrix to be the capacity matrix \n\n\nmaxflow{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Int64, t::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/flow.jl:4\n\n\nnbri\n\n\nnbri{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphUtils.jl:12\n\n\nnbrs\n\n\nnbrs{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphUtils.jl:14\n\n\npathGraph\n\n\nThe path graph on n vertices\n\n\npathGraph(n::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:45\n\n\npcg\n\n\npcg(mat, b::Array{Float64,1}, pre)\npcg(mat, b::Array{Float32,1}, pre)\npcg(mat, b, pre)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/pcg.jl:42\n\n\nplotGraph\n\n\nPlots graph gr with coordinates (x,y)\n\n\nplotGraph(gr, x, y)\nplotGraph(gr, x, y, color)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:138\n\n\nppr\n\n\ncomputes the personal page rank vector from a starting vector s and an alpha; operates with lazy walk matrix \n\n\nppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64)\nppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, niter::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/cutPageRank.jl:38\n\n\npr\n\n\ncomputes a page rank vector satisfying p = a/n * 1 + (1 - a) * W * p\n\n\npr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, alpha::Float64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/cutPageRank.jl:20\n\n\nprefAttach\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.\n\n\nprefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:238\n\n\nprn\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)\n\n\nthe PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper\n\n\nv is a set of vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]\n\n\nprn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/cutPageRank.jl:115\n\n\nproductGraph\n\n\nThe Cartesian product of two graphs.  When applied to two paths, it gives a grid.\n\n\nproductGraph(a0::SparseMatrixCSC{Tv,Ti\n:Integer}, a1::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:69\n\n\npureRandomGraph\n\n\nGenerate a random graph with n vertices from one of our natural distributions\n\n\npureRandomGraph(n::Integer)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:347\n\n\nrandGenRing\n\n\nA random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution\n\n\nrandGenRing(n::Int64, k::Integer)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:98\n\n\nrandMatching\n\n\nA random matching on n vertices\n\n\nrandMatching(n::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:154\n\n\nrandRegular\n\n\nA sum of k random matchings on n vertices\n\n\nrandRegular(n::Int64, k::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:167\n\n\nrandWeight\n\n\nApplies one of a number of random weighting schemes to the edges of the graph\n\n\nrandWeight(a)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:463\n\n\nrandperm\n\n\n..  randperm([rng,] n)\n\nConstruct a random permutation of length ``n``. The optional ``rng`` argument\nspecifies a random number generator, see :ref:`Random Numbers \nrandom-numbers\n`.\n\n\n\n\nRandomly permutes the vertex indices\n\n\nrandperm(r::AbstractRNG, n::Integer)\nrandperm(n::Integer)\nrandperm(mat::AbstractArray{T,2})\nrandperm(f::Expr)\n\n\n\n\nat random.jl:1341\n\n\nreadIJ\n\n\nreadIJ(filename::AbstractString)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:7\n\n\nringGraph\n\n\nThe simple ring on n vertices\n\n\nringGraph(n::Int64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:60\n\n\nsetValue\n\n\nsetValue{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti, a::Tv)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphUtils.jl:17\n\n\nshortIntGraph\n\n\nConvert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much\n\n\nshortIntGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:30\n\n\nshortestPaths\n\n\nshortestPaths{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, start::Ti)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphAlgs.jl:122\n\n\nspectralCoords\n\n\nComputes the spectral coordinates of a graph\n\n\nspectralCoords(a)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:183\n\n\nspectralDrawing\n\n\nComputes spectral coordinates, and then uses plotGraph to draw\n\n\nspectralDrawing(a)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:175\n\n\nsubsampleEdges\n\n\nCreate a new graph from the old, but keeping edge edge with probability \np\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:87\n\n\ntarjanStretch\n\n\ntarjanStretch{Tv,Ti}(t::Laplacians.RootedTree{Tv,Ti}, mat::SparseMatrixCSC{Tv,Ti}, depth::Array{Tv,1})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/treeAlgs.jl:272\n\n\ntoUnitVector\n\n\ncreates a unit vector of length n from a given set of integers, with weights based on the number of occurences\n\n\ntoUnitVector(a::Array{Int64,1}, n)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:192\n\n\ntwoLift\n\n\nCreats a 2-lift of a.  \nflip\n is a boolean indicating which edges cross\n\n\ntwoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:108\n\n\nuniformWeight\n\n\nPut a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.\n\n\nuniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:65\n\n\nunweight\n\n\nCreate a new graph in that is the same as the original, but with all edge weights 1\n\n\nunweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphOps.jl:43\n\n\nvecToComps\n\n\nvecToComps{Ti}(compvec::Array{Ti,1})\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphAlgs.jl:88\n\n\nweighti\n\n\nweighti{Tv,Ti}(mat::SparseMatrixCSC{Tv,Ti}, v::Ti, i::Ti)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphUtils.jl:13\n\n\nwtedChimera\n\n\nBuilds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nGenerate a chimera, and then apply a random weighting scheme\n\n\nwtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)\n\n\n\n\nat /Users/spielman/git/Laplacians.jl/src/graphGenerators.jl:528", 
            "title": "Whole API"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html", 
            "text": "graphGenerators\n\n\npathGraph\n\n\nThe path graph on n vertices\n\n\npathGraph(n::Int64)\n\n\n\n\ngraphGenerators.jl:45\n\n\ncompleteGraph\n\n\nThe complete graph\n\n\ncompleteGraph(n::Int64)\n\n\n\n\ngraphGenerators.jl:54\n\n\nringGraph\n\n\nThe simple ring on n vertices\n\n\nringGraph(n::Int64)\n\n\n\n\ngraphGenerators.jl:60\n\n\ngeneralizedRing\n\n\nA generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example, \n\n\ngeneralizedRing(17, [1 5])\n\n\n\n\ngeneralizedRing(n::Int64, gens)\n\n\n\n\ngraphGenerators.jl:75\n\n\nrandGenRing\n\n\nA random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution\n\n\nrandGenRing(n::Int64, k::Integer)\n\n\n\n\ngraphGenerators.jl:98\n\n\nhyperCube\n\n\nThe d dimensional hypercube.  Has 2^d vertices\n\n\nhyperCube(d::Int64)\n\n\n\n\ngraphGenerators.jl:105\n\n\ncompleteBinaryTree\n\n\nThe complete binary tree on n vertices\n\n\ncompleteBinaryTree(n::Int64)\n\n\n\n\ngraphGenerators.jl:119\n\n\ngrid2\n\n\nAn n-by-m grid graph.  iostropy is the weighting on edges in one direction.\n\n\ngrid2(n::Int64)\ngrid2(n::Int64, m::Int64)\n\n\n\n\ngraphGenerators.jl:139\n\n\ngrid2coords\n\n\nCoordinates for plotting the vertices of the n-by-m grid graph\n\n\ngrid2coords(n::Int64, m::Int64)\ngrid2coords(n)\n\n\n\n\ngraphGenerators.jl:143\n\n\nrandMatching\n\n\nA random matching on n vertices\n\n\nrandMatching(n::Int64)\n\n\n\n\ngraphGenerators.jl:154\n\n\nrandRegular\n\n\nA sum of k random matchings on n vertices\n\n\nrandRegular(n::Int64, k::Int64)\n\n\n\n\ngraphGenerators.jl:167\n\n\ngrownGraph\n\n\nCreate a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.    \n\n\ngrownGraph(n::Int64, k::Int64)\n\n\n\n\ngraphGenerators.jl:182\n\n\ngrownGraphD\n\n\nLike a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices\n\n\ngrownGraphD(n::Int64, k::Int64)\n\n\n\n\ngraphGenerators.jl:214\n\n\nprefAttach\n\n\nA preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.\n\n\nprefAttach(n::Int64, k::Int64, p::Float64)\n\n\n\n\ngraphGenerators.jl:238\n\n\nErdosRenyiCluster\n\n\nGenerate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.\n\n\nErdosRenyiCluster(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:319\n\n\nErdosRenyiClusterFix\n\n\nLike an Erdos-Renyi cluster, but add back a tree so it has n vertices\n\n\nErdosRenyiClusterFix(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:332\n\n\npureRandomGraph\n\n\nGenerate a random graph with n vertices from one of our natural distributions\n\n\npureRandomGraph(n::Integer)\n\n\n\n\ngraphGenerators.jl:347\n\n\nchimera\n\n\nBuilds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nBuilds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace\n\n\nchimera(n::Integer)\nchimera(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:392\n\n\nrandWeight\n\n\nApplies one of a number of random weighting schemes to the edges of the graph\n\n\nrandWeight(a)\n\n\n\n\ngraphGenerators.jl:463\n\n\nwtedChimera\n\n\nBuilds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.\n\n\nGenerate a chimera, and then apply a random weighting scheme\n\n\nwtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)\n\n\n\n\ngraphGenerators.jl:528", 
            "title": "graphGenerators"
        }, 
        {
            "location": "/graphGeneratorsAPI/index.html#graphgenerators", 
            "text": "pathGraph  The path graph on n vertices  pathGraph(n::Int64)  graphGenerators.jl:45  completeGraph  The complete graph  completeGraph(n::Int64)  graphGenerators.jl:54  ringGraph  The simple ring on n vertices  ringGraph(n::Int64)  graphGenerators.jl:60  generalizedRing  A generalization of a ring graph. The vertices are integers modulo n. Two are connected if their difference is in gens. For example,   generalizedRing(17, [1 5])  generalizedRing(n::Int64, gens)  graphGenerators.jl:75  randGenRing  A random generalized ring graph of degree k. Gens always contains 1, and the other k-1 edge types are chosen from an exponential distribution  randGenRing(n::Int64, k::Integer)  graphGenerators.jl:98  hyperCube  The d dimensional hypercube.  Has 2^d vertices  hyperCube(d::Int64)  graphGenerators.jl:105  completeBinaryTree  The complete binary tree on n vertices  completeBinaryTree(n::Int64)  graphGenerators.jl:119  grid2  An n-by-m grid graph.  iostropy is the weighting on edges in one direction.  grid2(n::Int64)\ngrid2(n::Int64, m::Int64)  graphGenerators.jl:139  grid2coords  Coordinates for plotting the vertices of the n-by-m grid graph  grid2coords(n::Int64, m::Int64)\ngrid2coords(n)  graphGenerators.jl:143  randMatching  A random matching on n vertices  randMatching(n::Int64)  graphGenerators.jl:154  randRegular  A sum of k random matchings on n vertices  randRegular(n::Int64, k::Int64)  graphGenerators.jl:167  grownGraph  Create a graph on n vertices. For each vertex, give it k edges to randomly chosen prior vertices. This is a variety of a preferential attachment graph.      grownGraph(n::Int64, k::Int64)  graphGenerators.jl:182  grownGraphD  Like a grownGraph, but it forces the edges to all be distinct. It starts out with a k+1 clique on the first k vertices  grownGraphD(n::Int64, k::Int64)  graphGenerators.jl:214  prefAttach  A preferential attachment graph in which each vertex has k edges to those that come before.  These are chosen with probability p to be from a random vertex, and with probability 1-p to come from the endpoint of a random edge. It begins with a k-clique on the first k+1 vertices.  prefAttach(n::Int64, k::Int64, p::Float64)  graphGenerators.jl:238  ErdosRenyiCluster  Generate an ER graph with average degree k, and then return the largest component. Will probably have fewer than n vertices. If you want to add a tree to bring it back to n, try ErdosRenyiClusterFix.  ErdosRenyiCluster(n::Integer, k::Integer)  graphGenerators.jl:319  ErdosRenyiClusterFix  Like an Erdos-Renyi cluster, but add back a tree so it has n vertices  ErdosRenyiClusterFix(n::Integer, k::Integer)  graphGenerators.jl:332  pureRandomGraph  Generate a random graph with n vertices from one of our natural distributions  pureRandomGraph(n::Integer)  graphGenerators.jl:347  chimera  Builds the kth chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  Builds a chimeric graph on n vertices. The components come from pureRandomGraph, connected by joinGraphs, productGraph and generalizedNecklace  chimera(n::Integer)\nchimera(n::Integer, k::Integer)  graphGenerators.jl:392  randWeight  Applies one of a number of random weighting schemes to the edges of the graph  randWeight(a)  graphGenerators.jl:463  wtedChimera  Builds the kth wted chimeric graph on n vertices. It does this by resetting the random number generator seed. It should captute the state of the generator before that and then return it, but it does not yet.  Generate a chimera, and then apply a random weighting scheme  wtedChimera(n::Integer)\nwtedChimera(n::Integer, k::Integer)  graphGenerators.jl:528", 
            "title": "graphGenerators"
        }, 
        {
            "location": "/graphOpsAPI/index.html", 
            "text": "graphOps\n\n\nshortIntGraph\n\n\nConvert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much\n\n\nshortIntGraph(a::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:30\n\n\nlap\n\n\nCreate a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry\n\n\nlap(a)\n\n\n\n\ngraphOps.jl:39\n\n\nunweight\n\n\nCreate a new graph in that is the same as the original, but with all edge weights 1\n\n\nunweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\ngraphOps.jl:43\n\n\nmapweight\n\n\nCreate a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write\n\n\nb = mapweight(a, x-\nrand(1)[1])\n\n\n\n\nmapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)\n\n\n\n\ngraphOps.jl:56\n\n\nuniformWeight\n\n\nPut a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.\n\n\nuniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})\n\n\n\n\ngraphOps.jl:65\n\n\nproductGraph\n\n\nThe Cartesian product of two graphs.  When applied to two paths, it gives a grid.\n\n\nproductGraph(a0::SparseMatrixCSC{Tv,Ti\n:Integer}, a1::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:69\n\n\nedgeVertexMat\n\n\nThe signed edge-vertex adjacency matrix\n\n\nedgeVertexMat(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphOps.jl:78\n\n\nsubsampleEdges\n\n\nCreate a new graph from the old, but keeping edge edge with probability \np\n\n\nsubsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)\n\n\n\n\ngraphOps.jl:87\n\n\ntwoLift\n\n\nCreats a 2-lift of a.  \nflip\n is a boolean indicating which edges cross\n\n\ntwoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)\n\n\n\n\ngraphOps.jl:108\n\n\njoinGraphs\n\n\ncreate a disjoint union of graphs a and b,  and then put k random edges between them\n\n\njoinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)\n\n\n\n\ngraphOps.jl:120\n\n\nplotGraph\n\n\nPlots graph gr with coordinates (x,y)\n\n\nplotGraph(gr, x, y)\nplotGraph(gr, x, y, color)\n\n\n\n\ngraphOps.jl:138\n\n\nspectralDrawing\n\n\nComputes spectral coordinates, and then uses plotGraph to draw\n\n\nspectralDrawing(a)\n\n\n\n\ngraphOps.jl:175\n\n\nspectralCoords\n\n\nComputes the spectral coordinates of a graph\n\n\nspectralCoords(a)\n\n\n\n\ngraphOps.jl:183\n\n\ntoUnitVector\n\n\ncreates a unit vector of length n from a given set of integers, with weights based on the number of occurences\n\n\ntoUnitVector(a::Array{Int64,1}, n)\n\n\n\n\ngraphOps.jl:192\n\n\ndiagmat\n\n\nreturns the diagonal matrix(as a sparse matrix) of a graph\n\n\ndiagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})\n\n\n\n\ngraphOps.jl:214\n\n\ngeneralizedNecklace\n\n\nConstructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.\n\n\ngeneralizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti\n:Integer}, k::Int64)\n\n\n\n\ngraphOps.jl:234", 
            "title": "graphOps"
        }, 
        {
            "location": "/graphOpsAPI/index.html#graphops", 
            "text": "shortIntGraph  Convert the indices in a graph to 32-bit ints.  This takes less storage, but does not speed up much  shortIntGraph(a::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:30  lap  Create a Laplacian matrix from an adjacency matrix. We might want to do this differently, say by enforcing symmetry  lap(a)  graphOps.jl:39  unweight  Create a new graph in that is the same as the original, but with all edge weights 1  unweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  graphOps.jl:43  mapweight  Create a new graph that is the same as the original, but with f applied to each nonzero entry of a. For example, to make the weight of every edge uniform in [0,1], we could write  b = mapweight(a, x- rand(1)[1])  mapweight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, f)  graphOps.jl:56  uniformWeight  Put a uniform [0,1] weight on every edge.  This is an example of how to use mapweight.  uniformWeight{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind})  graphOps.jl:65  productGraph  The Cartesian product of two graphs.  When applied to two paths, it gives a grid.  productGraph(a0::SparseMatrixCSC{Tv,Ti :Integer}, a1::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:69  edgeVertexMat  The signed edge-vertex adjacency matrix  edgeVertexMat(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphOps.jl:78  subsampleEdges  Create a new graph from the old, but keeping edge edge with probability  p  subsampleEdges(a::SparseMatrixCSC{Float64,Int64}, p::Float64)  graphOps.jl:87  twoLift  Creats a 2-lift of a.   flip  is a boolean indicating which edges cross  twoLift(a)\ntwoLift(a, flip::AbstractArray{Bool,1})\ntwoLift(a, k::Integer)  graphOps.jl:108  joinGraphs  create a disjoint union of graphs a and b,  and then put k random edges between them  joinGraphs{Tval,Tind}(a::SparseMatrixCSC{Tval,Tind}, b::SparseMatrixCSC{Tval,Tind}, k::Integer)  graphOps.jl:120  plotGraph  Plots graph gr with coordinates (x,y)  plotGraph(gr, x, y)\nplotGraph(gr, x, y, color)  graphOps.jl:138  spectralDrawing  Computes spectral coordinates, and then uses plotGraph to draw  spectralDrawing(a)  graphOps.jl:175  spectralCoords  Computes the spectral coordinates of a graph  spectralCoords(a)  graphOps.jl:183  toUnitVector  creates a unit vector of length n from a given set of integers, with weights based on the number of occurences  toUnitVector(a::Array{Int64,1}, n)  graphOps.jl:192  diagmat  returns the diagonal matrix(as a sparse matrix) of a graph  diagmat{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti})  graphOps.jl:214  generalizedNecklace  Constructs a generalized necklace graph starting with two graphs A and H. The resulting new graph will be constructed by expanding each vertex in H to an instance of A. k random edges will be generated between components. Thus, the resulting graph may have weighted edges.  generalizedNecklace{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}, H::SparseMatrixCSC{Tv,Ti :Integer}, k::Int64)  graphOps.jl:234", 
            "title": "graphOps"
        }, 
        {
            "location": "/graphAlgsAPI/index.html", 
            "text": "graphAlgs\n\n\nbiggestComp\n\n\nReturn the biggest component in a graph\n\n\nbiggestComp(mat::SparseMatrixCSC{Tv,Ti\n:Integer})\n\n\n\n\ngraphAlgs.jl:111", 
            "title": "graphAlgs"
        }, 
        {
            "location": "/graphAlgsAPI/index.html#graphalgs", 
            "text": "biggestComp  Return the biggest component in a graph  biggestComp(mat::SparseMatrixCSC{Tv,Ti :Integer})  graphAlgs.jl:111", 
            "title": "graphAlgs"
        }, 
        {
            "location": "/cutPageRankAPI/index.html", 
            "text": "cutPageRank\n\n\npr\n\n\ncomputes a page rank vector satisfying p = a/n * 1 + (1 - a) * W * p\n\n\npr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, alpha::Float64)\n\n\n\n\ncutPageRank.jl:20\n\n\nppr\n\n\ncomputes the personal page rank vector from a starting vector s and an alpha; operates with lazy walk matrix \n\n\nppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64)\nppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, niter::Int64)\n\n\n\n\ncutPageRank.jl:38\n\n\napr\n\n\ncomputes an approximate page rank vector from a starting vector s, an alpha and an epsilon \n\n\napr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, eps::Float64)\n\n\n\n\ncutPageRank.jl:71\n\n\nprn\n\n\nprn{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)\n\n\nthe PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper\n\n\nv is a set of vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]\n\n\nprn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)\n\n\n\n\ncutPageRank.jl:115", 
            "title": "cutPageRank"
        }, 
        {
            "location": "/cutPageRankAPI/index.html#cutpagerank", 
            "text": "pr  computes a page rank vector satisfying p = a/n * 1 + (1 - a) * W * p  pr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, alpha::Float64)  cutPageRank.jl:20  ppr  computes the personal page rank vector from a starting vector s and an alpha; operates with lazy walk matrix   ppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64)\nppr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, niter::Int64)  cutPageRank.jl:38  apr  computes an approximate page rank vector from a starting vector s, an alpha and an epsilon   apr{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, s::Array{Float64,1}, alpha::Float64, eps::Float64)  cutPageRank.jl:71  prn  prn{Tv, Ti}(G::SparseMatrixCSC{Tv, Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)  the PageRank-Nibble cutting algorithm from the Anderson/Chung/Lang paper  v is a set of vertices, phi is a constant in (0, 1], and b is an integer in [1, [log m]]  prn{Tv,Ti}(G::SparseMatrixCSC{Tv,Ti}, v::Array{Int64,1}, phi::Float64, b::Int64)  cutPageRank.jl:115", 
            "title": "cutPageRank"
        }
    ]
}